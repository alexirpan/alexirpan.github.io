---
layout: post
title:  "Writing MIT Mystery Hunt 2023"
date:   2023-02-06 00:32:00 -0700
---

*This post is XXX words long, and is riddled with spoilers about pretty much every aspect of MIT Mystery Hunt 2023. There are no spoiler bars. You have been warned.*

I feel like every puzzle aficionado goes through at least one conversation where they try
to explain what puzzlehunts are, and why they're fun, and this conversation goes poorly.
It's just a hard hobby to explain.
Usually, I say something about escape rooms, and that works, but in many ways the typical
puzzlehunt is *not* like an escape room? "Competitive collaborative spreadsheeting" is more
accurate but less clear why people would find it entertaining.

Let me put on my machine learning researcher hat for a bit and try to explain puzzles
differently. In a puzzlehunt, each puzzle is a bunch of data. In a good puzzle,
that data has **exactly one good explanation**, one which fits better than every
alternative. Your job as a solver is to find that explanation.

A list of clues, a small game, a bunch of images, whatever. The explanation is usually
not obvious and fairly indirect, but there is a guiding contract between the puzzle
setter and puzzle solver that the puzzle can be solved. At the end of the puzzle,
you usually get an English word or phrase, but that is more to give a puzzle its
conclusion. The core appeal of a puzzle is figuring out what's going on, and when
you find the explanation (get the a-ha), that's when you're having fun.

This is why puzzlehunting as a hobby is really biased towards professors and programmers.
Research and debugging share a similar root of trying to explain the behavior
of a confusing system. It stretches the same muscles.

Okay. That gives more of an answer to why people do puzzles. Why do people *write* puzzles?
For me, Mystery Hunt escalated from a thing on the side, to a part-time job, to a full-time
job.
Writing a puzzlehunt is incredibly time consuming and stressful, you're usually not getting much money,
and your work will, in the end, only be appreciated by a small group of people. It all
seems pretty irrational.

Well...in some sense it is. That doesn't mean it's not worth doing.


# Post Structure

Oooh, a section of the post describing the post itself. How *fancy.* How *meta.*

There are going to be spoilers for just about all aspects of Hunt. Much of Mystery Hunt is
bled together in my mind and it's not easy to segment it into nice pieces.

I have tried to present everything chronologically, except when talking about the construction
of specific puzzles I worked on, in which case I've grouped the discussion

<div class="shaded" markdown="1">
like so.
</div>

Usually I was juggling multiple puzzles at once, so strict chronology would be more confusing
than anything else.

I've decided to be complete rather than entertaining. I'm doing this because I'm not sure
of the exact audience for this post, and figure it'd be useful if I just dumped everything I thought
was relevant. Hopefully you find it entertaining anyways.


# December 2021 - Oh Boy, Mystery Hunt is Soon!

Writing and post-hunt tech work for [Teammate Hunt 2021](https://2021.teammatehunt.com/)
has been done for a while. Life is good. Team leadership sends out a survey to see if the
team has enough motivation to write Mystery Hunt.

I use a time tracker app for a few things in my life, and puzzle writing is one of them.
My time spent on Teammate Hunt clocked in at 466 hours. I do some math and find it averaged
to 15 hours/week. This is helpful when trying to decide how to answer the question for
how much time I'd commit to Mystery Hunt if we won.

I already had some misgivings around how much time and headspace Teammate Hunt took up for me.
On the other hand, it is Mystery Hunt. Noting that I felt like I did too much for Teammate Hunt,
I targeted 10 hours/week for Mystery Hunt.

The survey results come in, and there is enough interest to go for the win. I have zero ideas
for a puzzle using Mystery Hunt Bingo, but figure that *maybe* we'll win Hunt, and *maybe*
there will be a puzzle using Mystery Hunt Bingo, so I'd better [remove the "this is not
a puzzle"](https://github.com/alexirpan/mystery-hunt-bingo/commit/77842e1962ac791ce77c56d8a25d00f79227befc)
warning early, just in case.
I didn't want to face any [warrant canary](https://en.wikipedia.org/wiki/Warrant_canary)
accusations if we actually won.


# January 2022

## The Game is Afoot

Holy shit we won Hunt!!!!!

I write a post about [Mystery Hunt 2022]({% post_url 2022-01-22-mh-2022 %}), where I make a few predictions about how writing
Mystery Hunt 2023 will go.

> After writing puzzles fairly continuously for 3 years (MLP: Puzzles are Magic into Teammate
Hunt 2020 into Teammate Hunt 2021), I have a better sense of how easy it is for me to let puzzles
consume all my free time [...]
> Sure, making puzzles is rewarding, but lots of things are rewarding,
and I feel I need to set stricter boundaries on the time I allocate to this way of life - boundaries
that are likely to get pushed the hardest by working on Mystery Hunt of all things.
>
> [...] I'm not expecting to write anything super crazy. Hunt is Hunt, and I am cautiously
optimistic that I have enough experience with the weight of expectations to get through the writing
process okay.

Before officially joining the writing Discord, I set myself some personal guidelines.

**Socializing take priority over working on Mystery Hunt.** I know
I can find time for Mystery Hunt if I really need to. A lot of puzzle writing can be done asynchronously,
and I'm annoyingly productive in the 12 AM - 2 AM time period.

**No more interactive puzzles, or puzzles that require non-trivial amounts of code to construct.**
The goal is to make puzzles with good creation-time to solve-time ratios. Puzzles that require
coding are usually a nightmare on this axis,
since it combines the joys of fixing code with the joys of fixing broken puzzle design.

**No more puzzles where I need to spend a large amount of time studying things before I can even
start construction.** Again, similar reason, this process is very time consuming for the payoff.
I'd estimate I spent 80 hours on
[Marquee Fonts](https://2021.teammatehunt.com/puzzles/marquee-fonts), since I started with knowing
nothing about how fonts worked and ended with knowing much more than I'd ever need to know.

**No more puzzles made of minipuzzles.** Minipuzzles are a scam. "Oh, we don't have any ideas that
are big enough to fill one puzzle. Let's make a bunch of minipuzzles instead because it's easy to
come up with small ideas!" Then you get halfway through, and realize that ideation of small puzzles
is easy, but execution takes way longer since the process of finding suitable clues is somewhat
independent of puzzle difficulty. I also felt it was a crutch I was relying on too often.

**No more puzzles with very tight constraints.** It collectively took 60-100 person hours to
find a good-enough construction for [Mystical Plaza](https://2021.teammatehunt.com/puzzles/the-mystical-plaza),
even with breaking some puzzle rules along the way. Usually, the time spent fitting a tight constraint
does not directly translate into puzzle content.

These guidelines all had a common theme: keep Hunt managable, and make puzzles that needed less time
to go from idea to final puzzle.

I would end up breaking every one of these guidelines.

## Team Goals and Theme Proposals

The very first thing we did for Hunt was run a survey to decide what Hunt teammate wanted to write. What
was the teammate experience that we wanted solvers to have?

We arrived at these goals:

1. Unique and memorable puzzles
2. Innovation in hunt structure
3. High production value
4. Build a great experience for small / less intense teams

**Unique and memorable puzzles:** Mystery Hunt is one of the few venues where you can justifiably write
a puzzle about, say, grad-level complexity theory. That's not the only way to make a unique and memorable
puzzle, but in general the goal was to be creative and have fewer filler puzzles.

**Innovation in hunt structure:** This is something that both previous Teammate Hunts did, and as a team
we have a lot of pride in creating puzzles that stretch the boundaries of what puzzles can be.

**High production value:** teammate has both a lot of software engineers and a lot of art talent, which
let us make prettier websites and introduce innovations like copy-to-clipboard. We wanted to make a Hunt
that lived up to the standards set by our previous Hunts

**Build a great experience for small / less intense teams:** We generally felt that Mystery Hunt had
gotten too big. Before winning Hunt, we had already downsized and were around 60% the size of Palindrome
when they won last year. Correspondingly, we spent a while discussing how to create fewer puzzles while
still creating a Hunt of satisfying length, as well as the importance of mid-Hunt milestones.

We then held team elections for leadership roles. People ran for roles, wrote a short blurb, and
we voted for each. I deliberately did not run for leadership roles, since they implied a baseline
level of commitment that was above my 10 hr/week target.

With elections done, we started on theme proposals. This is always an interesting time in hunt development,
since it sets the agenda of the entire upcoming year. Things can change later, but writing a hunt is an
especially top-down design process. You decide your story, which decides your metametas and metas,
which decides your feeders, and all of this is handing off work to your future selves.

Historically, at the start of theme writing, I say I don't have theme ideas. Then I get an idea right
before the deadline and rush out a theme proposal.
This happened in Teammate Hunt 2021 and it happened for Mystery Hunt. The theme I pitched for Teammate
Hunt 2021 was not revived for Mystery Hunt (I didn't think it scaled up correctly), but the Puzzle
Factory theme is recycled from a Teammate Hunt 2021 proposal.
We talked a bit about whether this was okay, since some organizers for Teammate Hunt 2021 were not
writing Hunt this year. In the end we decided it was fine. At most there would
be plot spoilers, not meta spoilers.

A few members with past Mystery Hunt experience
mentioned that theme ideation could get contentious. People naturally get invested in themes,
and spend time polishing their theme proposal. People working on *other* themes would observe
this, and feel obligated to polish their proposals. This could escalate into a theme arms race,
where lots of time was spent on themes that would ultimately not get picked.

To try to avoid this, a strict 1 page limit was placed on all theme proposals. People
were free to read discussion threads of longer freeform brainstorming, but there would be no expectation
to do so, and all plot and structure proposals needed to fit in 1 page.

Did this work? I would say "maybe". It definitely cut down on theme selection time, and reduced work on
discarded themes, but it also necessarily forced theme proposals to be light on details. Team memes
like "teammate is the villain" seemed to work their way into every serious theme proposal at some level.
Maybe that was genuinely the story we wanted to tell, but it could also have been an artifact of writing
themes while the memes were fresh. There may have been more diversity in theme ideas if they were written
over a longer period of time.

I was not on the story team, but in our post-Hunt retrospective, members of the story team
mentioned they were under a lot of pressure to fill in plot details that weren't in the theme
proposal, because, well, there wasn't space for them in the proposal! Even the details that do exist
differ a lot from where the story ended up. Here is how I would summarize the final version of the Hunt
story.

> teammate announces a Museum themed puzzlehunt written by MATE, a puzzle creating AI. teammate is
> really concerned with making a "perfect" Mystery Hunt that isn't doing anything too crazy. The Museum
> is Act I of the Hunt. Over the
> course of solving, teams discover the Puzzle Factory, the place where Mystery Hunt puzzles are created.
> The Puzzle Factory is not a place that solvers were supposed to discover, and teammate does their best
> to pretend it doesn't exist when interacting with teams. The Puzzle Factory is Act II of the Hunt, and is
> explored simultaneously with Act I. As they explore the factory, teams learn that
> MATE is overworked, and other AIs that could have helped MATE were locked away by teammate due to being
> too weird.
>
> Solvers reconnect the AIs, and this prompts teammate to shut down MATE, Mystery Hunt and the Puzzle Factory.
> They berate teams for trying to turn on the old AIs, then leave.
>
> However, there is some lingering power after Mystery Hunt is shutdown, which solvers can use to slowly
> turn the Puzzle Factory and other AIs back on. This starts Act III of the Hunt.
> Each AI round is gimmicked in some way, ending in a feature request
> that AI wants to add to the Puzzle Factory. When all AI rounds are complete, teammate comes back and
> admits that they were wrong, and the Puzzle Factory makes one more puzzle, which has the coin.

Now, here is the start of the original proposal:

> Act I begins with the announcement of an AI called MATE that can generate an infinite stream of perfect puzzles, as well as provide real-time chat assistance for hints, puzzle-solving tools, etc). During kickoff, teammate gives a business presentation with MATE in the background– but at the end, the video feed glitches briefly and other AIs show up for a split second (“HELP I’M TRAPPED”); teammate doesn’t notice. Stylistically, the first round looks like a futuristic, cyberspace factory. As teams solve the initial round of puzzles, errata unlock (later discovered to be left by AIs locked deeper in the factory), hinting that there’s something “out of bounds”. No meta officially exists for this round (the round is “infinite”), but solving and submitting the answer in an unconventional way leads to breaking out. (To prevent teams from getting stuck forever, we can design the errata/meta clues to get more obvious the more puzzles they solve.) Solving this first meta also causes MATE to doubt their purpose and join you as an ally in act II.

Quite a lot changed from the start to the end. The infinite stream idea was cut because we couldn't
figure out the design. Kickoff did not show the other AIs at all. The surface theme was changed to something
completely different. In a longer ideation process, perhaps more of this design work could be done by the entire team,
rather than just the story team.
Maybe allowing wasted effort is worth it if it gets the details filled out early?

Themes were rated on
a 1-5 scale, where 1 = "This theme would directly decrease my motivation to work on Hunt (only use if serious)" and 5 = "I'll put in the hours to make this theme work".
I don't remember exactly how I voted, but I remember voicing some concerns about the Puzzle Factory.
The plot proposal seemed pretty complicated compared to previous Hunts. I wasn't sure how
well we'd be able to convey the story - [You Get About Five Words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words)
felt accurate for Mystery Hunt, where some people will speedrun the story in favor of focusing on
puzzles. I was also hesitant about whether we'd have enough good ideas for gimmicks to fill out the AI rounds in
Act III. It seemed like a good theme for a hunt with 40 puzzles, but I didn't know if it worked for
a Hunt with 150+ puzzles.

I'm happy I was wrong on both counts. Feedback on the story has been good, and I feel the AI round gimmicks
all justified themselves. I was imagining a Mystery Hunt where Act III was the size of Bookspace (~10 rounds)
and limiting it to 4 rounds did a lot for feasibility.

The Puzzle Factory did not win by a landslide, but it was the only theme with no votes of 1, and had more
votes of 5 than any other theme. Puzzle Factory it is!


## Hunt Tech Infrastructure

I'm going to talk a lot about hunt tech, a very niche topic even within the puzzle niche.
Still, I'm going to do so anyways because

1. It's my blog I get to write what I want.
2. By now I've worked with four different puzzlehunt codebases (Puzzlehunt CMU, gph-site, tph-site,
and spoilr) so I've got some perspective on the different design decisions.

The first choice we had to make was whether we'd use the hunt codebase from Palindrome, or use the tph-site
codebase we'd built over Teammate Hunt 2020 and Teammate Hunt 2021. Our early plan is to mostly build
off tph-site. The assumption we made is that most Mystery Hunt teams do not have an active codebase,
and default to using the code from the previous Mystery Hunt. However, teammate had tph-site, knew how
to use it, and in particular had accumulated a lot of helper code to make crossword grids, implement
copy-to-clipboard, and create interactive puzzles.

The only recent team that seemed like they'd face a similar decision was Galactic, who decided to
build off the spoilr codebase into [silenda](https://github.com/YewLabs/silenda) rather than use gph-site.
After asking some questions, it sounded like the reason this happened was because parts of the tech
team were already familiar with spoilr. So for our situation, it seemed correct to use whatever code
we knew best, which was tph-site.

We start work on open-sourcing tph-site, partly because it's not too much work to do so and partly
because [Huntinality](https://2022.huntinality.com/) is asking if they can see the work we did to
convert our frontend to React.

<div class="shaded" markdown="1">
### A React Tangent

Almost every hunt codebase is written in [Django](https://www.djangoproject.com/). It's
a Python web framework that does a lot of work for you. Python code lets you define your
database schema, user model, what backend code you want to run when users make a request,
and what URLs you want everything to live at. Although it is helpful to know what happens under
the hood, Django makes it possible to build a site without knowing what happens under the hood
using just Python, one of the most friendly beginner languages.
I first learned Django 11 years ago and it's still relevant today.

The default recommended approach in Django is that when a request comes in, you render an
HTML response based on a template file that lives on your backend. The template gets filled out
on the server and then gets sent back as the viewed webpage.

tph-site still uses Django as its backened, but differs in using a React + Next.js based frontend.
React is a Javascript library whose organizing principle is that you describe your page in components.
Each component either has internal state or state passed from whatever creates the components.
A component describes what it ought to look like according to the current state, and whenever the
state is updated, React will determine everything that could depend on that state and rerender it.
The upside: dynamic or interactive web pages become a lot easier to build, since React will handle
a lot of boilerplate Javascript and state management for you. The downside: extra layers of indirection
between your code and the resulting HTML, and potentially larger network downloads depending on how
you set up your site.

Next.js is then a web framework that makes it easier to pass React state from the server, and
support rendering pages server-side. This is especially useful for puzzlehunts, where you want to
do as many things server-side as possible to prevent spoilers from leaking to the frontend.

The tph-site fork exists because teammate devs wanted to use React to implement the Playmate in
Teammate Hunt 2020. Porting gph-site to React was quite painful, but I don't think Playmate was
getting implemented without it, and we've since used it to support other interactive puzzles.
In general, I believe we made the codebase more powerful, but also increased the complexity by
adding another framework / build system. (To use tph-site, you need to know both Django and React,
instead of just Django.) For new puzzlehunt makers, I would generally recommend starting with a
setup like gph-site, until they know they want a more complicated frontend.
</div>



# February 2022

## PuzzUp

[PuzzUp](https://github.com/Palindrome-Puzzles/puzzup) is Palindrome's fork of [Puzzlord](https://github.com/galacticpuzzlehunt/puzzlord), and is a Django app for managing puzzles and testsolves. We considered
giving it a teammate brand name and didn't because there were more important things to do.

The mantra of puzzlehunt tech is that it's all about the processes. The later in the year it gets,
the busier everyone is with puzzle writing, and good luck implementing feature requests during
Hunt. Early in the year is therefore the best time to brainstorm ways to reduce friction in
puzzle writing and hunt HQ management.

The PuzzUp codebase had some initial Discord integrations to auto-create Discord channels when puzzles
were created in PuzzUp. We wanted to extend this integration to auto-create testsolve channels for
each puzzle. However, Discord limits servers to have a max of 500 channels. Based on an extrapolation
from Teammate Hunt, we'd have more than 500 combined puzzle ideas + testsolve by the end of Mystery
Hunt writing. (I just checked out of curiosity, and we hit 338 puzzle ideas and almost 500 testsolves by the
end of Hunt.)

We poked around and found Discord has much looser limits on threads! So anything that lets us permute
channels into threads lets us get around the Discord limits.

Here's what we landed on: all testsolves are threads. Each thread is made in a #testsolve-mute-me
channel. Muting the channel disables all notifications from the channel. Whenever a testsolve session
is created, the PuzzUp server would start a thread, tag everyone who should be in the testsolve,
then immediately delete the message that linked to thread creation. The thread would still exist,
and could be searched for, but no link would appear in the text channel. We also extended the codebase
to have Google Drive integration, to auto-create testsolve spreadsheets for each new testsolve.

I say "we" here but I did none of this work. I believe it was mostly done by Herman. Much later in
the year, I updated the Google integration to auto-create a brainstorming spreadsheet for new puzzles,
because I got annoyed at manually making one and linking it in PuzzUp each meeting. You don't know what will
be tedious until you've done it for the 20th time.


## Puzzle Potluck

A puzzle potluck (no not [that one](https://puzzlepotluck.com/4)) is announced for early March.
The goal is to provide a low-stakes, casual venue for people to start writing puzzle ideas.
There's not much to do in tech yet, so I start working on three ideas. One does not work and does
not make it into Hunt. One is an early form of 5D Barred Diagramless with Multiverse Time Travel.
The last goes through mostly unchanged.

<div class="shaded" markdown="1">
### Quandle

Perhaps you remember that I set a personal guideline for "no more interactive puzzles", and think
it's strange that I was working on an interactive puzzle within a month.
Yeah, uh, I don't know what to tell you.

In my defense, as soon as "Quantum Wordle" entered my brain, I was convinced it would be a good
puzzle and that I had to make it.
I found an [open-source Wordle clone](https://github.com/cwackerfuss/react-wordle) and got to work figuring out how to modify it to support a quantum superposition
of target words. This took a while, since I started with the incorrect assumption that letters
in a guess are independent of each other. This isn't true. Suppose the Wordle is ENEMY, and you
guess the word LEVEE. The Wordle algorithm will color the first two Es yellow, and the last one
gray.
When extended in the quantum direction,
you can't determine the probability distribution of one E without considering the other Es. They're
already dependent on each other. (Grant Sanderson of 3Blue1Brown would [put out a video admitting to
a similar mistake](https://www.youtube.com/watch?v=fRed0Xmc2Wg) shortly after I realized my error, so at least I'm in good company.)

After I got the proof of concept setup, I considered how to do puzzle extraction. I considered trying
to have the extraction be based on finding all observations that forced exactly one reality, but after thinking
about it more, I realized it was incredibly constraining on the wordlist. This certainly wasn't a
mechanic I was going to figure out in time for potluck, so I went with an arbitrary order of words and
an arbitrary letter from each one, to give me flexibility to make whatever cluephrase I wanted. Making
that cluephrase point to specific words feels like the most interesting idea, and after a bit more
brainstorming, the superposition idea came out.

Internally, the way the puzzle works is that the game starts with 50 realities.
On each guess, the game computes the Wordle feedback for every target word, then averages the feedback
across all realities.
When making an observation, it repeats the calculation to find
every target word consistent with that observation, deletes all other realities, and recomputes
the probabilities for all prior guesses. Are there optimizations? Probably. Do you need to optimize
a 50 realities x 6 guesses x 5 letter problem? No, not really. This will become a running theme. For
Hunt, I optimized for speed of implementation over performance unless it became clear performance
was a bottleneck.

The puzzle could have shown 50 blanks, revealing each blank when you solved a word, but I deliberately
did not do that to make it harder to wheel-of-fortune the cluephrase.
During exploration, I generated random sets of 50 words, to get a feel for how the game
played.
My conclusion was
that 1 observation was too little information to reliably constrain to 1 reality, while 2 observations
gave much more information than needed. I considered making the word list more adversarial, but in my
opinion, the lesson
of Wordle is that it's more fun to give people more information than they need to win. People are not
information-maximizing agents [citation needed]. I left it as-is.

As one of the first tests of our Puzzup setup, I did a puzzle exchange with Brian. He tested Quandle and I tested [Parsley Garden](https://puzzlefactory.place/office/parsley-garden). Around 60 minutes into the Quandle test, I
ask how the puzzle is going. Brian says he's stuck, and after asking a bunch of questions, I figure
out that he's never clicked a guess after making one, meaning he's never seen the probability
distributions or used an observation. Oops. I added a prompt to suggest doing that, and the solve was better
from there.

After potluck, I asked for a five letter answer, but none were available. Instead I got an answer that
was two five letter words. Aside from the design changes needed to make that work, the rest of the puzzle
mechanics stayed the same, and the work later in the year was mostly figuring out how to share team state.

I've been told that technically, the quantum interpretation of Quandle is bad. I believe the core
issue is that you're not supposed to be able to observe the probability distribution of a letter before
observing the outcome. The distribution should immediately collapse to a fixed outcome as soon as you look at it,
and you certainly shouldn't be able to make an observation that collapses from one superposition of 50
realities to another superposition of < 50 realities. This is probably all true, and I don't care.
</div>


# March 2022

## Of Metas and MATEs

The internal puzzle potluck runs! It goes well. Editors tell me that there were multiple Wordle-themed
potluck ideas, the Hunt should only have one, and Quandle is the one they're going to go with. Hooray!
I apologize to the other puzzles in the Wordle graveyard.

Chat implementation for MATE and other AIs is coming along smoothly. teammate has multiple people with NLP experience,
including with large language models, and for this reason we immediately know we are not going to touch
those with a ten foot pole. We are going to stick to hardcoded chat responses, that trigger according
to a hardcoded chat interaction graph, where at most we do a bit of NLP to determine chat intents. As
a proof of concept, we use regex-based intents. Everyone involved with chat then gets busy, and we never
move past regexes. I do wish we'd used one of the lightweight ML libraries for chat intents, to reduce
the "sorry, I don't understand" replies, but I do think it was correct to deprioritize this.

Meta writing is also now in earnest. I mean, it was going ever since theme finalization, but now it's
*extra* going. Very approximately, these are the steps of writing Mystery Hunt puzzles.

1. Decide on a theme.
2. Figure out the major story beats that you want in the Hunt.
3. Ideally, your major story beats are tied to metapuzzles, since this connects the solving process
to the narrative. Those metapuzzles block on story development. However, a bunch of
metapuzzles are off that critical path. Think, say, [Lake Eerie](https://puzzles.mit.edu/2022/round/lake-eerie/) in Mystery Hunt 2022. Good round?
Absolutely! Was its answer critical to the
story of that Hunt? No, not in the way that [The Investigation](https://puzzles.mit.edu/2022/puzzle/the-investigation/) was.
4. When story is decided, start writing the metas for story-critical answers.
5. Once the major story beats are decided, meta proposals for story-critical answers can begin.
6. Whenever a meta finishes testsolving, release all its feeder answers.
7. When all your metas and feeders are done, your puzzles are done.

Mystery Hunt writing is very fundamentally an exercise in running out of time, so everything that
can be done in parallel *should* be done in parallel. Interestingly, for the Puzzle Factory, that
means the AI rounds were ideated first, because their answers were not story-critical, whereas
the Museum and Factory metas were. This was a good idea, since AI rounds were gimmicked
for story reasons, and gimmicked rounds take longer to design.

There weren't too many guidelines on AI round proposals. They had to have a gimmick, and their
final meta needed to be a feature request, but besides that anything went.
It turns out asking teammate to come up with crazy round ideas is pretty easy! In teammate parlance,
an "illegal" puzzle is a puzzle that breaks puzzle convention, and we like them a lot.
I think we ended up with around 15 proposals for 4 slots.

The difficult
part was doing the work to decide if an idea that sounded cool on paper would actually
work on closer inspection. One of my hobbies is Magic: the Gathering, and this issue comes up in custom
Magic card design all the time. Very often, someone will create a card that tells a joke, or makes a cute
reference, and it's cool to read. But if it were turned into a real card, the joke wouldn't convert into
fun gameplay.
Similarly, we needed to find the line between round gimmicks that could support interesting puzzles,
and round gimmicks that could not.

For example, one of my round proposals was a round where every puzzle was contained entirely in its title.
It would involve doing some incredibly illegal things, like "the puzzle title is an animated GIF"
or "the puzzle title changes whenever you refresh the page".
There was some interest, but as soon as we sat down to design the thing, we realized the problem was
that it was practically impossible to write the meta without designing the title for every feeder at the
same time. The gimmick forced way too many constraints way too fast. So, the proposal died in a few
hours, and as far as I'm concerned it should stay that way.

There was a time loop proposal, where the round would periodically reset itself, you'd unlock different
puzzles depending on what choices you made (what puzzles you solved), and the meta would be based
on engineering a "perfect run". This idea lost steam, which is really for the best.

In one brainstorming session, I off-handedly mentioned a [Machine of Death](https://en.wikipedia.org/wiki/This_Is_How_You_Die) short story I read long ago. In it, the brain scan
of a Chinese woman named 愛 is confused with the backup of an AI, since both files were named "ai".
I didn't think much of it at the time, but many people in that session went on to lead the Eye
round, and I'd like to think I had some tiny contribution to that round.

The main round I got involved with was "Inset", which you know as Wyrm. But we'll get to that later.

In one of our weekly general meetings, the plan for the Factory metas are announced. There will be
three metas that need to deliver these story beats.

1. There are multiple AIs.
2. teammate discarded all AIs except for MATE.
3. Remnants of the AIs are still causing strange things in the Mystery Hunt.

We split into groups to brainstorm the three Factory metas, which is where we came up with...

<div class="shaded" markdown="1">
## The Filing Cabinet

I'm not sure how people normally come up with meta puns. What I do is use [RhymeZone](https://www.rhymezone.com/)
to look up rhymes and near-rhymes, then bounce back and forth until something good comes out.
The brainstorm group I was in was focused on the "multiple AIs" story point. Looking for rhymes
on "multiple" and "mate", we found "penultimate".

At which point Patrick proclaimed, "Oh, this puzzle writes itself! We'll find a bunch of lists, give a thing
in each list, and extract using the penultimate letter of the penultimate thing each list."

And, in fact, the puzzle idea did write itself! Well, the idea did. The execution took a while to
hammer out. A rule of thumb
is that [there's a 10:1 ratio](https://www.ybrikman.com/writing/2018/08/12/the-10-to-1-rule-of-writing-and-programming/)
for raw materials to final product in creative endeavors, and that held true here too. The final puzzle
uses 16 feeders, and this was sourced from around 140 different lists.
Our aim was to balance out the categories used, which specifically meant not all music, not all
literature, not all TV, and not all things you'd consider a well-known list (like the eight planets).
Lists were further filtered down to interesting phrases that
ideally wouldn't need to be spelled letter by letter, while still uniquely identifying their list from
a single entry. The last point was the real killer of most lists. I liked [Ben Franklin's list
of 13 virtues](https://fs.blog/the-thirteen-virtues/), but the words ended up being too generic.

Despite having the entire world as reference
material, some letters (especially the Ps) were really difficult to find. I remember arguing
against SOLID YELLOW for a while, saying it was ambiguous between "green stripe" and "striped green"
no matter what Wikipedia said, but didn't find a good enough replacement in the 20 minutes I spent
looking for an alternative, and decided I didn't care enough to argue more.

I feel every puzzle author tries to shoehorn their personal interests into a puzzle, and this
was a fun exercise in trying to do so.
[FISH WHISPERER](https://vyletpony.bandcamp.com/album/can-openers-notebook-fish-whisperer) did not
make the cut, but I knew it had zero chance of clearing the notability bar.
[MY VERY BEST FRIEND](https://www.google.com/search?q=madoka+magica+episodes) was a funny answer
line that got bulldozed in the quest to fit at least one train station into the puzzle, which was
harder than you'd think. Not a great showing for stealth inserts, but I'm happy
[WAR STORIES](https://en.wikipedia.org/wiki/Firefly_(TV_series)) stuck around until the end.

Also, have a link to some [Santa's reindeer fanart and fanfiction](https://holidappy.com/holidays/The-Personalities-of-Santas-Reindeer)
discovered during testsolving. Testsolvers cited it as a source for "Olive is Santa's 10th reindeer",
a mondegreen from people who misheard the song as
"Olive the other reindeer used to laugh and call him names".

![A conversation about reindeer](/public/mh-2023/reindeer.png)
{: .centered }
</div>


# Early April 2022

## Round and Round and Round and Round!

![Wyrm](/public/mh-2023/wyrm_avatar5.png)

It's Wyrm time!

Wyrm took *quite a while* to come together, but was started in earnest around April.
From the start, the round proposal was "really cool fractal art", and the design around it
was figuring out what an infinitely zooming fractal round could look like. This started with
the metameta.

<div class="shaded" markdown="1">
### Period of Wyrm

Really, I did not do much on this puzzle. The mechanics stayed the same throughout all
testsolves. My main contributions were helping on feeder search during round writing,
and writing a script to auto-search for equations that would give a desired period. I limited
the search to linear functions, which was good enough most of the time.

I learned a lot about how Mandelbrot periods work during this puzzle. Although we suspected that
writing code would be the way most teams solved this puzzle, we wanted the puzzle to support
non-coding solutions, given it was required to finish the Hunt. The way that Mandelbrot
set periods work is based on the "bulbs" along the outer border of the Mandelbrot set. The central
cardioid has period 1, and you can find any period by finding the right bulb connected to
the cardioid. Each bulb is self-similar to the original set, so instead of only going around
the central heart, you can use the bulb of a bulb. For example, to get a period of 6, you either
use a 6-bulb, or a 3-bulb branching off a 2-bulb, or a 2-bulb branching off a 3-bulb. Long story
short, composite periods are easier than prime ones.

Qualitatively, the period converges faster if the point is towards the middle of the bulb, so
we tried to do that when possible.
We also aimed to use the largest bulb per period to reduce precision needed to the solve the puzzle,
and spread the points across the border of the Mandelbrot set (a holdover from
an earlier version of the puzzle that hinted the Mandelbrot set less strongly).

The puzzle was very deliberately designed to be flexible enough for any answer, as long as we
had a good enough set of feeders.
</div>

The round structure went through multiple iterations, done over Jamboard. Here's a version where
every puzzle would be 1/4th of a future puzzle:

[Wyrm brainstorm](/public/mh-2023/Inset Jam 6.png)
{: .centered }

Here's one where every puzzle's answer would depend on answers from the previous layer,
such that one puzzle could be backsolved per layer.

[Wyrm brainstorm 2](/public/mh-2023/Inset Jam 3.png)
{: .centered }

And here's one where the entire round would be serial, each puzzle would rely on the
previous puzzle's answer, and you'd need to figure out how to bootstrap from nothing to solve
the entire round.

[Wyrm brainstorm 3](/public/mh-2023/inset_jam_line.png)
{: .centered }

Most of these ideas would have been quite tricky to pull off, especially given we needed to
fit it within the Period of Wyrm constraints. This led to the cyclic round structure proposal.
Each layer would be normal puzzles, building to a meta, which would then be 1 pre-solved feeder
for the next layer's meta. The rounds would then form a cycle, where the last meta would be a feeder
in the first layer. This restricted the "weirdness"
to just the metas of each layer, and all regular feeders in each layer could be written without
constraints besides the answer. The zoom direction of moving outwards rather than inwards was
done to make it more distinct from [⊥IW.giga](https://puzzles.mit.edu/2021/round/giga/).

[Wyrm last brainstrom](/public/mh-2023/Inset Jam 2.png)
{: .centered }

Our first plan was to have one unsolvable puzzle in the first layer of puzzles, that would
become solvable once you got to the last layer of puzzles. This idea got discarded pretty
early because it didn't feel very impactful, it seemed hard to guarantee that a puzzle couldn't
be backsolved, and giving teams an unsolvable puzzle would be pretty rude. That led to the
road of creating a metapuzzle disguised as a feeder puzzle, solvable from 0 feeders but still
allowing for backsolving of feeders. Figuring out exactly what that meant would be a future problem.

Since I didn't have any leadership responsibilities, and tech was still on the slow side,
I ended up self-assigning myself a lot of work in brainstorming metas that fit the answer
constraints.
I noted that our Hunt had a lot of similarities to Mystery Hunt 2018:
a goal to reduce raw puzzle counts, but including complex meta structures in their stead.
As homework, I spent a lot of time reading through the solutions for
both the Sci-Fi round and Pokemon rounds from Mystery Hunt 2018, since they also had
overlapping constraints between metas and metametas. Going through each solution several times,
I started to appreciate some 2018 metas that I found dull at the time, but which made the round
construction possible when viewed through a constructor's lens.

You can read more about the Wyrm answer design process in the AMA reply here, but in short,
metas range between using answers semantically and using answers syntactically. Since the metameta
forced semantic constraints, almost all metas were based on brainstorming syntax based ideas.
The first step was generating a list of categories for the metameta, to find answers that
could also be good meta answers. This ended with a list of around 60 categories, of which 13 were used in the end.
An early search turned up FELLOWSHIP = FELLOW SHIP for an
answer that naturally lent itself to a pun, and INCEPTION as a good "teaser" answer for
the rest of the round. From there, all the metas were written simultaneously, keeping an
eye out for what categories a meta wanted to use, and trying to reduce the number of
half-used categories as much as possible. For example, if the ship meta wanted to
use MONTGOMERY BURNS from the "Socialist" category for the USS Montgomery, then we'd try
to fit TODD DAVIS into a meta to reduce the spaghetti
needed at the end.

Now, the example I just gave didn't actually get used, because we decided TODD DAVIS was too
ambiguous, and large numbers forced awkward equations in the meta. INCEPTION got a free pass because
it was too good of an answer, but we tried to keep the remaining numbers under 100. Too bad,
since "Socialist" was my favorite category that didn't make the cut. The only category that
was treated as required was Hausdorff, since we believed it was a key hint towards the
metameta. (At the time, the only hint towards Mandelbrot Set was the flavor text and
round structure.) Factchecking that particular category was a fun time.

> aw man why have so many recreational math people tried to estimate the dimension of brocolli and cauliflower
>
> [their] values are like +/- 0.2 the value from wikipedia
>
> but that value is based on some paper someone put on arxiv in 2008 with 4 citations

> put some notes in the sheet but in summary, of the real-world fractals, the most canonical ones are
>
> 1) the coastline based ones, because they were so lengthy that only 1 group of people really bothered estimating them.
>
> 2) "balls of crumpled paper", which is usually estimated at dimension 2.5 and I found a few different sites that repeat the same number (along with 1 site that didn't but the one that didn't was purely experimental whereas the wikipedia argument is a bit more principled)

After more investigation, I found that the Hausdorff dimension of coastlines is only really well-defined
for Great Britain, where both the original paper by Benoît Mandelbrot and almost all online sources
agree it's 1.25-dimensional. Every other coastline in Wikipedia's list relied on a Springer book from
1988 that was hard to verify, and online reproductions of the dimensions of Ireland, Norway, etc. gave
different values than that book. Which was quite annoying, because it meant our uncuttable category had
a mandatory answer.

In my experience, factchecking is the most underappreciated part of the puzzle writing process.
The aim of factchecking is to make sure that every clue in the puzzle is both true, and only
has one unique solution. And even with the solution, this can take a long time to verify, on par with
solving the puzzle forward.
Although Wikipedia is the most likely source for puzzle information, Wikipedia
isn't always correct, and it's important to verify multiple sources share consensus, because you never
know what wild source a puzzler will use during Hunt.

(Sometimes, that consensus can be wrong and you still have to go with
it for the sake of solvability! See Author's Notes for
[Hibernating and Flying South](https://2021.galacticpuzzlehunt.com/puzzle/hibernating-and-flying-south)
from GPH 2022. It's unfortunate to propagate falsehoods, but sometimes that's how it goes.)


Mid April 2022 - The First Writing Retreat
------------------------------------------------------------------------------------------

The first writing retreat was not a team-wide gathering. The logistics of gathering everyone was hard,
and the COVID situation around April was also tenuous in many ways. We decided that instead, we would
have smaller retreats around geographic hubs, one of them in the Bay Area. The aim was partly to
do general puzzle writing, and partly to meet people on the team who lived nearby.

We started brainstorming the start of Weaver at this retreat. The full story will come later, but this
was the first time where Brian mentioned wanting to make an underwater basket weaving puzzle, using
special ink that dried white and became transparent when wet. The idea sounded **awesome**, so we did
some brainstorming around what the mechanics should be (different weaving patterns, presumably), as
well as some exploration into the costs. Then we came across an [Amazon review by one Daniel Egnor](https://www.amazon.com/gp/customer-reviews/R2HQQ7DWE56RY1/ref=cm_cr_dp_d_rvw_ttl?ie=UTF8&ASIN=B086Q344PQ).

> I've tried a bunch of hydrochromic paints and they're all kinda like this one. It's a fun idea in theory -- a paint that goes on white when dry and turns clear when wet, so you can reveal something fun on your shower tile or umbrella or sidewalk.
>
> But... it doesn't work great. It takes a pretty thick set of coats to actually hide (when dry) what's underneath, and that makes it prone to cracking, and also not entirely transparent (more like translucent) when wet. It's hard to get the thickness just right. Mixing some pigment into the hydrochromic helps a bit but adds a tint when wet. And even aside from all that it's not very durable paint, it's kind of powdery and scratches off. And you can't add a top-coat, otherwise the water won't get to it.
>
> You *can* make it work, we *did* make it work for a puzzle application (invitation cards that reveal a secret design when wet) but I'd prefer not to use it again.

For those who don't know, [Puzzle Hunt Calendar](http://puzzlehuntcalendar.com/) is run by Dan Egnor.
I assume they're the same person. This was possibly the most helpful Amazon review of all time, and
it suggested our idea was dead in the water (pun intended).

Still, underwater basket weaving was too compelling to discard entirely, so Brian ordered some to
experiment with later.

We then moved on to writing a draft of another puzzle:

Broken Wheel
----------------------------------------------------------------------------------------------

This is one of those puzzles generated entirely from the puzzle answer. There were a few
actually-serious proposals about treating the answer as PSY CLONE and doing some Gangnam Style
shitpost, but, I mean, there's already been one Gangnam Style Mystery Hunt puzzle, it doesn't need
a second.

Alright, then what is a Psyclone? There are two amusement park rides named the Psyclone, one of which is a spinning ring.
How about a circular crossword that spins?
There were some
concerns about constraints, but I cited [Remy](https://puzzles.mit.edu/2022/puzzle/remy/) from
Mystery Hunt 2022 to argue that it'd be okay to not check every square of the crossword.
This is the kind of puzzle that was easy to construct in parallel. I guess that shouldn't be
surprising, since crosswords are easy to solve in parallel too.

Enumerations were added in the middle of the first testsolve because it was too hard to get
started without them. As for the final rotation, we went through many iterations of
flavortext and clue highlighting, before settling on placing the important clue first and
mentioning "Perhaps they can be rotated" directly in the flavortext. "Rotated"
in particular (over "spin" or "realigned") seemed to be the magic word that got testers on the
right idea.


Late April 2022 - "I Have a Conspiracy"
------------------------------------------------------------------------------------------

By now, the general shape of the AI rounds had been picked: Wyrm, Boötes, Eye, and Conjuri.
Also around this time, the story team was brainstorming how to convey the midpoint capstone of the
hunt, where solvers would reactivate old AIs, teammate would shut down Mystery Hunt, and solvers would
repower the factory by solving scraps of puzzles leftover in the factory post-shutdown.
(This Puzzle Factory runs on puzzles, after all!)

At the same time, the Wyrm round was significantly larger than all the other AI rounds.
In one general meeting, the Wyrm round
authors and Museum metameta authors were gathered into a meeting with the editors-in-chief and
creative leads for a conspiracy: what if Act 1 feeders from the Museum repeated in Wyrm's round?

This would solve two problems at once:

* The time spent in Wyrm's round for Act 3 could be cut by 6 solves, bringing it in line with the
other AI rounds.
* The reused feeders could become the story justification for puzzles that teams solved after shutdown
to start bringing power back to the Factory.

Wyrm was also the best AI round for reusing feeders, since Boötes and Eye had answer gimmicks and
Conjuri would not be able to finalize its answers until the game was more developed. It would also
have other benefits:

* The overall hunt would require 6 fewer puzzles to write. Puzzle production was starting to fall
under the target trendline of all puzzles written by December, and reducing feeders was one way to
catch up.
* If we could make that set of feeders fit 4 meta constraints (Museum meta, Museum metameta, Wyrm
meta, Wyrm metameta), it'd be really cool.

Making this happen would be quite hard. We needed to make a call on whether this
was ambitious-but-doable, or too ambitious. We decided to go for it, with a backdoor of letting ourselves
exit early if it ended up being impossible.

Although I'm happy we pulled it off, given another chance I would have argued against this.
First of all, I don't think many solvers really noticed the overlapping constraints. It leaned too
hard towards "interesting design problem" without a corresponding "fun" payoff. The more problematic
issue that I only realized in retrospect was what it did to feeder release.
To recap, here is the state of Hunt by this point:

* The Office meta is done and its feeders are released.
* The Basement meta is on a final double-check round of testsolving, but its feeders have already
been released and this is just to verify some minor edits.
* Innovation and Factory Floor is doing its own crazy thing, and won't be ready for a while.
* All AI rounds are in the middle of design, but it's already known that Bootes and Eye will have
answer gimmicks that make their puzzles harder to write, and Conjuri feeders will likely be
released last because the meta relies on developing the game.
* The Museum metameta is roughly done, but the Museum metas have yet to be written.
* The Wyrm metameta is roughly done, but the Wyrm metas still need to be written.

By connecting feeders between Act 1 and Act 3, we were creating a dependency between the
5 Museum metas and 4 Wyrm metas. Including a retest of the 2 metametas after feeders were locked
down, that's **eleven** metas blocking answer release. (Technically, ten metas, since Collage
could be assembled after the other 3 Wyrm metas as long as all three were finalized.)
All of those metas
needed to get to a testsolved state before it would be safe to release any of their feeder answers.
There were 53 feeders in that pool, which is approximately 40% of the feeders in the entire Hunt
and a majority of the non-gimmicked feeders.

I'd estimate that the extra design constraints delayed the release of that pool of 53 feeders by 2-4 weeks, and this
wasn't time we had. Perhaps in a more typical Hunt, this would have been fine, but the AI rounds
had already spent a lot of complexity budget and this may have pushed it over. Instead of reusing
feeders, we could have use silly incomplete puzzles like 2x2 Masyus, and still gotten the benefits
of six fewer AI round puzzles with less work and fewer design complications.

Hunt writing tends to follow a [Zipfian distribution](https://en.wikipedia.org/wiki/Zipf%27s_law).
A small number of people will write many puzzles (or do a lot of leadership work), and a long tail
of people will write a small number of puzzles each. Usually, that long tail is where all the
new puzzle writers are. Put yourself in novice puzzle writer shoes. Do you want to write a puzzle
where whitespace and capitalization matters, a puzzle that must be closely tied to a language, or a normal
puzzle? Unless you're really excited about an idea for the gimmick, you'd probably prefer a normal
puzzle answer. If you didn't get in on the two rounds that had feeders released, then you had to
wait.

This isn't quite as bad as I'm making it out to be.
There's useful work that can be done without knowing the puzzle feeder. In general though, if you
have a feeder from the start, you'll do less redundant work, it's easier to stay motivated if you're
expected to pull through on a puzzle, and you can brainstorm ideas from the puzzle answer rather
than trying to generate one from the ether.

But, this is all said with hindsight. At the time, I did not realize the consequences and I'm
not sure anyone else did either. Adding more complexity to round structures mostly seemed like a teammate thing to try doing, and we told
ourselves that if we found it too hard we'd leave the option of reverting back if we couldn't make
it work.

People say "restrictions breed creativity", but what they don't say is that it's not necessarily
*fun*. The process of finding feeders that fit both Wyrm and Museum metameta constraints took a
long time, but interestingly writing The Legend around those feeders wasn't so bad.


The Legend
-----------------------------------------------------------------------

Before
deciding to share feeders, I had sketched some ideas around using the Sierpinski triangle, after
noticing INCEPTION was $$9 = 3^2$$ letters long. The shape is most commonly associated with
Zelda in pop culture, so ideas naturally flowed that way.

![Prototype Legend triangle](/public/mh-2023/triangle.svg)
{: .centered }

The early prototype was reference heavy and not too satisfying, extracting one letter per
triangle. After talking with Patrick a bit, he proposed turning it into a logic puzzle, by
scaling up to 27 triangles, and cutting out all the intermediate steps. This was especially
appealing because it meant we could take almost any feeders, as long as their total length
was close to around 60. Around this time, Brian mentioned that TRIFORCE was a plausible answer
for both the Museum and Wyrm metameta, so if we could make the Sierpinski idea work,
everything would come together nicely for prepping the infinite loop.

This was all great, with one small problem: I'd never written a logic puzzle in my life.
There are, broadly, two approaches to writing a logic puzzle.

1. Start with an empty grid. Place a small number of given clues, then solve the logic puzzle forward
until you can't make any more deductions. Add the clue you wish you had at that part of the solve,
then solve forward again, until you've filled the entire grid. Then remove everything except the
givens you placed along the way.
2. Implement the rules of the logic puzzle in code, and computer generate a solution.

Option 1 tends to be favored by logic puzzle fans, because by starting from an empty grid you
essentially create the solve path as you go. This makes it easier to create a novel interesting
solve path. The computer approach tends to create what puzzle snobs call "computer generated crap".
(See the many books of 500 random Sudokus that took two minutes to create and feel identical after
the 10th one.) It may tend to be less interesting, but it was also *much* faster, and I knew I
was going to eventually want a solver to verify uniqueness. So I went with option 2.

I started with [grilops](https://github.com/obijywk/grilops), then realized it didn't support
custom grid shapes, so I ended up implementing the solver myself in Z3, a constraint satisfaction
library that grilops uses.
I then needed to figure out how to represent
the Sierpinski triangle grid in code. After some exploration I figured out a very
satisfying coordinate system. Let 0, 1, 2 be the top, left, or right corner of the triangle. Then
a single letter could be described by the path you took at each level of the Sierpinski triangle,
starting from the biggest triangle and working inward.

(explanatory diagram)

With 27 triangles, each letter would be at $$(a, b, c, d)$$, and then to check if two letters
are neighbors, we can also check this recursively.

* If $$a$$ do not match, then it's enough to check the neighbors of the largest triangles.
* If $$a$$ matches, then we can recursively check if $$(b, c, d)$$ are neighbors in the smaller
Sierpinksi triangle.

(diagram 2)

The first draft of my code took 3 hours to generate a puzzle, and the uniqueness check failed
to finish. Still, when I sent it to other authors they found the same solution,
so we sent it to testsolving to get early feedback while I worked on speeding up my solver.

Testsolving went well. The first testsolve took pretty much exactly as long as we wanted it to
(2 hours with 5/6 feeders), and solvers were able to use the Sierpinski structure to logic out
deductions that combined into the final grid. Not too bad for a computer generated
puzzle! This was a case where we "got lucky", and discovered a solve path that felt like
a designed one.

I found a way to describe the letter constraints in Z3 that made the solver run 10x faster
(more precisely, I stopped describing constraints in a dumb way), and played around with the
feeders until I got a solution that verified as unique. There were around 5 different fills,
and I suspect all of them were unique, but I was only able to get my code to halt on one of them
and didn't particularly feel like trying to prove the rest.

After the first draft, there were only two revisions. The first was deciding how much hinting to give
towards the Sierpinski triangle, since that was the step with the largest leap of faith. In the
end we decided to hint that all triangles should stay upright, and the final shape should
be triangular, leaving the rest as an a-ha to find.

The second isn't really a revision, but it's close. Although the penny puzzle from Mystery Hunt 2020
was quite painful, we liked that in end we ended up with a bunch of small souvenirs that people could
take home. Team leadership was not sure if Mystery Hunt would be on-campus or not, but if it was,
it seemed cool to replicate the same experience. Much later, when we got the go for on-campus,
the leads for physical puzzle production started estimating costs for laser cutting wood, and making
prototypes to figure out how long it'd take to make enough sets. The actual laser cutting happened
in December. It would have happened earlier, except the person who ordered the wood had their
shipment of wood stolen from the mailroom and had to order a replacement. (Turns out stealing wood is...a thing
people will do? The more you learn.)

I'm not sure if teams use the wooden triangles as a souvenir in the way we imagined, but I hope shuffling
wooden triangles was more fun than manipulating spreadsheets!


May 2022 - The Triangles Will Continue Until Morale Improves
--------------------------------------------------------------------------

Museum and AI round development is fully underway. The EICs have created a "bigram marketplace" puzzle,
which is really just a placeholder puzzle to make it easy to create a Discord channel that's only open
to certain authors. The bigram marketplace held every bigram used by the Museum metameta (MATE's META),
along with a sketch of expected bigram extraction mechanisms. The repeated feeders in Wyrm were given
top priority on claiming bigrams, and as Museum metas were proposed, the EICs compared their constraint
levels to decide which got first rights to bigrams, and which needed to absorb the repeated feeders.
I field questions from Museum meta authors. (Yes, someone needs to take the answer GEOMETRIC SNOW, we
know it's not a great answer but haven't found an alternative that fits the double O constraint.
Sorry, TRIFORCE can't change. Oh, this feeder isn't as tightly constrained, here's five options pick one.)

As part of this process, other Wyrm authors and I were fasttracked into first testsolves of
every Museum meta draft, so that we could learn their feeder constraints without wasting our lack of knowledge.
Meanwhile, testsolves of The Legend were biased towards Museum meta authors so that they could understand
what we were doing. (The same was not done for the Wyrm metameta, because it already had two clean tests
and we didn't want to do extra work.)

As the bigrams settle down and The Legend feeders get more locked in, we start looking at how to
make our own constraints work...

The Scheme
--------------------------------------------------------------------------

This was the 3rd Wyrm meta to get drafted. Lost at Sea had been tested, and although revisions were
planned, EYE OF PROVIDENCE was locked in as the target answer for The Scheme.

The original idea for The Scheme was based on an idea that I quite liked. We weren't able to make it
work, partly because we didn't have the right skills in the construction group, but it's good enough
that I don't want to reveal it.

After that idea fell apart, we noticed that the Eye of Providence was a triangle, and we had triangles
in The Legend, so why don't we try to make this triangle Wyrm meme a reality? It wasn't a hard
requirement, but it would be cool...

When researching Mystery Hunt 2018 to get ideas for constrained metas, I found
[Voltaik Bio-Electric Cell](http://puzzles.mit.edu/2018/full/puzzle/voltaik_bio_electric_cell.html),
a triangular meta that mostly used feeder length, with the rest coming from shell. Well, we already
had a 1 letter word in one of our feeders (at the time, it was V FOR VENDETTA). It seemed plausible
we could make a full triangle out of words in our feeders, and a length constraint was light enough to not
put too much pressure on the semantic categories from the metameta.

We did a search for missing feeder lengths, made a version that picked letters out of the triangle with
indices, and sent it for editor review. I was fully expecting it to get rejected, but to my surprise
the editors for this puzzle thought the word triangle was pretty elegant, and only had token comments
on the puzzle draft. (Perhaps the more accurate statement is that the editors knew what the Wyrm constraints
were, most metas we'd proposed required a hefty shell, and this was one of the purer meta proposal we'd
managed for Wyrm so far.)

The spiral index order was originally added because we were concerned a team could cheese the puzzle by
taking the indices of all 6! orderings of the feeders. Doing so wouldn't give the answer, but it seemed possible
you'd get out some readable partials that could be cobbled together.
With hindsight, I don't think that was actually possible, but one of our testsolve
groups did write code to bash all 6! orderings, so it was definitely worth considering.
We kept the spiral in the final version because it let the arrow diagram serve two purposes: the ordering of
the numbers, and a hint towards the shape to create.

DIAGRAM

In Puzzup, there were a list of tags we could assign to puzzles, to help editors gauge the puzzle balance
across the Hunt. This puzzle got tagged as "Australian", which is shorthand for "a minimalistic puzzle with one key idea,
where before you have that idea nothing makes sense, and after that idea you're essentially done." Our team
uses that shorthand because puzzles like this tended to appear in the Australian puzzlehunts that used to
show up every year (CiSRA / SUMS / MUMS). One of the tricky parts of such puzzles is that you get exceptionally
few knobs to tweak difficulty. Basically all we got was the diagram and the flavortext.
The other tricky part is that solve time can have incredible variance. The first test got the
key idea in 20 minutes. The second test got horribly stuck and was given four different flavortext and diagram
revisions before finding the idea 3.5 hours later.
The second group did mention the meta answer an hour before the solve, in one of the
best examples of dramatic irony I've ever seen.

(add the Discord screenshot here)

Maybe having an Australian puzzle as a bottlenecking meta was a bad idea. A high variance puzzle naturally means
some teams *will* get it and some teams *will* get walled, and getting walled on a bottleneck is a sad time. This is
on my shortlist of "puzzles I'd redo from scratch with hindsight", but at the time we decided to ship it so we could move on to feeder release. ["You get one AREPO per puzzle"](https://docs.google.com/presentation/d/166MkkDmij_4_XcA8JP8JcSztVPjHJeibDJxr9EUMEv4/edit#slide=id.p) - I'd say this is the one AREPO of the Wyrm metas.

Even in batch testing, where teams solved The Scheme right after The Legend, none of our testsolves consider
using the arrangement from The Legend when solving this puzzle. A few teams got caught on this during the Hunt -
sorry about that! The fact that The Legend triangle had an outer perimeter of 45 was a complete coincidence,
and if we'd discovered that early enough it would have been easy to swap BRITAIN / SEA OF DECAY back to GREAT BRITAIN / SEA OF CORRUPTION to make the triangle 55 letters rather than 45.


Lost at Sea
-----------------------------------------------------------------------------

Nominally, I'm an author on this puzzle. In practice I did not do very much. The first version provided the
cycle of ships directly, and tested okay (albeit with some grumbling about indexing with digits).
By the time I joined, the work left to do was finding a way to fit
in triangles, and finding answers suitable for the metameta.

I looked a lot into the Bermuda triangle, but didn't find any reasonable puzzle fodder. So instead, we looked
into triangular grids. Over a few rounds of iteration, the puzzle evolved into seeding a triangular
[Yajilin](https://en.wikipedia.org/wiki/Yajilin) puzzle where feeders were written into the grid, black triangles
gave a cluephrase hinting towards digit indexing, and ship information from the feeders gave direction for
the Yajilin arrows.

It all sorta worked, but the design was getting unwieldly and we had a really hard time finding a way to
clue all the mechanics properly in the flavortext. There were lots of facts about each ship, so the longer
we made our flavortext, the more rabbit holes testsolvers considered. (It's the natural response: if you
get more data, then maybe you need to research more data to solve.)
I'd say my main contribution was suggesting we try cutting the Yajilin entirely and brainstorm something else.
The final version of the puzzle is the result of that brainstorm. I'm pretty happy with the way the puzzle
guides towards the hull classification, self-confirms it with the USS Midas (ARB-5) thanks to A and B flags
being weird, and then leaves the classification number suspiciously unused if you haven't figured out they're
important.

The other main contribution was helping find feeders. The first draft used MRS UNDERWOOD as a feeder, which
*worked* but was really not a good answer. The S in MRS was needed for extract, but it looked so much like
a cluephrase for Claire Underwood. Brian told me that it could be fixed if we added a feeder that clued
San Francisco, which fit the metameta, and had an S as its 5th letter. We collectively spent 5-10 hours
doing a search for one, before landing on the "stories" idea with SALESFORCE TOWER or
TRANSAMERICA PYRAMID. The puzzle got rewritten around SALESFORCE TOWER, went through an entire testsolve
with zero issues,
and then we learned that [actually there are multiple Salesforce Towers](https://www.salesforce.com/company/ohana-floors/salesforce-tower-atlanta/) in a final runthrough of the metameta. Oops. Thank goodness we had
a backup answer!


June 2022 - It's Collaging Time
-----------------------------------------------------------------------------------

Wyrm feeders are almost ready for release. We've made the feeder constraints work, and every meta has gotten two clean
testsolves. There are just two action items left:

1. Write the 4th Wyrm meta.
2. Do a batch solve of the entire round to get data on what it's like to solve each meta sequentially.

We had multiple meetings about the 4th Wyrm meta starting all the way back in March.
The design requirements were pretty tight:

* The puzzle had to look like an regular Act 1 puzzle.
* It also needed to be interpretable as a metapuzzle.
* Feeders had to be used in enough of way to allow for backsolving.
* At the same time, the puzzle had to be solvable without knowing any of those feeders.

The very first example for what this could look like was a [printer's devilry](https://en.wikipedia.org/wiki/Printer%27s_Devilry) style puzzle. Instead of each clue solving to the inserted word, inserting a word would complete a crossword clue with its own answer that you'd index from. The inserted words would then be the backsolved feeders.

None of the authors were too excited by this, but it was important to prove to
ourselves the design problem was solvable.
we came up with it in March mostly to convince ourselves
the design problem was solvable). Once we knew the answer was TRIFORCE in April, the idea died more officially
because we could not longer do a 1 feeder -> 1 letter idea. Another idea proposed for this puzzle
placed too many constraints on the feeders given the Wyrm metameta - that idea eventually turned into
[Word Press](https://puzzlefactory.place/factory-floor/word-press).

One of the challenges was that the puzzle needed to embed some process for backsolving. But, that backsolving
process might look like unused information during the initial forward solve, which could turn into
a rabbit hole if teams got stuck. The round structure only really *worked* if almost all teams solved
the Act 1 puzzle by the time they got to the end of Wyrm.

In late-April, we came up with the word web idea, which I immediately started advocating for. Word webs
are the closest thing to guaranteed fun for puzzlehunts, and were a highlight of the now-defunct Google
Games hunts run for university students.
It solved all our backsolve problems because spamming guesses in a word web is just what you're supposed
to do, and there was a natural way to solve around unknown nodes and guess them later. My main worry was
that it'd take a while to construct.

It was clear we *could* construct it though. We punted on doing so until the backsolve feeders were
more certain. Well, now they were. Time to reap what we'd sown.


Collage
-------------------------------------------------------------

The first thing I did was reach out to David and Ivan to get the code they used to build [Word Wide Web](https://2020.teammatehunt.com/puzzles/word-wide-web). David ended up sending me a D3.js based HTML page that
automatically layed out a given graph, with some drag-drop functionality to adjust node positions.
I repurposed that code to create a proof-of-concept interactive version that ran locally. This quickly
exposed some important UX things we'd want to support, like showing past guesses and allowing for some
alternate spellings of the answer.

![Prototype of filled out web](/public/mh-2023/webprototype1.png)
{: .centered }

![Prototype of solvable web](/public/mh-2023/webprototype2.png)
{: .centered }

For unlocks, I decided to always recompute the web state whenever the list of solved words changed.
This wasn't the most efficient, it could have been done incrementally, but in general I believe
people underestimate how fast computers can be. Programmers will see something that looks like
a software interview question, and get nerd sniped into solving that algorithms problem, while neglecting
to fix their page loading a 7 MB image file. (And, the simpler the data, the easier it is to
*recognize* it's an algorithms question, and the less likely it is to matter.)
I figured recomputing the entire graph would be more robust,
and didn't want to deal with errors caused by incremental updates failing.

![Advice on building the most robust thing](/public/mh-2023/braid.png)
{: .centered }

(From ["The Implementation of Rewind in Braid"](https://www.youtube.com/watch?v=8dinUbg2h70), a talk by Johnathan Blow)
{: .centered }

From here, I worked on creating a pipeline that could convert Google Sheets into the web layout code. The
goal was to remove tech from the flow of updating the word web, to make it easier to collaborate on.
I added a bunch of deduplication and data filtering in my code to allow the source-of-truth spreadsheet
to be messy, which paid off. Pretty sure around 10% of the edges appear twice in the raw data.

![Web spreadsheet](/public/mh-2023/webspreadsheet.png)
{: .centered }

The process of coming up with the words themselves took a while. Collage is the first time I've
ever done this kind of thing, but my assumption going in was that word webs are best when there's a
high density of edges and vertices have large average degree. To get a sense for how big the web needed
to be, I took the [Black Widow](https://puzzlepotluck.com/3/14) web from Puzzle Potluck 3 and collected
some stats on number of vertices and edges to figure out the average degree I wanted to hit. Whenever I loaded
a web into my code, I printed out stats on the graph, as well as how many leaf nodes there were, with
the goal of having as few as possible. I gave very serious though to dusting off my [spectral graph
theory](https://en.wikipedia.org/wiki/Spectral_graph_theory) to estimate if the web was an
[expander graph](https://en.wikipedia.org/wiki/Expander_graph), but decided that was overkill.
There was an initial "expansion" phase, where I put down literally anything I could think of starting
from the backsolve feeders, and asked other authors to do the same. Then, after the web grew enough to
have some collisions occur naturally, there was a "contraction" phase where I trimmed nodes that were
hard to connect to the rest of the web, and tried to more directly brainstorm topics that could be embedded
in the graph without adding too many new words. I'd estimate about 80% of the web is from me, and that's why
there's so much My Little Pony and MLP-adjacent material in the web. I apologize for nothing.

After the prototype got tested a bit, it was time to get it into the site for real. I'd written
interactive puzzles before, but this was my first time implementing websockets for a puzzle, as well as
using D3.js within React. (Websockets are what allow the puzzle to display updates when another solver
on the team guesses a word. They're also how all the AI chat messages are managed.) I knew that I
wanted to use websockets to sync team state on Quandle, and I wanted the zooming code in D3.js for
the Wyrm round art, so I treated it as an investment that would save time later.

Thanks to us using a similar codebase as previous Teammate Hunts, it was straightforward for me to find
previous websocket code in old teammate puzzles, and then implement something similar. Very broadly,
the backend uses [Django Channels](https://channels.readthedocs.io/en/stable/) as a websocket
management layer, handling team authentication and routing different websocket URLs to different puzzle endpoints. The frontend then defines helper functions to support connecting to either a user-specific channel,
team-wide channel, or hunt-wide channel (although I don't think we ever used the hunt-wide channel).
The main danger of using websockets is that they widen the set of things-that-can-go-wrong in the Hunt
site, but given that we were already relying on chats with MATE as a key part of Hunt, we were going to
eat that risk regardless.

Polishing and factchecking this puzzle was a bit of a nightmare. It turns out graph layout is a really hard
problem, and even after tuning D3.js force graph parameters a bunch, I needed to do several adjustments
by hand to clean up collinear points and reduce overlaps. The click-to-highlight feature was a concession
to the fact that getting to 0 overlaps was impossible in the given time. As for factchecking, the graph has over 300 words
and I knew there was no way we were going to exhaustively verify all O(N^2) pairs of those words.
I think we got most of them, but a few slipped through ("the princess bride" is somehow not connected to "bride").

I was happy to see
that no one who testsolved Collage suspected a thing. Even during Hunt, most teams successfully got past
the zero starting word hurdle without thinking too much of it. Thanks to Patrick for suggesting a hardcoded
threshold of 90%, rather than revealing the goal node when its 3 neighbors were found, since the neighbors were usually revealed within the first 15% of the solve.

\*\*\*

With all the Wyrm metas at first draft, we did a final batch test of all the Wyrm metas and the metameta.
We debated back and forth on whether there was a way to have teams testsolve Collage when it was advertised
as a "test of all the Wyrm metas". If we didn't test it before feeder release, and something went wrong
in the true test, then we'd have a huge problem. But the only test of it that seemed like it'd
accurately model the real Hunt would be having teams do testsolves of many random puzzles including
Collage before they tested Wyrm metas. Those random puzzles didn't exist...because they were waiting
on feeder release, which was waiting on testing the metas, which really wanted the random puzzles to
exist, which...you get the idea.

Who knew making an ouroboros-style round would make it hard to find a starting point?

There were arguments both ways, and the decision was to not test the gimmick. We did not have distractor
puzzles to hide Collage, we didn't want to wait for real puzzles to get written to have distractors,
and we certainly didn't want to write fake puzzles when we could spend the time writing real puzzles.
The batch test group tested the Wyrm metas in sequence, starting with 4 feeders each and occasionally getting more
if needed, and when they got to the metameta, we revealed the round gimmick and gave them the full set of
answers.

![Wyrm structure](/public/mh-2023/wyrmstructure.png)
{: .centered }

We did redact the title of Collage to something else, because there was no reason to reveal the name
of the looping puzzle. The meta names used to follow a pattern of "The Legend", "The Scheme",
"The Sea", and "The Collage" (redacted to "The Sage"). We ended up dropping the "The" pattern since we
were concerned it might be too much hinting. If a team got suspicious of looping, and identified the
looping puzzle before unlocking the blank puzzles, then they could solve the 3rd Wyrm meta immediately on
unlock, which seemed like far too much cheesing. But we kept the pattern in mind as a potential nerf to make down
the line.

The metameta test passed, and we were able to release all the Wyrm feeder answers. The Museum metas
were finished shortly, letting us release a bunch of feeders before the in-person writing retreat. The
retreat got scheduled for late-August. The goal was to get together, test drafts of physical
puzzles and events, and start puzzle drafts for AI round puzzles that we expected to be hard to
write solo. We were also planning to do a team solve of a good chunk of Act I, so it was pretty
important to get Wyrm metas and Museum metas out the door early enough to give time to construct
those Museum puzzles.

However, the true goal of the in-person retreat was a more closely-held secret. And I was in the thick
of that conspiracy.


July 2022 - It's Breakout O'Clock
-----------------------------------------------------------------------------------------------

"Breakout" was our internal name for what would eventually become the Loading Puzzle. It was the
moment when teams would discover the Puzzle Factory. It was the goal we wanted all teams to see,
and the key introduction to the entire story that would unfold.

And 80% of the team was spoiled on it by February.

This is always a risk you take with structure-level gimmicks. The transition to discovering the
Puzzle Factory was a key part of the theme proposals, which were all heavily discussed. How do you
testsolve something that heavily discussed?

The traditional answer is external testsolves. We tested the Teammate Hunt 2021 gimmick on a testsolve
team from Galactic, and Galactic tested the gimmick of Galactic Puzzle Hunt 2022 on some people from
teammate. This is a lot harder to do for Mystery Hunt though.

For Hunt, teammate did a lot of recruiting for puzzle writers. This was
mostly because we were concerned about the workload we had in front of us. To get a sense of scale,
we won hunt with about 60 people, not all of them decided to write, and we ended the writing process
with around 70 people on the credits page. Approximately 30% of people joining partway through the year
sounds right.

Any recruited person that joined after theme discussion was not spoiled on breakout. The true goal
of the in-person retreat was to do a testsolve of Act 1 puzzles, with a testsolve of breakout running in
the background for everyone who wasn't spoiled on its existence. After the team broke out, we'd stop
the Act 1 testsolve and reveal the story of the Hunt to those unspoiled on the theme.

To do that however, the breakout would need to exist by retreat.


Loading Puzzle...
-----------------------------------------------------------------------------------------------

During one of the weekly general meetings, I was asked if I could join a brainstorm group for "a small puzzle".
Said brainstorm group had Jacqui (creative lead) and Ivan (tech lead).

Having been in the trenches with Jacqui and Ivan on many an occasion for tech-heavy puzzlehunt
story integrations,
I was, like, 80% sure this "small puzzle" was breakout, and 100% sure it would be small to solve
but take a stupidly long time to make. I want it for the record that I was right on both counts. I joined
an in-progress brainstorm in June and we weren't done until August.

Over the course of a few months, we and other authors of the breakout puzzle brainstormed how to
create a puzzle that looked innocent at first glance, and became more suspicious the more you looked at it.
A common theme was "peeling back the facade". Those of you readers who have every played an RPG should
know the feeling of talking to NPCs until you see all the dialogue, or trying to run over a waist-high
fence and running into an invisible wall. We wanted something in that vein, that rewarded exploring
the contours of the world, and surprising you when the contours broke instead of holding firm.

Many of the early ideas from this would get repurposed for the final clickaround of the hunt,
MATE's TEAM. But for breakout, we converged towards something like a loading animation. The loading
animation would represent MATE getting increasingly overworked trying to create puzzles, diegetically
getting longer as teams solved puzzles and forced MATE to write new ones, and non-diegetically
scaling up with solve progress as desired for hunt structure. It would be easy to have this loading
animation appear on every puzzle, and that would increase the odds teams would notice what was
going on.

As for exactly how that would all work? Boy was there a lot to figure out.

![Breakout brainstorm](/public/mh-2023/breakoutplanning.png)
{: .centered }

There was a fairly serious proposal that the "hole" between the Museum and Factory would only exist on the puzzle page where
you first solved the loading animation. You'd be able to visit the URL directly once you knew it, but
for within-site navigation, you'd need to memorize which puzzle you first found the factory from.
I liked this a lot, but we dropped it due to complexity concerns.

Previous Teammate Hunts have always had a rule that solvers can look at source code if they want,
because we trust in our ability to hide things from the client. For the loading animation, it meant
the animation was drawn using CSS rather than a video file, since we did not want solvers to
right-click -> "Download Video" and solve the animation offline. The first idea proposed was to
create a conveyor belt of puzzle pieces, and entering STOP would cause the conveyor belt to
break down. (Technically, the answer at the time was TERMINATE, due to its similarity to
TEAMMATE.)

![Early breakout version](/public/mh-2023/breakout_old.png)
{: .centered }

This was later changed to the version that went in the Hunt, since having letters on the puzzle pieces
was deemed too obvious. At the same time, we didn't want it to be too *non*-obvious.

VIDEO HERE

Our testing process for this was pretty goofy. We would take groups of people spoiled on the
existence of breakout, and tell them that we wanted to test the unlock structure and backend of the
hunt website, using fake puzzles that would ask you to wait for 60 seconds, then immediately give
the answer.

![Fake puzzles](/public/mh-2023/fakepuzzles.png)
{: .centered }

This testsolved correctly, but we knew the real test would be at the retreat, when we had real puzzles.

Meanwhile, we were working on the collaborative jigsaw that appeared after solving the loading
puzzle. There are a few principles that guide teammate's approach to storytelling. I don't want to
put words in the mouth of the story team, which I've never been on, but in my opinion the key points are:

* Decide on the story you want to tell, then make sure as much of the Hunt as possible acts
consistently with that story. This will take a long time to polish - do so anyways.
* Tie changes in narrative or story state to actions the solvers take. These actions are usually
solving puzzles, but they don't have to be.
* Use bottlenecks to direct team attention towards the same point, then put the most
important story revelations at those bottlenecks.

The breakout puzzle acts the bottleneck into the Puzzle Factory, but the main thing we wanted to
avoid was one rogue team member solving the puzzle, finding the Factory, and leaving the rest of
the team in confusion about what was going on. Our solution was to have the second part of the
puzzle be a "teamwork time" puzzle (borrowing a term from MIT Mystery Hunt 2020), where collaborating
with others would make it go faster. We then would require each user to enter STOP on the loading
animation individually before they could access the jigsaw puzzle's URL, to force a flow where
early solvers would need to tell others how to break out of the Museum if they wanted to *have*
collaborators.

The collaborative jigsaw itself married three of the worst parts of frontend development:
handling different screen sizes, live updates of other people's actions, and non-rectangular
click regions. Many thanks to Ivan for figuring out the design of those details.
Internally,
what happens is that every user fires a cursor location update in Websockets every 150ms,
and the team-wide cursor state is broadcasted back to all viewers, using [a spring animation](https://liveblocks.io/blog/how-to-animate-multiplayer-cursors)
to make cursor movement smoother by respecting momentum. The click regions are then based on SVG
regions with some custom hooks on mouse-enter and mouse-leave to track which puzzle piece the
user is working with. I mostly played QA testing on checking
it worked properly on Firefox and Safari,
and making sure puzzle pieces couldn't get stranded by phantom cursors. (Fun fact:
did you know that browsers can sometimes fire a 2nd mouse-enter while you're in the middle of entering
an element? It's true! It happens entirely randomly, and when it does it fires a mouse-leave while you're still
entering said element. I'm sure this won't cause a bug that take 5 hours to root-cause.)

At one point, we planned to implement an adversarial UI, where pieces would move on their own, wiggle out
of their position, etc. The story justification would be that MATE was trying to stop you from getting
into the Factory. We ended up cutting this because it was more work, and also in internal testing
we got trolled enough by our own team members that we decided we didn't need to make solving the jigsaw
any harder.

![Conversation during Discord testing](/public/mh-2023/breakout_discord.png)
{: .centered }

In the live Hunt, a few teams somehow managed to drag a puzzle piece offscreen, and we had to manually
advance them past the jigsaw puzzle. I still don't understand exactly what happened there,
we never saw it in testing after adding bounds on the drag region.

\* \* \*
{: .centered }

The other thing we needed for retreat was enough Act I feeders to fill out the start of the Museum
round. As part of prep, we started really ramping up puzzle production, along with a "hack weekend"
where we'd try to draft a puzzle within a weekend. That weekend was where we ended up writing all of:

Museum Rules
---------------------------------------------------------------------------------------------

There isn't really too much of a story with this one. The brainstorm group I was was randomly
assigned a feeder answer, and we tried to brainstorm ideas based on that. That landed on doing
something with a list of rules placed in the reception of the Museum in-story. After the US
states idea, the idea of extracting from Supreme Court seals felt like a more suitable way
of using the puzzle information, and the puzzle presentation was stolen directly from
[Storytime](https://2020.teammatehunt.com/puzzles/storytime) in Teammate Hunt 2020.

The puzzle was written in about 2 days - I worked on creating the seal overlay extraction,
Catherine did the drawings, and Harrison did the factchecking + research of weird laws. I'd
say this puzzle was the strongest stretch of deciding whether to use true laws, or easy to
search laws. Some laws were deliberately misinterpreted for humor, like "no marathon dancing",
but other ones like "killing a fly next to a church" were not supported by any part of the
Ohioan legal code. Meanwhile, I know that
[New York has a specific child labor law exception for working as a bridge caddie in a bridge tournament](https://law.onecle.com/new-york/labor/LAB0130_130.html). Is anyone else going to be able to research that?
Maybe, but better not to risk it. Around 30% of the laws used in the puzzle are not actually real weird laws.

Really my main regret is that we didn't figure out a way to cleanly push to Supreme Court seals
rather than state seals. The ambiguity came up in testing, but we didn't get around to a fix making
it more obvious.


Interpretive Art
-------------------------------------------------------------------------------------------

As mentioned in this solution, this puzzle started from a shitpost someone posted on Facebook.

Me: "haha. Wait this isn't a bad puzzle idea."

One of the common tools a puzzle constructor reaches for is "does this have a small, canonical
dataset"? Such things are easier to search and usually give a pre-built Eureka moment when
solvers discover the canonical dataset. Accordingly, the first idea on this puzzle was to
use only songs from the Guardians of the Galaxy soundtrack. It was a good intersection of
pop music and alien interpretations of that music.

After I realized the GotG soundtrack did not have Never Gonna Give You Up or All Star, I decided
that was a bad idea, and we switched to "literally any popular song". If you cannot meme in
a puzzle, what are you even doing? My proudest moment of the puzzle was in the first testsolve,
where testsolvers solved NA NA NA NA NA NA NA NA NA NA NA HEY JUDE, and placed it while saying
"what an absolute shitpost".

For a puzzle written so quickly, this puzzle got insanely high fun ratings in testing and seems
popular with Hunt participants as well. Between this and [Young Artists](https://2021.galacticpuzzlehunt.com/puzzle/young-artists) from GPH 2022, I'm guessing that any puzzle that riffs on pop music is
going to be a fan favorite.


Conglomerate
-----------------------------------------------------------------------------------------

On this puzzle, I did less work. The story is that chuttiekang came up with the overall
skeleton of the puzzle, but wanted help brainstorming and writing minipuzzles. So
Nishant and I got recruited to do so. I ended up writing Birdhouse Kit, Camera, Microphone,
Quilt, and Telescope. I also helped on a revision of Fishing Rod.
Some of you may remember me saying that I'd try to do fewer minipuzzles for Mystery Hunt,
but if someone else is designing their inclusion, then sure I'll help out.
The sheer quantity did make construction a bit difficult, since we wanted to avoid
repeating encodings or extraction mechanisms. That left fewer options after we exhausted
binary, semaphore, Braille - you know, the old standbys.

This puzzle
had more errata than I'd like, and that ties into part of the reason I've been trying to avoid
minipuzzles. The amount of content you need to create is a lot larger, and your exposure
to missing something in factchecking is larger as well.


August 2022 - Retreat (but Also Other Puzzles)
------------------------------------------------------------------------------------------

As the date for retreat approached, most of my work was on fixing parts of the Loading
Puzzle, which was getting updated up to the day of retreat. However, puzzle writing is
a constant process during the year, so August was also the time of writing:


Lost to Time
----------------------------------------------------------------------------------------

Whenever I wrote a puzzle, I did a search in the /dev/joe index to double check it's not
too close to a prior Mystery Hunt puzzle. At one point, I opened a link to the 1995 Mystery
Hunt, and got really confused why the solution page redirected to devjoe's domain. I mentioned it
in #puzzle-ideas and kept going.

About a week later, Bryan said it was turning into a real puzzle and I should help out since
I'd already be too spoiled to test. The plan was to pretend we found all the missing parts of the
1995 Mystery Hunt in the Puzzle Factory basement, and we'd turn them into a puzzle answer. Given
how niche the idea was, it seemed like a shame to leave any stone unturned, so we aimed to use
as much of the missing content as possible. That meant writing 8 minipuzzles, based on the
constraints given by devjoe's writeup and the 1995 Hunt document. We landed on the idea of writing
puzzles with two answers, one for the 1995 Hunt and one for the 2023 Hunt, where knowing the
1995 answer is helpful for solving the 2023 answer, then having them feed into a meta based
on the 1995 Hunt meta.

This definitely contributed to the length of the final puzzle. We knew it was reallllly pushing
it for a puzzle in the FActory, but decided it could be okay if it was a one-off exception. As a
microcosm of the entire Hunt, it made sense to have as many minipuzzles as we did, but each minipuzzle
needed to be easier. (Not shorter, easier. We were constrained in length by the original 1995
Hunt.) I know an old Dan Katz post speculated whether an entire early Mystery Hunt could act
as one puzzle in a modern Hunt. I think this puzzle shows that no, it can't, but it's kind of close.

The minipuzzles I directly made were the conundrum and the screenshot of Rick Astley, but in general
I helped out on the presentation or brainstorming of other minipuzzles as well.
In the vein of [Cruelty Squad](https://store.steampowered.com/app/1388770/Cruelty_Squad/), we
spent a lot of time trying to make everything look as bad as possible. Well, "dated" would
be the more accurate term. Every image in the puzzle was saved as a JPEG with lower resolution
and image compression dialed up to the maximum. The conundrum was printed out on a piece of paper,
which I cut by hand and then scanned. The scanner I used was too good though, so we had to
JPEG-ify it a bit. Last, we planned to run the video through a filter to add VHS lines, but this
got dropped because none of the premade filters we found matched our desired aesthetic, and
making a custom one was definitely not worth it.
Shoutouts to
"The Boston Paper". I briefly considered buying a [newspapers.com](https://www.newspapers.com/)
subscription to track down the original newspaper used for the 1995 Hunt, and then realized that
it would be much better if the puzzle did not suggest looking up historical newspapers. That
being said, I bet you could find what the original puzzle looked like if you dug hard enough.

I'm a bit disappointed this wasn't solved more, but I understand given its length and placement
in the Hunt. It's also interesting to write a puzzle that will potentially be broken if
anyone finds more material about the 1995 Mystery Hunt. I'm usually rooting for better archiving.
This is a rare example where I hope it stays in a steady state!

\* \* \*
{: .centered }

I also did an internal test on a paper version of Weaver around this time. I failed to understand
the final cluephrase even with the correct extraction, so it got revised. I also remember commenting
that it might be 1 weave too long, but we didn't plan any changes before retreat. The main goal was
to get it validated enough to create the physical prototype.

This was also around the time where we were more seriously working on Hunt tech. (Move sections from
above to down here).

One interesting thing about tech for Mystery Hunt in particular is that there's greater emphasis on
creating internal tooling. In a smaller Hunt, it's possible for a small number of tech-inclined
people to get all the puzzles into the Hunt website. In Mystery Hunt, this is less true, just because
of the scale. We knew the end of Hunt writing would be busy with art and puzzle postproduction, so
any tools we wanted were best planned and coded early, before art and puzzle drafts were done.

Top of our list was improving our postproduction flow, given Palindrome's comments that they
had a postproduction crunch. We also wanted a better puzzle icon placement system.
Icon placement is a field on the Puzzle object in the database, which lets artists change its
location without needing to write a commit, but the flow of adjusting a random number was kinda
painful in Teammate Hunt 2021.

For postproduction, Ivan proposed an "auto-postprod" system. Given a Google Doc link, this tool
would load the page, retrieve the HTML within the Google Doc, and auto-create a commit and pull
request for that puzzle draft. I said "sure", not really expecting it to ever exist. Imagine my
surprise when it actually got built! It wasn't perfect. It didn't handle Google Sheets, it
would sometimes timeout if a Google Doc had an especially large image, there was an issue with
HTTP link escaping at one point, and in general if it errored on a puzzle it would have fits
when trying to auto-create the commit for the postprodded puzzle. It was still quite helpful.
You don't realize how long it takes to mechanically copy-paste paragraphs from Google Docs
and wrap them in `<p>` tags until you've done it thousands of times.


Retreat!!!
-------------------------------------------------------------------------------------

Retreat was held in late August, in a big AirBnB in the Bay Area. Going in, there were four
public priorities:

* Testsolve physical puzzles
* Test events
* Draft a new event
* Start drafting 2+ AI round puzzles

along with the secret priority of

* Testsolve breakout

Friday
----------------------------------------------------------------------------------

![Retreat schedule, Friday + Saturday](/public/mh-2023/retreat_friday.png)
{: .centered }

As an icebreaker, we went around saying what our favorite puzzle was. One person said, "134".
Later I'd learn this was the puzzle ID for the Hall of Innovation meta, which was in the
middle of taking over the lives of its constructors.

Retreat started with a group testsolve of all the Act 1 puzzles and physical
puzzles. We aimed to frontload all the physical puzzles, based on feedback that
[Diced Turkey Hash](https://puzzles.mit.edu/2022/puzzle/diced-turkey-hash/) showed up late
last year, so most physical puzzles were in Act 1 anyways.

Almost immediately, we started hitting technical problems. A few were real software bugs,
but most of them were tied to trying to have ~20 laptops and ~20 phones all connect to
the same WiFi, and this...not working that reliably. I switched to using a WiFi hotspot
from my phone. The other issue was that we provisioned a smaller server for testing,
and it turned out that server was fine for 3-5 people and less fine for 20.

So, we also immediately got evidence that if your site is slow, then people assume the
loading animation is just a real problem with the site. We also saw that as load times
increased, teammates from teammate would get in the habit of opening every puzzle at once in a
separate tab, then looking elsewhere while waiting for the puzzle to load. Which also
wasn't great. The loading puzzle was not getting solved, teams were not breaking out.

In an attempt to get people to look at the loading animation more, the editors-in-chief
nudged testsolvers towards puzzles that unlocked later (had longer load times). Meanwhile,
on the tech side we did a number of deploys to "fix a bug", that was *actually* just
increasing the time of the loading animation and encouraging solvers to refresh their
pages to get the "bug fix" (see the loading animation for longer). Testsolvers were progressing
through Act I slower than expected, so the load time curve needed to get adjusted.

(I can see some of you yelling about the "slower than expected" part, and in hindsight,
yes that should have been considered more seriously. But we had already marked retreat as
our one good testsolve of breakout, and it was *not working*. We were of the mindset that
we'd lock in all details of breakout by today, and if breakout didn't work then the
entire Hunt's design was in jeopardy. I don't even think that's wrong.
In comparison, many of the Act I puzzles were on their 1st draft and would likely get easier as they got polished.
Again, I think "puzzles get easier as clues are cleaned up" is also totally true!
The error may have been in the estimate for how *much* the solve time would drop in
future puzzle revisions.)

In between trying to ramp up loading times, I testsolved many puzzles, including
[Exhibit of Colors](https://interestingthings.museum/puzzles/exhibit-of-colors) (solved the
flower puzzle and got the animal a-ha), [Dropypasta](https://interestingthings.museum/puzzles/dropypasta)
(complained heavily about the old Pokemon Stadium mechanic - it got changed to what our testsolve
group thought it would be), and [Brain Freeze](https://interestingthings.museum/puzzles/brain-freeze). At
the end of Brain Freeze, someone asked why it had such a garbage answer, and I had to stay silent
knowing I was responsible for pushing that garbage answer into the Hunt.

(meme image here?)

After that, Collage got unlocked! Except, it was totally broken. Oh no. One teammate meme
during writing was "bamboozle insurance". You offer bamboozle insurance when you want to assure
people something will be true, and you pay bamboozle bucks when you are wrong. The nature of writing
puzzlehunts is that you'll be wrong quite often. Example uses include:

"This meta testsolve needs 2+ hours but it will be worth your time, you can buy bamboozle insurance from me"

"Can I get bamboozle insurance the loading animation time won't change, I'll have to redo math if it does"

DALL-E and other image generation models were quite hot at the time and we used them accordingly. Although
a surprisingly large fraction of teammate works in machine learning or related fields, the choice to
have an AI-themed puzzlehunt in the year generative models became commercially viable was entirely
coincidental.

![Bamboozle emoji generations](/public/mh-2023/bamboozle.png)
{: .centered }

I bring this all up to say that I offered bamboozle insurance that Collage would be fine,
since we literally testsolved it a few weeks ago, and then it wasn't.
It turned out the issue was that the loading animation was conflicting with the startup of
the interactive puzzle content, replacing it entirely rather than making it hidden, and this
caused the async initialization requests for Collage to never fire. We spun up a hotfix
to get Collage working, and got it going enough to run a testsolve, but this fed
into the narrative of "the site is struggling, so the loading animation isn't special".

The original plan was to get to breakout by around the evening (8 PM to 10 PM), then leave time for
board games and socializing. Instead, at 1:40 AM, the Act I testsolve was officially put on pause,
and then a bunch of hunt exec and breakout authors shuffled into a meeting room, where we
discussed what nerfs we wanted to add before the testsolve resumed Saturday morning.

The additions we landed on were:

* Be much more aggressive on hinting with the messages MATE sent during loading ("It's puzzling
why this is taking so much time.")
* Stop interleaving the pieces of each color, just show all the pieces in order.

![New puzzle piece order](/public/mh-2023/neworder.png)
{: .centered }

By the time this was resolved and implemented, it was 4 AM. I went to go collapse on an air
mattress so that I could be sort-of awake tomorrow.

Saturday
---------------------------------------------------------------------------------

The Act I testsolve was still running in the testsolve, but we did want to do the originally
scheduled events, so the morning was dedicated to event testsolving and Act 3 puzzle drafting.
In Saturday morning, we started writing what would eventually become:


We Made a Quiz Bowl Packet but Somewhere Things Went Horribly Wrong
---------------------------------------------------------------------------------

Okay. I know this puzzle isn't that popular, but all puzzles will have their stories told.

We were given the feeder answer for this puzzle, and after investigating various pyramid options,
we laneded on pyramidal quiz bowl questions, partly because some of the authors were fans of
quiz bowl. The idea of extracting based on diagonalizing words based on a Hamiltonian path of
the United States seemed like a good way to tie into both the feeder answer and the structure
of a quizbowl question, so we started with constructing a suitable quiz bowl question for
the final answer. Since solvers had access to the Internet, and we knew this would be towards
the end of the Wyrm round, we ended up using a very loose searchability bar (which was one
of the big causes of the puzzle's length).

Originally, the plan was that every question would semantically clue a state in the Union.
This was thrown out pretty quickly when we tried to tie DADDY LONGLEGS to South Carolina, and
the best we could come up with was a South Carolinian high school's rendetion of the musical,
which was a bridge too far.

A sketch of 4 sentences per clue x 48 clues was, in hindsight, a lot. Part of the justification
was that unlike many other difficult puzzles, this was the sort of puzzle that parallelized
well and was easy to jump in and out of. There wasn't a lot of puzzle-specific context you
needed to absorb to make progress. With so many puzzles trending in the direction of serial
deductions + not much branching, it seemed okay to have a big puzzle that could run in the background.

Over the course of a few weeks, we started filling out the draft. It was clear that the puzzle
skeleton would work mechanically, but we were quite unsure if the puzzle would be fun. It
seemed like the solve process would be that you Googled a bunch of phrases, reordered,
indexed the way you were told to, then followed the instruction you were given. You'd then
be done without really having an a-ha anywhere. Given the structure, the easiest place to
add an a-ha was in the 1st step of Googling everything. That motivated the decision to
obscure questions by pretending they were always talking about a location. In this way,
although the bulk of the puzzle would still be Googling, there'd be a local a-ha in deciding
how to reinterpret the clues when formulating that search. "Cryptic-like" is the right analogy
here. Cryptics work because you need to decide what is wordplay and what is definition,
and similarly you'd need to decide what parts to read straight and what parts were location
wordplay.

This made the puzzle...a *lot* harder, which got corroborated by testsolving. To paraphrase, the
feedback was "this was really fun at the start, then unfun after we got really stuck".
I am
not tying to be flippant here, this was not a [sour grapes](https://www.merriam-webster.com/dictionary/sour%20grapes) problem. There are puzzles where you get stuck, and it's a pleasing stuckness to try to
figure out what idea you're missing. In this puzzle, that was less true.

(I suspect the issue is worse if you are solving puzzles from a mindset of trying to 100%
every step before doing the next one. The intermediate cluephrase is really helpful for
constraining how sentences can pair up and trying to solve for it sooner does a lot for
the solve experience.)

My suspicion is that if you remove the obfuscation, the puzzle gets even more mechanical
and it doesn't really save the design.
The right fix would have been to keep the obfuscation, but cut the puzzle size in
half and brainstorm a new extraction. That way, the disparity between easiest and hardest clue
would have been smaller, the matching problem would have been easier, and it would be more
likely you'd hit the cluephrase before the puzzle overstayed its welcome. The fun ratings
weren't great, but they weren't bad either, so it went through without many changes and
never got revisited.

\* \* \*
{: .centered }

The brainstorming for Quiz Bowl was running the same time as event testsolving. We still
weren't sure if we could run Hunt in person during retreat, but decided to plan as if we'd be
allowed to run on-campus. Accordingly, events were designed around in-person interaction.
This was done with the understanding that the events wouldn't be doable without on-campus
presence, and this would upset fully-remote teams, but, well...in my opinion,
it's called the MIT Mystery Hunt for a reason. We had just done 2 years of remote-only events.
Swinging the pendulum back to in-person events seemed appropriate. TTBNL is free to do what
they want here.

AFter lunch, priority shifted back to continuing the Act I testsolve. We had already pushed the
4 AM fixes to the testing site, but embarassing we had to do another hotfix becaues we'd accidentally
made the loading animation cover the answer box with an invisible div.

![Invisible box](/public/mh-2023/answerboxcovered.png)
{: .centered }

Solving of the last few Act I puzzles continued...and the unspoiled testsolves still hadn't
noticed anything. From our #puzzle-ideas channel during retreat:

![Someone saying the loading animation should be a puzzle](/public/mh-2023/itisapuzzle.png)
{: .centered }

Let's say it was an interesting time. They did solve it eventually, but I'm not sure if they
found it organically or if we had to tell them to look at the loading animation for longer.
Once again, the breakout authors and hunt exec went into another room to do a secret testsolve
debrief, with the people who solved breakout. Their initial feedback was "yeah, this seems
obvious, we should have noticed this earlier", to which we said, "You say this, but we have 12 hours of
empirical feedback to the contrary."

After some more Q&A on what made them believe it wasn't a puzzle, reasons why they didn't pay
much attention to the animation, etc, we planned a few more changes:

* The four green dots (used to give an ordering on the letters) would be moved out of the
answer submission box. Their appearance within the submit box made it look like the answer submission
field was "loading" and could not be submitted to.
* Submitting an answer during loading would give a custom message ("Whatever you do, definitely don't submit anything until it's finished!").
* MATE would become increasingly depressed as loading continued, to entice people to keep
watching the page to see what would come next. We planned to add this Friday night but the art
assets couldn't get written overnight. (Much later, this led to the quote of "why does the animation look better when MATE is perpetually sad?")
* Internally, we called the infinite loading time "the hammer" that would push teams to breakout.
Evidently the hammer was not strong enough, so we designed a "megahammer". We'd add a
time-unlock breakout, by sending an email to all teams with a video tutorial of how to solve the
loading animation (explained in story by teammate wondering if MATE had a problem).

We weren't going to have another unspoiled testsolve of these elements, so we biased towards
safety and would need to assume it would all be good enough.

With breakout resolved, we could go back to puzzle brainstorming and wrapping up testsolves. The
[Teammating Dances event](https://interestingthings.museum/solutions/teammating-dances) was
written and tested. I later joined a very-stuck testsolve of [Tissues](https://puzzlefactory.place/factory-floor/tissues). At the time, it did not have the TetraSYS hint. We got it partway through the testsolve,
I mentioned there were four Greek elements (no idea why), and we gave the authors a bit of an
aneurysm by saying we should dunk the tissues in water, or set them on fire. "We should go do it,
even though it's likely not correct! If it's the wrong idea they'll stop us because we don't have
any backups."

(Meanwhile testsolvers for Weaver were dunking the puzzle in water.)

We got up to the cluephrase, again quite late, and I decided to turn in for the night to catch up
on sleep.


Sunday
---------------------------------------------------------------------------------------

I woke up and continued the Tissues testsolve while waiting for people to wake up. I did a few
searches to prove that "Nikoli + black cells" was not enough to get to the next step, and
although I eventually got
"Nikoli + [dokodesu](https://www.google.com/search?q=dokodesu+in+english) + black cells",
it wasn't very reliable for a step so late in the puzzle. By now other testsolvers were awake,
so we had enough manpower to finish the last step in time for the State of Story meeting.

The creative team presented the overall broad strokes of the Hunt story, the existence of
breakout, and the broad art direction for each AI round. For breakout, an audience member asked
"what's our plan if a team doesn't notice breakout?", which had *incredible* dramatic irony.
We said we had a plan and moved on.

Wyrm would be a precocious child with
a construction paper aesthetic (since every explanation of a wormhole in fiction involved sticking
a pencil through two pieces of paper). Bootes would be a meme-y ASCII art cat in space, because
the answer gimmick involved characters and whitespace mattering. Eye would
be a noble, biblically accurate angel, as a nod to the Tower of Babel. Conjuri would be a pixel
art owl eager for new challengers, with pixels from gaming and owl as a suitable animal for the
magic theming of the game. Eye's gimmick was spoiled at this time, since we needed
more authors for Eye puzzles, but the rest of the gimmicks were not spoiled to save them for
testsolving.

We did a final round of puzzle brainstorming and testsolving. I remember getting pulled aside to
a brainstorm meeting because we thought it'd be funny to write a puzzle with "the full house
of Alexs over Brians" (Alex Gotsis, Alex Irpan, Alex Pei, Bryan Lee, Brian Shimanuki,) Don't let
your memes be dreams! We got in a room, tried to start a puzzle for [l(a](https://en.wikipedia.org/wiki/L(a),
spun in circles for an hour, and failed. The puzzle was officially killed a few weeks later. Some
memes should stay dreams.

Retreat concluded with a bunch of board games. We played some rounds of Just Two (it's [Just One](https://en.wikipedia.org/wiki/Just_One_(board_game)) except a clue is only provided if exactly two people
put it done. It's good for large groups and is a *lot* harder. I'm told that when the word "pony"
showed up, the only clue that made it through was "Irpan" which, okay, fair's fair.

East Coasters flew back home, Europeans flew *really* back home, and we ramped up the main push
towards finishing Hunt.


September 2022 - Ah Yes, Websites are a Thing
------------------------------------------------------------------------------------

It's a bit disingenous to say that I started working on the site in September, becaues I had been
working on parts of it starting even back in January. But I'd say my main ramp-up started around
here.

There were a number of tasks to do, and the main one I took was handling the Wyrm round page.
Which ended up being a *lot* more complicated than I thought it would be. During Wyrm ideation,
we discussed the [Zoomquilt](https://zoomquilt.org/) and its [sequel](https://zoomquilt2.com/)
as both art inspiration and proof-of-concept that we could make a infinitely zooming round page.
{I remember Huntinality 2022's [registration form](https://2022.huntinality.com/) also came up.)

Each of the Wyrm layers was done by a different artist. Turns out that if you want every puzzle
to have its own icon, it doesn't matter that 13 puzzles in the round are "fake" or repeats. You
still need to draw 13 art assets. (Creatively, the art team did not want to reuse art assets
between the Museum and Wyrm versions of the Act I puzzles, since the Museum aesthetics were
intentionally not going to match the Wyrm aesthetics, to play up the difference between MATE-created
puzzles and Wyrm-created puzzles.)

As a very early proof of concept, I looked into making a barebones page that would smoothly
zoom between different placeholder art, looking into how to add a zoom with D3.js. The default
behavior of d3 is to both pan and zoom the content, so I needed to manually strip out
the pan information before passing it into the d3 code. I also needed to figure out how to make
the zoom level "roll over" once you passed the infinite threshold. I expected that to be a nightmare,
and was pleasantly surprised when it wasn't.

Based on the barebones version, I advised that tech would be easier if we had a consistent
aspect ratio, each layer shared a common zoom point (likely the center), and the ratio of the
zoom between layers was identical. I also decided to remove scroll wheel support. It was easy to
add, but it was a slightly annoying user experience to have the round art steal your scroll wheel
inputs when you wanted to move down the page. I hardcoded some placeholder puzzles to prove it
worked. Then it was optimization time.

> Based on the profiler call, current D3 setup is 11 ms to re-render all 4 layers when I do the 1 -> 2 zoom. (All 4 divs get re-rendered since they all look at scale variable). I set up the CSS transition for just layer 1 -> 2 and that was 8ms. The gap should get smaller when I make the CSS animation apply to all 4 layers.
>
> So, in short, yes it'll be faster, I'm not sure it'll be appreciably faster.
> I'll probably leave this for later based on the timing, in favor of some other things I need to fix in the prototype. But can keep it in mind as a performance win that may be more important when we have more assets in the round.

Performance optimization is not really in my wheelhouse, but it's something we thought about
a lot. Some of the choices we made (more art, structuring around [single page applications](https://en.wikipedia.org/wiki/Single-page_application)) naturally push towards downloading more data and doing more client-side rendering. To make the zoom work, we *really* needed the zoom implementation to look seamless.
The initial prototype was not. There was very noticeable pop-in as you swapped between layers,
on both the background image and puzzle icons.

It 100% seemed liked a solvable problem, given that Zoomquilt was seamless and I remember
it looking seamless on 10 year old hardware. When I looked more closely, their code was based
on using an HTML canvas. I was less excited about this, because HTML canvases don't use vector
graphics and you can get weird rasterization artifacts when zooming text. (I tried a canvas
briefly in Collage and switched back to SVGs afterwards.)

I asked the tech people with more frontend experience for advice, and they helped explain more
of React's rendering logic and where I could run a profiler. It turned out the problem was that
my prototype was taking fixed `<img>` tags and changing the `src` field based on what zoom level
I was on. This effectively forced the image to clear and re-render every time I crossed layers,
and no browsers are built to do this seamlessly. So instead, we redesigned it to create all the
`<img>` tags for all four layers at once, then use CSS and JS to adjust the zoom and
[z-index](https://developer.mozilla.org/en-US/docs/Web/CSS/z-index) ordering of the static images.
That way, the browser would only load + draw the images once, and the rest would just be image
transforms. This fixed the bulk of the performance issues.

Next was fixing the blurriness. That was easy. When I set up the prototype, I defined the zooms
as $$1, 8, 8^2, 8^3$$. But that meant you would sometimes see parts of a 64x magnified image
during transitions, which looked just the worst, so I inverted the scale to $$1, 1/8, 1/8^2, 1/8^3$$.

With that done, I decided the frontend had been proof-of-concepted enough, and moved towards
other work. Over the past few months, we had made a list of feature requests and bugs on Github,
and I started running through ones I knew how to do.

The first was improving our puzzle icon placement system. In tph-site, the location and size of a puzzle
icon is defined in the database, to make it adjustable without doing an entire deploy. But editing it still
requires digging in Django admin pages, which can be a scary prospect given how many footguns it has,
and the user flow of tabbing back and forth between the admin and site pages is pretty annoying.
This was already a pain point in Teammate Hunt 2021, and was ready for improvement.

I coded a basic WYSIWYG tool, where if you logged in as admin, it would overlay a movable box whose
size and position would correspond to each puzzle icon. Resizing or moving the box would move the
puzzle with it and save the updates to the database, which we'd then download and commit into
the codebase to keep a more permanent record. I think there are still some issues where the box isn't
drawn consistently with the puzzle, but hopefully it helped.

![Drag drop UI](/public/mh-2023/dragdropui.png)
{: .centered }

Meanwhile, I did some work on postproduction of Quandle (to move the wordlist and game logic to
server-side), and handling some of the TODOs tied to breakout's testsolve at retreat. I also got around
to adding a bug fix that Huntinality folks found in tph-site's websocket management, and stole - er, copy-pasted
a k6 loadtesting script to test out later. (I didn't ask for it, it came up in
a discussion about website issues GPH 2022 ran into in a Discord server.) It looked a lot
nicer than the hodge-podged Locust code I had written for Teammate Hunt 2020.

I also joined the brainstorm group for what we called the midpoint capstone puzzle, which you know as:

Reactivation
-------------------------------------------------------------------------------------

I'm on the author list for this puzzle but I really didn't do much. I was in the initial brainstorm
meeting, where the creative team said they wanted each puzzle to be thematic to the AI. I shared some
links to [turtle graphics](https://en.wikipedia.org/wiki/Turtle_graphics) for Wyrm, which matched
the aspect of repeating patterns in the round, then stopped paying attention to this puzzle so I could
go back to web development. That's it!



October 2022 - How About a Nice Game of Chess?
--------------------------------------------------------------------------------------------

By this point I had finished design work on all in-progress puzzles that I was motivated to
finish, and was planning to mostly due token tech tasks until the end of Hunt. All of them, except for one.


5D Barred Diagramless with Multiverse Time Travel
-----------------------------------------------------

This puzzle may be my magnum opus. From a fun standpoint, I suspect Interpretive Art wins.
From a solve experience standpoint, I'm guessing Puzzle Not Found wins. But, from a
construction and spectacle standpoint, I would picked 5D Barred Diagramless. It
is really stupendously long, but I claim it is both entirely fair and very up-front
on what you're getting into.

As mentioned way near the start of the post, the first draft of this puzzle was written in
February for an internal puzzle potluck, and did not include any 5D Chess elements. I just really liked the joke, and knew that
[it would be an inherently terrifying puzzle title](https://dp.puzzlehunt.net/team/5D+Crosswords+With+Multiverse+Time+Travel.html).
Since the puzzle had started
design from the [Clinton/Bob Dole](http://www.alaricstephen.com/main-featured/2017/7/3/the-clintonbobdole-crossword)
crossword, the first version went all-out on pushing that idea as far as possible.

In any puzzle where clues solve to a pair of answers, you either need a way to order the pair,
or you need to pick an extraction method that uses both answers of the pair in an unordered way.
I first realized this when solving [Split the Reference](https://2018.galacticpuzzlehunt.com/puzzle/split-the-reference.html)
in GPH 2018, but then started noticing that pattern all over the place.

After doing some fudging to get JOSEPHBIDEN/DONALDTRUMP to line up, I noticed that JOJORGENSEN
*also* happened to be 11 letters long, and, I mean, come on, you have to try doing something with that.
Two timelines would be clued by the grid, and then the 3rd timeline would provide the extraction content.
I had trouble finding other presidential elections where the two parties' candidates could also be
forced to 11 letters, but was able to make it work for a bunch of years if I allowed dipping into
the many 3rd-party candidates. It still felt wrong...but, the potluck deadline was coming soon, so I
decided to just go for it and figure out a fix later.

The grid fill went well, the extraction less so. I needed to give a hint that the 3rd timeline
existed. Still, it seemed promising. I then moved on to lots of other work.

Fastforward to 8 months later. In a team meeting, the EICs mentioned we could use more crosswords in
the Hunt for puzzle variety. Hey, I have an unfinished crossword idea! Let's see if we can fix it.

I reflected on the design, and came away with these notes.

* As much as possible, I wanted
the puzzle to look like a single grid. A 5D crossword should only have one 5D grid! (With that grid
having many projections into 2D space.) More specifically, the given borders and black squares should
match across all of the 2D grids.
* If the extraction is only based on alternate timeline entries, then those are the only parts
of the grid that matter to extraction, and that's a bit disappointing if you worked on the fill
elsewhere. (This was one of the bits of feedback I remember from [Cross Eyed](https://www.puzzlesaremagic.com/puzzle/cross-eyed.html)
years before.)
* If the extraction uses letters from the entries that are time-dependent, then I'd need to call them out
or indicate them in some way, which I wasn't too fond of. Doing so would remove the mini a-ha
for how to fill those entries. For similar reasons I didn't want to label the alternate timeline entries.
* Really, it's a shame the puzzle doesn't use 5D Chess in any way, given how directly it's called
out in the title.

Remembering [✅ ](https://2020.galacticpuzzlehunt.com/puzzle/check.html)
from GPH 2020, I considered the 5D Chess point more seriously. You'd want to solve the entire grid before
doing the chess, because chess pieces and the extracted letters would be all over the grid. The puzzle
wouldn't need to label any of the special entries, preserving the a-ha during the grid fill. Thematically
and mechanically, it just made sense to make it a chess puzzle. Making it barred would let me fill
every square, and making it diagramless would led me keep the given borders identical (since there wouldn't
*be* any).

Great, problem solved! Time to learn how 5D Chess works. How hard could it be?

The editors assigned me an answer from Bootes, with the reasoning that a 5D puzzle could accomodate
a 2D answer. The answer I got assigned had some punctuation characters in it, which I agreed to try
fitting into the puzzle. I said that I could plausibly do things with rebuses, like cluing
GAIL PARENT and filling it in as GAI(T. The answer options weren't great, but were workable.

Over the span of about 4 weeks, I put increasingly deranged comments into the puzzle brainstorm channel.
I'd say the default state of the editors was "confusion", as I monologued my struggle learning 5D chess
to people who only sort of understood what I was struggling with.
I initially
tried to learn the rules by watching gameplay videos. Within an hour I decided that it would
be more efficient to buy the game, so I did. I mostly paid for Mystery Hunt with time. This
was the rare puzzle where I spent money.

I have yet to play a full game of 5D Chess with Multiverse Time Travel. Instead I played the in-game
puzzles, which are specifically constructed to teach you the rules. After getting a basic understanding,
I decided to have the puzzle only use bishops, knights and kings. Rooks were too boring in 5D, pawns could introduce
questions about whether they had been moved yet or not, and queens were terrifying. A single queen in
5D chess has, like, 60 possible moves. There are arrangements where one queen can mate a king in 7 different ways. Trust me it would have been a bad time.

I made a 5D chess problem, filling the rest of the grid with random letters to get early feedback on
just the chess step.
What was immediately clear from those tests was that it was very easy for someone to read a 5D chess guide
fairly quickly, come away with 50% of the rules, and then attempt to solve the chess puzzle incorrectly.
As a solver, there is really no way to distinguish "I do not understand the rules" from "I understand
the rules and am just bad". Like, they would be 50 minutes in, asking for hints and we'd have a conversation
like this:

![A conversation about 5D chess rules](/public/mh-2023/thefuture.png)
{: .centered }

"The future doesn't exist yet" is really such a great line. I recommend using it without context, it can
apply to so many things.

In response to the pre-test, I created some 5D chess examples that could not be solved without understanding
the base rules of timeline creation and branching. This was part inspired by Time Conundrum,
and part inspired by [Cryptoornithology](https://ecph.site/puzzle/cryptoornithology.html) from EC Puzzle Hunt.

Anyways, when factchecking that testsolve on what mates were correct / not, I decided to double-check my
work and found they had found a checkmate that I missed! At that point I decided that okay,
I need to write code to verify this, I'm not going to be able to trust my 5D chess solving ever again.

I reimplemented the rules engine in Python. Not the full engine, but enough to test for mate-in-ones.
I went back and forth between my code and the in-game puzzles until I got them to agree with each other,
discovering a few other edge cases along the way.

At the same time, I sent a question upwards: if I clue the answer directly, do my letters need to be
aligned exactly like the answer, or would it be okay to assume solvers would find the correct whitespace
themselves? My question went to the editors, who sent it to meta authors, who sent a reply back through
the editors, and the answer was: yes, if you aren't using a cluephrase the spacing needs to line up
exactly.

Well okay then I'm not going to be able to get the extraction to work that way, it's too constrained, so
I dropped the rebus ideas and started digging for cluephrases. Finding a new cluephrase of the same
length, I went back to my code, this time modifying it to place pieces for me. By this point, I had
realized a constraint that was not obvious beforehand.

As designed, chess pieces can only differ
between the two timelines if they appear in the alternative timeline entries that have 2 answers.
Given the construction, I was only planning to have 3-4 of those entries across all grids, since each one
took a lot of real estate to support. (The down clues that cross them take up about 25% of the grid squares
per entry.)
If an extraction square appears in the same spot in both timelines, then those letters are forced to
match in the cluephrase, which would add annoying constraints on how many mates needed to exist in each timeline.
To maintain flexibility, I wanted extraction to never agree between timelines, but this was only possible if
I concentrated as many chess letters into the alternate timeline entries, since that was only source
fot timeline deviation.

Does any of that previous paragraph make sense? No? Don't worry about it. The short version is that
every alternative timeline pair needed to satisfy at least one of the following:

* One has kings (K), the other does not.
* One has bishops (B), the other does not.
* One has knights (?), the other does not.

This is pretty much the only reason that the 2008 entry was BEIJING / BANGKOK, even though Bangkok
failed in the very first round of IOC evaluation.
I really, desperately
needed to have an alternate timeline entry where kings existed in one half but not the other.
For a similar reason, the Kentucky Derby was picked just because it was easy to find a historical
winner with a K in its name.

This requirement also influenced the decision to make knights be represented as Js. I knew using N
would make the crossword fill way too difficult. Using J both made the fill easier, and created
differing pieces in the JOSEPHBIDEN / DONALDTRUMP and KENJENNINGS / LEVARBURTON pair. (I wanted
to use MAYIMBIALIK / LEVARBURTON originally, since it seemed like a fun bit of trivia to use "the person who wasn't Ken", but having a shared B in the same spot created too many problems.)

With a better understanding of the requirements, I placed my seed entries in the grid, then had my code
greedily place kings until it had achieved 22 checkmates. It failed to do so. The first king
placed created 17 checkmates, and from that position it had a parity issue where I could get either
21 or 23 checkmates, but not 22.

Hang on, 17 checkmates from one king? That seems wrong.
I added tons of debug print statements to track down the bug. That was how
I learned it wasn't a bug at all. My brute forcer had correctly found a discovered mate.

I verified it in game, and went "holy crap that's so cool." But should it be in the puzzle? I tried
banning my code from using it, and found it made construction impossible without adding more alt-timeline pairs
into the last grid. Essentially, there are around
30-40 different moves possible from the alternative timeline entries, and you literally can't distribute 20+ checkmates among them unless you use discovered mates.

With an understanding of what was going on, I created the final chess position by hand, arranging
half of the seed entries to create the discovered mates, and placing the other half to fix the parity
issue. Now that the full chess puzzle was created, I sent it to another round of testing just the chess.

This testsolve went a bit better thanks to the examples, but still had some trouble due to confusion
on how to set up the chess problem and what "one move" meant. In response I prepared many more seed entries
describing aspects of 5D chess. By this point the puzzle had evolved from "solve a crossword, get an answer"
to "solve a crossword, congrats you're 50% done", and it felt like it was on the edge of unreasonably
long. I expressed these feelings of uncertainty to testsolvers, at which point they said 5D Chess was
"exactly the bullshit I expect to see in Mystery Hunt". I'd say a stronger statement is true: I don't think
this puzzle could exist outside of Mystery Hunt.

So really, the question was whether I wanted it to exist. I think people believe that, like, I was super
gung-ho about getting 5D Chess into Mystery Hunt, and I really wasn't! I was only motivated to do so
after sinking weeks and weeks into understanding 5D Chess enough to appreciate its design. All I wanted
was a multi-dimensional crossword with a suitable extraction, and all roads for that led to chess.
There may be a world where the puzzle is just a 5D Chess puzzle, no frills, but I don't think there's
a world where the puzzle is a 5D crossword without a chess step.

With the chess position constructed and two pretests of the chess step done, the puzzle still hadn't had a fully unhinted solve of the chess,
but the crossword was already 1/3rd filled, and I decided to guess that any future chess nerfs could be applied
after the grid was completed. I spent the next week completing the fill and writing clues. Around the middle of
the fill, I realized I had created some 1-wide columns, which I wasn't too happy about. But fixing it would
have required changing the placement of the alternate timeline entries, which would require regenerating
the chess position, which would have meant restarting from scratch. And the chess problem given
had already gone through one partial testsolve...I decided I didn't have time to fix it.

By now it was late October, where I had spent my entire month of Mystery Hunt time trying to make 5D Diagramless
a reality. I would guess I was at the 60 hour mark and we were just getting to the first full testsolve.
In that testsolve, the grid fill went great! The chess continued to go less well. This time the solvers did not understand the rules for how chess timelines are arranged, as well as some other details about setting up the
chess problem. I let them flounder for a while, realized they were never going to fix it, and
revised the critical 55-Across clue to be extra explicit about timeline arrangement. I also changed
the clue for TWO into a seed clue reinforcing the rules of 5D chess. It was a stroke of good luck that TWO was already in the fill,
although perhaps having it appear in one of three random 13x13 diagramless puzzles isn't too rare.

Once I had hinted the testsolvers to the correct timeline arrangement, and after giving the total number of
checkmates, they solved the chess without any more guidance. I'd say that was the first moment I
started to believe the end of puzzle construction was in sight. I was originally hesitant to give the
total number of checkmates out of cheese concerns. If you find all squares that pieces *could* move to,
then construct a regex to drop out letters until you get to 22 letters, then the phrase is mostly readable
from nutrimatic and you can skip the chess puzzle. In practice, the chess theming is strong enough that
very few people attempt to cheese the puzzle. The goal of giving the number of checkmates was to
give a minor hint towards the discovered mates. Only 5 mates can be found without it. If you make mistakes on
checkmates, maybe you get 1-3 more. It's far enough away from 22 that eventually you're forced
to confront the possibility that you are entirely missing a key idea.

The 2nd testsolve of the full puzzle went well, and I was looking to do a 3rd testsolve, given that I'd made
changes to the chess step in the 1st full testsolve and it had yet to go through 2 fully unhinted tests.
However, the editors decided that between the 1 fully clean solve, and the 3 previous partly-hinted solves,
the value of another test wasn't worth it compared to spending the testsolve time on other puzzles.

By this point, the puzzle had become a bit of a meme. About 1/3rd of teammate had testsolved it and another
1/3rd had heard a rumor that there was a "five-dimensional puzzle". This was shortly after 4D Geo finished
testsolving, and it was funny to realize that our hunt had a 3D puzzle, a 4D puzzle, a 5D puzzle, and
a non-integer dimension cameo in Period of Wyrm. "We have too many dimensional puzzles, new dimensional puzzles are banned."

Internally, as we ramped up postproduction and factchecking, this puzzle got flagged as being especially
scary on both fronts. Many thanks to Holly for factchecking all the crossword clues, and Evan for creating
all the chess diagrams with hundreds of lines of [Asymptotte](https://asymptote.sourceforge.io/) code.

The chess factchecking was easily the most terrifying factchecking experience of my life. I had
researched and installed a mod for 5D Chess that let me load arbitrary positions, but the game is hardcoded
to only work for boards up to 8x8, so the final check had to be done by hand. Here's a list of
things I did not know until after testsolving was finished:

* A player can delay an inevitable mate by branching back in time, unless they have timeline disadvantage.
* A player can mate a king in an active board if they fork two active kings.

The first was only fine because I decided to have black make a timeline instead of white. If I had
messed up that coin flip a month earlier, it would have completely invalidated the chess problem and I would have had to reconstruct the entire puzzle + retest the chess puzzle.
None of the testsolvers caught this, because none of them had actually learned 100% of the rules of 5D Chess.
Instead they had learned the 90% needed to solve the puzzle. But for a Mystery Hunt, 90% isn't good enough.
You never know how far solvers will go.

The check for forks was also really close to failing, although I think my odds of failure were more
like 20% rather than 50%.
As for the analysis on moving two pieces in one move, I had some backups in mind if that didn't work,
but thankfully it did.

Factchecking the Appendix was a casualty of the last-minute push to get Hunt ready in time. Although
it wasn't independently verified, we were willing to say it was unlikely solvers would go as
in-depth as I did on checkmate verification, Evan hadn't noticed any errors when reading through the solution
for postprodding, and there were puzzles in more dire need of factchecking.

The solution ended up at around 8000 words of just complete, utter absurdity. I'd say that's why I consider
it my most important work. It is this perfect encapsulation of what it's like to fall down a puzzle construction
rabbit hole by taking a joke way too far, and come out the other side with a coherent, self-consistent, and
beautifully terrifying puzzle. Only two teams solved it forward, but I was expecting at most 10 teams to
finish it, so I'm really not too torn up about it. I got most of my enjoyment from making it,
not watching it get solved. (It helps knowing that one of the forward solvers said it made their top 5 puzzles ever.)

The one regret I have is that one of the examples could have
relied on a discovered mate to make the endgame extraction flow smoother.
As a standalone puzzle I believe it is better without a hint in the examples, but in the context of Hunt,
including it would have given most of the Eureka moment while making it less likely teams got stuck at the end.

\* \* \*
{: .centered }


November 2022 - AH YES, WEBSITES ARE A THING
---------------------------------------------------------------------------------------------

There were a...concerning number of things to figure out for the Hunt website.

A lot of it was classic stuff. Figuring out how to make the Factory round page meet the design
spec, supporting different site themes per round, etc. *Yawn*. Like, the problems were straightforwardly
solvable. There were just a lot of them and concerningly few people to work on them. My understanding is
that a lot of people who planned to do tech were either busier with real life than expected, or sucked
into the Conjuri rabbit hole. This left a lot of infra tasks on the table.

With 5D Chess wrapping up, I decided I was mostly done with starting new puzzles, and offered to
go more all-in on tech, given my experience with tph-site internals.


Events
-----------------------------------------------------------------------------------

This was less tech work, and more design for how we wanted to represent events in our codebase.

We decided events would act like globally unlocked puzzles, with some event-specific metadata to manually open them
when the event was completed. We considered how to do event accounting, with the goal of minimizing
logistical overhead, and decided to have every event solve to an answer that teams would enter themselves.
Pretty much every recent Mystery Hunt has used this system, it just works.

There was no plan to have an events round like 2019 Mystery Hunt, so what should events do? EICs decided that
events should act as either free answers (manuscrip) or free unlocks (Penny Passes), with free answers
only usable in non-AI rounds and free unlocks usable everywhere. We wanted event rewards to be useful at
all stages of the hunt, but didn't want teams breaking into the AI round gimmicks with free answers.

(The church bells of dramatic irony are ringing. We'll get there, I promise.)

I asked some pointed questions about edge cases. How do free unlocks affect meta unlock? Is all of Museum
one round for unlock purposes or not? The real reason I was asking was because I wanted to figure out
requirements for the real tech blocker:


Unlock System
---------------------------------------------------------------------------------

gph-site and tph-site use a concept called "deep". I don't know why it's called deep. Maybe because of
GPH 2019's starter theme about an excavation? In any case, "deep" is a measure of how far you are in
a Hunt, but how that is measured can be done in different ways.

The 2022 Mystery Hunt chose to have a fixed, single track unlock order. All puzzles, both feeders and
metas, were on that track. The 2021 Mystery Hunt had per-round tracks, where each puzzle gave a lot of
JUICE to its round and some JUICE to smaller rounds.

Our Hunt was looking like it would be more like JUICE, escept weirder. AI rounds were going to have
independent unlocks, since each was its own hunt in-story. Oh except we wanted new AI round unlocks to
be based on Act 3 solve progress. The Museum was slated to have a shared
unlock track between all rounds, but with a min-solves-in-round threshold for metas. Factory rounds were
going to be mostly normal. Oh, and the number of puzzles initially visible in the Factory was going to scale with the number of Museum solves teams had before unlocking it.

I decided that, you know what, we don't know what our unlock system is yet, it could change wildly,
so I am just going to implement the most generic unlock system I can think of. I arrived at what I'd
call the "deep key" system.

* Every puzzle has a deep key (an arbitrary string), and a deep value (an arbitrary integer). A puzzle
unlocks if the team's deep for a deep key is at least the deep value.
* By default, solving a puzzle contributes 100 deep to its round name.
* It would also support two different overrides: a round-level override (changes deep-on-solve for the round)
and a puzzle-level override (changes deep-on-solve for that puzzle).
* Solving a puzzle can contribute any amount of deep to any number of deep keys.

To justify it, I gave examples of how to implement all of the proposals.

* Museum rounds: Every museum puzzle contributes 100 to "museum" and 1 to "X-meta" where X is its round. The meta for each round has deep key "X-meta" and all other puzzles have deep key "museum"
* Unlock an AI round after N solves in Act 3: each AI round puzzle contributes 100 to its own round and 1 to "act3-progress", the round opens at N act3-progress, then solves within the round contribute 100 to itself.
* Breakout: starting Museum puzzles could contribute 1 to "factory". The breakout puzzle would contribute 100 to
factory, then the starting set of Factory puzzles could open at 100 + N depending on how many we wanted open at the start.

This is essentially a generalization of JUICE, or a generalization of Puzzlehunt CMU's "solve K of N"
system. Each puzzle's database entry would only need to store 2 fields, making it easier to scan in Django
admin, with the heavy lifting of the logic done in Python. In the design notes, I mentioned that
arbitrary string keys could potentially be hard to maintain, but I expected us to have at most 20-30 different
keys and for the key names to be mostly self-explanatory, and that max flexibility now would let us adapt
more later.

I didn't get any pushback, and spent a day or so implementing it. Along the way, I implemented a "deep floor" system, basically our version of time unlocks. A deep floor is a Django model that essentially says, "team X should
always have at least this much deep for this deep key". Floors were created in a default-off state, and could
be turned on whenever we wanted to time unlock something, along with an option for a "global deep floor" that
would force a minimum deep for all teams. I also attached a UUID to each deep floor, along with a magic endpoint that a team could hit to opt into a deep floor. This was to support optional time unlocks similar to what
Palindrome did. (I asked Palindrome's team where this was implemented in their codebase, and they told
me it was implemented very rapidly during Hunt and I should just redo it on my own.)


"Wyrm is a Special Child"
--------------------------------------------------------------------------------------------

In the middle of me working on time unlocks, Ivan reached out and asked if I could help on making the
Hunt "feature complete" by December 1st. This was an internal deadline we'd set for creating a minimally
viable Hunt, where a team could go in, solve puzzles, progress through the Hunt story, and reach the end
of Hunt assuming they had all puzzle answers.

When I looked into the TODOs, there was a rather terrifying list of things to implement, including:

* How do we represent repeated Wyrm puzzles across the Museum and Wyrmhole?
* All the logic around team interactions for physical puzzle pickup, and Reactivation, including
the story state changes required for that.
* Connecting the story state to the Factory's display code to support shutting down the Factory and
slowly reactivating it.
* Implementing the teamwide dialogues and connecting them into the unlock and story advancement logic.

I decided to take the Wyrm and story consistency issues, while Ivan set up the dialogue logic ("it's just
Kyoryuful Boyfriend all over again"). The Wyrm logic was especially wild, since each puzzle is actually
displayed in up to 3 contexts:

1. As a puzzle in the Museum.
2. As a puzzle on the conveyor belt in the shutdown Factory.
3. As a puzzle in the Wyrmhole.

Each context needed to display the puzzle differently and uses a different URL and puzzle icon. Additionally, we decided that the errata and hint
state should be identical across a puzzle and its copy, but their guess states needed to differ, since the
Museum puzzle would unlock different things than the Wyrmhole puzzle. We *also* needed to make sure you
couldn't use free unlocks in the Wyrmhole round until after you met Wyrm post-shutdown. Internally solvers
unlock the Wyrmhole round as soon as the teammate interaction is done, because that is easiest for puzzle
management, but externally solvers don't know the Wyrmhole round exists yet, so we had to add an edge case
for that. The List of Puzzles page also needed to redact the
name of the Wyrmhole round until you met Wyrm again, which also needed to propagate to the navbar, and this
all needed to be tied to story progress not solve progress because you are at 2 Wyrmhole solves both before
and after talking to Wyrm...in short, there was an inordinate amount of work done for a section of Hunt
most teams sped through in 3 minutes. :sadwyrm: was definitely the most used Wyrm emote in the server,
used whenever "yet another Wyrm edgecase" was added.

You might ask, was this all *really* necessary? And the answer is pretty much, yes, it was. Remember what
I said earlier: the way you make a Hunt story work is by considering every aspect of Hunt and making
it self-consistent with where solvers are in the story. I don't regret making this happen. (I do want to point out
that this is case where choosing to repeat puzzles 3 months ago led to an extra ~10-20 hours of tech work down the line.)

Aside from Wyrm stuff, I went through the spoilr code and cleaned up a lot of the interaction handling to
fit our use case, and created triggers on interaction completion to fire updates to the story state,
as well as Discord alerting for those story states. We realized that as implemented, story state had
to contribute to puzzle deep, because after Reactivation, teams would either be before or after teammate arriving
to their HQ. If unlocks were only based on puzzle solves, there'd be no way to distinguish before and after.
Our travel time to team HQs would also vary, so a timer wouldn't work. So, the unlock system got modified
to allow arbitrary story interactions to either add deep to a team, or create a minimum deep floor for a team.

Tied to this was live updates of round pages without needing to refresh the site. This was originally
labeled as a "nice to have" issue, but I raised the point that shutting down Mystery Hunt *does not* work
if the site didn't live-update. Imagine that you talk to MATE, and the conversation stops, but
the Factory is still brightly lit and everything is interactable until you refresh the page. Doesn't that
really break the immersion? Ivan went "oh shoot you're right" and we immediately bumped it up to
site blocking.

Very quickly, I was sure that I wanted to make as few changes to existing code as possible. I was already
making wide-reaching changes to support Wyrm edge cases and really did not want to add more complexity.
The frontend gets its state initialized by an API call against the server that populates various
React components, and we decided the easiest solution was to add a websocket-based trigger that resent
the API call.

"Wait, but do we need to worry about self DDOS-ing our site"?

Assuming an entire 100-person team was watching the story, we'd get a quick burst of 100 requests, which
we decided was fine. What would not be fine would be if every tab from that team fired at the same time,
since that could look more like 3000 requests at once instead, so we added some logic to only live-update
the tab when it became active.


December 2022 - The End is Never The End is Never The End is Never
-----------------------------------------------------------------------------------------------

Near the end of Hunt, one of the team leads said Wyrm was intended to be the AI round with the least
tech work involved. Bootes and Eye would have really weird answer checkers, and Conjuri was clearly
going to be a lot of work, but Wyrm would mostly have normal puzzles.

This...did not end up being true. Surprisingly Eye ended up as the most straightforward rounds, since its
round art could mostly be shared with the Museum code. Sure, there was diacritic and language
canonicalization hell, but it wasn't too bad. In comparison, Wyrm had all these subrounds, puzzles that
were either metas or not depending where you were in the round, and five different sets of avatars that
were unlocked in tandem with each round.

IMAGES HERE

Hunt was becoming more real - people outside teammate are talking about it, wow! Boy they sure seem excited.
Meanwhile I was working longer and longer hours on Hunt, and it was accurate to describe it as work. My job
has a cap on vacation hours, and I started taking days off to make sure I wouldn't hit the cap, but most
of those days off were spent working full time on Hunt instead. It was feeling like I was always spending
my time running for something, work or Hunt or this blog, and it had been a while since I stopped.

There was a robot learning conference in New Zealand
that I'd be going to in December. I let people on teammate know I would be travelling for work for a week,
then taking a *real* vacation for another week. But before that:

Walking Tour
------------------------------------------------------------------------

We'd actually started ideating this puzzle at a Bay Area puzzle writing meetup in October. It was a pretty
interesting meetup, since the first thing we did was testsolve Hall of Innovation. We were given most of the
feeders for the Factory Floor meta. After getting nowhere, we were
told "by the way, your team has unlocked a new round". We did all of Hall of Innovation, then were told
"Congrats! The Factory Floor meta isn't ready yet, come back in a few weeks". Really one of the most
trippy testsolving experiences I've ever had. I'm pretty sure they were trying to test whether we would
solve the Factory Floor meta without the Innovation answers or the Blueprint diagram. We didn't, which
was good, but it's still wild that we were told to testsolve a meta, was given an entirely separate
round of puzzles, and then were told we couldn't continue testsolving the meta until they'd finished
doing more art.

But! We are not here to talk about innovation in Museum design. We are here to talk about innovation
in the Tower of Eye. A number of Eye puzzles were written at in-person meetups like this one, since
they were harder to design and needed more manpower to brainstorm. Our group aimed to come up with
a suitable Dutch puzzle. We considered many things: Dutch auctions, Double Dutch, something with
tulips, etc. A teammate mentioned KLM Royal Dutch Airlines gives out Delft Blue house souvenirs that
correspond to real locations in Amsterdam, bringing out the Delft Blue houses they owned. (We were at their
place. They didn't, like, carry this around on a day-to-day basis.) There was a canonical list of KLM
houses, with their number, year of release, and corresponding Amsterdam address on Wikipedia.
For any puzzle constructor, having a canonical dataset is immedaite puzzle bait. That was how we arrived
at a Google Maps runaround in Amsterdam, where the first a-ha was that you were in Amsterdam, and the
second a-ha was figuring out KLM houses existed.

The idea sounded good. We then sat on it for 2 months and wrote it over a weekend in December. We started
by collecting data about all the houses, deciding on "blue shapes" to avoid having to define what counted
as a window. We then wrote a bit of code to randomly shuffle different yes/no conditions until we found
a set of five conditions that gave the correct binary. The directions were then written in parallel,
with each author taking a few, followed by a combining step to reduce overlaps in locations and
make sure that we described locations in different ways if we had to repeat spots.

Since the path was generated by code, it took us a while to notice the 3rd bit of every letter in the
answer was 1, an incredible coincedence. We had an entirely different yes/no question there, that got switched out for the
one about alcohol to make the break-in for the 2nd part a bit easier.

\* \* \*
{: .centered }

We finished writing the draft of Walking Tour the day before I started travelling.
Although, on the flight over, I did spend a solid chunk of it working on the Appendix of the 5D Barred
Diagramless solution.

Funnily enough, there was a puzzlehunt at the conference! The puzzlehunt was a student outreach event,
but was open to everyone. It also...had real prizes? Like, $400 NZD (= $244 USD) for
the first team that finished. I caught up with the organizers later. They said that the conference gave them a budget and
most of the budget was turned into prize money. I did the puzzlehunt, and it had all the classic first-time
constructor mistakes, but I had fun and ended up placing 3rd which was enough for a $100 NZD gift card that
I used to subsidize my travel.

The vacation really helped - I needed a few days to get into the headspace of "you have no obligations today",
but once I did it was nice to have no stress about deadlines for a bit. Of course, I wasn't able to
fully disengage from Hunt, and ended up idly working out design for the last puzzle I'd write for Hunt.


Win a Game of Bingo
----------------------------------------------------------------------------------------------

Other people on teammate had asked me if I were writing a bingo puzzle, given that I run Mystery Hunt
Bingo. I kept telling them that
I would if I had a good idea, but I didn't have any yet. Over time, I stopped thinking about it in
favor of other puzzles.

Well, it was December, and we were in our final puzzle writing push. The official answer claiming deadline
was December 1st, but it had gotten pushed to January 1st, and the list of unclaimed feeders was dropping
fast, so it was now or never to make a bingo puzzle happen. I didn't feel like I had to write one,
but it would be an enormous missed opportunity if I didn't.

The bingo puzzle in Mystery Hunt 2021 was based around scoring a Mystery Hunt bingo board, but
the author's notes for the puzzle mention a different aspect of bingo: rigging the board by refreshing until you get a board that you
like. This has happened for teammate before (ADD LINK HERE). Independently of this, I had considered making
my site pseudorandomly generated based on a given seed, to make it easier to share bingo boards with
other people. I remembered the Bloom Filter puzzle from Mystery Hunt 2019, where you were supposed to
look at the source code, and I rembmer how I initially spent time trying to reverse-engineer the
hashing process. This wasn't the right way to solve the puzzle (you were supposed to just throw a
dictionary at it), but it inspired the core idea of reverse-engineering a bingo generator.

I wrote up a sketch of the idea during vacation, saying I could take any answer, but wouldn't be
free to work on it until later. It got approved and I started work after I got back to the US. My past
experience with blackbox puzzles is that you don't need to be very tight on designing the solve path.
As long as the solution is unique, solvers can usually find a way. That's not to say you don't think
about the solve path, but it is pretty hard for the puzzle to be fundamentally unsolvable or unfun
once you have a proof of the solve path. And then, once the blackbox was designed, I was pretty confident
I could bash out the code in a day or two, since the tech was Collage but easier.

The issue was that I had no idea how to design a unique solution, and I wanted to get it done ASAP so that
I could spend more time on tech. I ended up asking if Justin was free, since he'd written
[Functional Analysis](https://2020.teammatehunt.com/puzzles/functional-analysis) and this would be very simliar.
He was! We met up and I explained the rough idea. I had already pretested the cluephrase on a few people,
and they said the right idea, so we knew the target seed length was 9 letters.

This puzzle was slated for Conjuri, which was targeted to be an easier round. There were a few reasons for
this. It was the last round, and we wanted it to act more like a victory lap than a gauntlet. Since the puzzles
would be opened last, any hard puzzles were much more likely to get completely skipped. The focus of the
round was meant to be the game, and it'd be better if more teams got to see the whole thing. Feeder release
of Conjuri would be last because it was blocking on game development, so puzzle development time would be
short, and that meant a bias towards puzzles that could be written quickly. This doesn't inherently
mean "easy", but it does push in that direction, because it is faster to design a puzzle with one a-ha
instead of two.

Justin suggested that in any game of Bingo, there were going to be three parties: you, your opponents, and
the person calling out numbers. So that suggested having each letter act as a function on one of the three
parties. That decreased the "depth" of the puzzle, since you'd only need to reason up to 3 functions deep
once you understood what was going on. He also suggested the core idea used in the puzzle: numbers in our
grid should satisfy some property X (like "even"), numbers in our opponent's grid should satisfy some disjoint
property Y (like "odd"), and the numbers called should be very biased towards Y. The functions should then generally
keep property Y true, except for a small number that pushed numbers towards X.

That's all very abstract, but based on that I decided it'd be easiest if our numbers started low and the opponent's numbers + numbers called started high, with some functions adding and other functions subtracting.
We realized the functions needed to be noncommutative, which led me towards permutations, since I figured
they'd be easier to derive from the data.

As I messed around with permutations, I found I didn't have too much trouble designing the functions to
make the letters for our grid and numbers called to be unique, but I had a lot of trouble designing the functions
to make the opponent's grid unique. I was trying to have the functions apply identically to all three grids
to reduce work, but it was just very hard to prove the opponent's grid would always win before our grid
in every combination except one. So, I relented, and allowed letters to act differently between the opponent's grid and the other ones. I was concerned about changing the number of functions from 26 to 52 (combinatorial
explosion is very scary), so as a compromise the opponent functions were forced to follow a stronger
pattern. During design, we were planning to require all seeds to be 9 letters. Editors pointed out it
could be really hard to break in without a lot of data, so we adjusted to 1-9 unique letters to make
it easier to get started. The nature of most blackbox puzzles is that they start very hard and end
quite easy, and our thinking was that if you could enter 1 letter seeds, you'd be able to run any experiment
you wanted without having to muddle through unknown intermediate transformations. That ended up working exactly the
way we wanted in testing.

I will just spoil the reason I wrote this puzzle now: when you win the game,
the puzzle directs you to reverse the winning seed. (I added the reverse step because I knew with 100%
uncertainty that people would try to nutrimatic from 3 letters. All the testsolvers did it, the one
Mystery Hunt writeup that mentioned Bingo did this - y'all are too predictable.)
The reversed seed would point to the external Mystery Hunt Bingo site, and using that site would give
the final answer. So, how did that work?

I obviously did not want to repeat the same logic as the puzzle. If anything, I wanted the bingo site
to be as random as possible. If you check the source code, the incoming seed is passed through a hash
function to generate the seed for a PRNG, which is used to sample from the list of phrases. By doing this it's
practically impossible to reverse engineer a desired board. However, as the site maintainer, I have
control over the order of the phrase list! The PRNG just outputs a list of indices, so after locking
in the seed, I just changed the order of the phrase list in the code such that the generated indices
lined up with the right phrases. It's been a long long time since I've done any crypto but I'm pretty
sure this protocol isn't breakable even if you knew exactly what it was before Hunt.

The side effect is that right now, I can't adjust the phrase list without breaking my Mystery Hunt
puzzle. I'll either hardcode the edge case or make a standalone copy of the bingo that can be hosted
from the MIT webserver. One of the two.

In the actual Hunt, teams got more stuck on the last step than I expected. I have the stats on time
between winnning seed and solve - they are all longer than the 2-4 minutes I wanted it to be.
I think this was a victim of "people on teammate know what Mystery Hunt Bingo is, people outside less
so", and I do wish the game win message was more direct about what the message meant.

Fun fact: the answer for this puzzle was picked entirely as a joke reference to it appearing in so many
recent puzzlehunts. I mean, the bingo eventually points to a puzzlehunt in-joke,
of course the answer is going to be another puzzlehunt in-joke.

\* \* \*
{: .centered }

There were two other major things to work on before December. First was physical production of a puzzle
that was a long time coming.

Weaver
------------------------------------------------------------------------------------------------

They say all good stories end where they began. Well, Weaver was one of
the first feeder puzzles I helped brainstorm, and now it was one of the last I'd help make.

"Make" is a very literal word here. Much of the puzzle idea and design was done by Brian. At most
I pointed in the direction of weaving patterns and testsolved a few versions on paper. Meanwhile, Brian
was doing some experimentation with hydrochromic ink on different materials. He arrived at wooden reeds,
covered with a coat of hydrochromic ink, with regular white paint on top, and regular black paint on top
of that. This sufficiently hid the underwater portion - the hydrochomic ink does feel a bit rough, but
if the entire reed is rough then that isn't too out of place. We assumed teams would not make the reeds
wet until instructed to, because what if the paint washes away? (A few teams *did* immediately put the
reeds in water to make them bendier, and sent a hint asking if they messed up. Answering those
hint requests was pretty tricky.)

A version made by hand was successfully testsolved at retreat, which left figuring out how we were going
to mass produce the puzzle. We spent some time looking for a company to do it, but weren't able to find
one, which wasn't too surprising.

Over the course of a few months, different production designs and prototypes were made. The core idea
in all the prototypes was building a stencil. There would be four different stencils, for front-white, front-black,
back-white, back-black respectively. The reeds would be held inside a frame maintain consistent alignment.
The frame was iterated a few times, with the last one being two flat pieces of wood, each etched with grooves
where the reeds should go, that were held together with binder clips for easier assembly and disassembly.

A teammate who could get to a makerspace in Berkeley ordered a bunch of wood, to laser cut the stencils,
frames, and triangles for The Legend. The wood arrived! It was then stolen from their mailroom. So they had
to ship another order. I would ask "who steals a bunch of wood from a mailroom", but, well, clearly it
was in high demand, and maybe it's easier to resell wood if you aren't stealing it for yourself.
(Yes, this is why people read my blog, for the riveting package stealing commentary.)

At a number of Bay Area retreats we'd tried making proof-of-concept stencils with the raw materials we had.
The plan was to spray paint the regular letters, and the first small scale trial showed that spray paint
would gum up the stencil holes really quickly. We'd either need to wash the stencil regularly to keep the holes
clear, or do something else.
We tried an airbrush owned by another teammate, and it did not have the same problem.

In late December, after Christmas but before New Year's, everything was ready for the final
mass production push. The Weaver assembly line (or puzzle factory, if you prefer) was set up in a garage to
allow paint fumes to escape, and had many steps.

1. The reeds were delivered in large rolls. One person would cut the reeds to a reference length using a
tree cutter.

IMAGE

The reference length was the size of the frame, plus a bit extra to give leniency.

2. The cut reeds would be painted with a coat of hydrochromic ink by hand. We did this by hand because it
was an even layer over the entire reed, so precision was not important. Saving ink was. The hydrochromic ink
was produced in Europe, and we would not be able to ship another container of hydrochromic ink in time
for Mystery Hunt. Our target was to produce 80 copies, but realistically we were going to make strips until
we ran out of special paint.

3. The loose painted reeds were placed inside an "oven", which was a portable heater directed towards a cardboard box. This was to dry them out faster.

4. Once dry, the reeds were sent to a framer. Reeds that were too bent were thrown out. The remaining reeds
were placed into a frame, then sent back to the painting station for touch-ups. It was hard to tell how
much hydrochormic ink to apply the first time, because it applied clear and dried white. The more you painted,
the more it looked like you were undoing your work, so we decided on a two-pass system.

5. After touch-ups, the frames were placed back in the oven for another round of drying.

6. The airbrushing station would take dried frames, and airbrush the remaining letters. One person manned
the airbrush, and a second person kept track of which stencil to use, did a final factcheck the letters
were correct, and bagged the reeds into individual puzzles.

The airbrush was the bottlenecking step, and it took 8 people manning the pipeline to saturate the
airbrush. (4 painters, 1 framer, 1 airbrusher, 1 airbrush supporter / quality controller, and
1 cutter / painter). We had enough paint to make 83 sets, which were painted over two 12-hour shifts (taking breaks for lunch and dinner). So, very rough order of magnitude was $$12 * 8 \approx 100 hours$$ for 40 sets = 2.5 hours per puzzle.

I don't think many teams made it all the way through the puzzle without relying on the virtual version, which
was unfortunate. There were differences between the paint-by-hand and mass produced version, and I would guess
the coat of paint was less thick. Paint naturally flaked off when you messed with the strips (remember,
you can't put a top coat since the water has to reach the hydrochromic layer for the puzzle to work), so a bunch
of teams lost too much paint by the end to do it as intended. Hopefully the intermediate a-ha was exciting enough
to make up for the deficits of material science. If a team asked for the answer after putting the weave
underwater and putting effort into the last step, we just gave them a picture of what the final basket
should look like.

\* \* \*
{: .centered }

The other major thing to work on was preparing the site for the 1st of many full hunt speedruns.

I think pretty much every Mystery Hunt does this at some point. The idea of a full hunt speedrun is not to
testsolve the puzzles. It's to test the overall logic of the Hunt structure and site. Teams enter
puzzle answers as soon as they unlock, and report any bugs they find in the unlock logic or site behavior.
Our site had a ton of intermediate states, especially in the Puzzle Factory. Monitors in the Factory
would display different content based on story progress (as measured by solve state and story interactions
completed),
the scrollable region within the Factory would rely on this as well, and puzzle display could be
very conditional on solve progress as well. So, this step was extra important for us.

I had implemented the unlock system by this point, so most issues were a matter of adjusting constants in
a config file, AKA work that I shouldn't be doing. What needed more love was the internal HQ flow.
A good way to view Hunt tech is that you spend the entire year designing processes and user experience
flows to make running Hunt itself as smooth as possible. Running Hunt HQ is hectic and mentally draining, so
you are trying to think up good tools now to reduce thinking later.

The main parts I was working on were connecting tph-site email handling code with spoilr's email flow. This
was a classic case of two codebases having two different functions that did similar things, and needing to
pick which one to keep and removing uses of the other. Along the way, I helped fix some longstanding bugs
in our interactions flow (roughly, "a team wants to do something with HQ", make sure it creates an
action item that someone can follow-up on to coordinate with that team), and setting up predefined
email templates for puzzles that relied on user submissions. This was also where we put standard hint
emails, like the one to send if a team asked why the loading animation in the Museum never finished.

The other big piece I worked on was the Wyrm round page.
The plan for the Wyrm art was to use tessalated puzzle icons, where some rounds would have multiple
copies of a puzzle icon to play up the fractal / self-similar aspect of the round. Tech-wise, our plan
was to have each puzzle icon be the same size as the round art, all overlapping each other. Each
of those icons would then have an SVG mask applied to its click region. Whenever the mouse entered
a click region, we'd display that puzzle's info on-hover.

This broke one of the rules we'd placed on round art: that you could tell at a glance which puzzles
were solved and which were not without interacting with the page. This was a rule put in place
to improve legibility of the hunt state.
In the Museum round, this was done by showing the answer below a puzzle's title if completed.
In the Puzzle Factory rounds, the highlight color of the puzzle icon would shift. A subtle change,
but better than doing nothing.

For Wyrm, it seemed bad to draw puzzle titles onto the round art, and it also seemed bad to highlight
puzzles differently. Either of those changes would need to carry over when zooming between layers,
and it felt like it would distract from the existing art, which was already really cool. So we decided
to break that rule, just this once. There would be a list of puzzles at the bottom anyways.

From an implementation standpoint, the main issue was that we needed to have very custom puzzle icon code
to make this work. "Tessalating puzzle icons" means that every icon's click region is both non-convex
and disconnected. Luckily SVGs are magic and handled this fine. Each icon was overlaid + displayed
normally, and made 50% brighter on hover. However, this had a different issue. It made the round
noticably laggier.

What we hadn't realized was that if you implement the round this way, each puzzle icon in a layer is
a full 1024 x 768 image, since the network (and rendering) cost is based on the size of the image
before trimming it to the clickable region. When zooming into or out of a layer, pop-in was a lot more
obvious.

IMAGE

Our solution was to send a single image combining all icons in the layer into a single image, to cut
the download requirements down. Then instead of brightening icons on hover, we created a 50% transparent
white overlay, cut to shape of the puzzle icon, that would appear on hover. This could be done
entirely in CSS and would be much faster, without needing to define a separate `<img>` tag per puzzle.

I am leaving out many random details about manual surgery of SVGs, fixing z-index ordering issues,
and other tedious visual bugs that were boring enough that I don't even remember them. It was a lot,
and maybe someone with more CSS knowledge should have done it, but somewhere along the way Wyrm
had turned into my baby and I was going to be damned if we didn't pull it off.

IMAGE

(Move the speedrun introduction down to here, segue into it from the Wyrmhole side)

Much like the in-person retreat, the full site speedrun had its own set of secret objectives.

The first was breakout. We had used up our one fully unspoiled test at retreat, but there were still
plenty of people who knew breakout existed, but did not know where it was or the mechanics of how to
solve it. That was ready, we'd done most of the action items shortly after retreat.

The second was Hall of Innovation. This had been testsolved multiple times by now, but every testsolve
group had been given a copy of the site where only the Factory Floor and Hall of Innovation rounds
were complete. Now that a lot more puzzles and art assets were complete, the gizmos on the Factory
Floor would be disguised better, and this was a place to test that they were still discoverable.
(By now, team exec had decided to not do a structured full-hunt testsolve. There were already more
pending testsolves than we had capacity to test, and coordinating everyone to meet at once would itself
take time, so people were instead instructed to form groups and start testsolving any open puzzle
whenever they were free.)

The third and last secret objective was Wyrm backsolving. Long ago, we said we would test the backsolve
step in a full Hunt testsolve. With that off the table, it wasn't clear where else we'd test the Wyrm
backsolving. There was some miscommunication between Wyrm authors and EICs, where they thought
the 4th layer would start with Collage shown as the meta immediately on unlock. After clarifying that
wasn't how the round would work, it got added into the speedrun. But the more specific consequence
was that a bunch of Wyrm art and tech deadlines got moved from "days before Hunt" to "2 weeks before Hunt".
I'm very grateful to the artists of the Wyrm layers for getting art done for the layers so quickly after
we told them the deadlines had moved up.

The speedrun got up to Hall of Innovation. I recused myself to go eat dinner, and came back to my speedrun
team in the middle of solving the round. They had found the gizmos that changed the puzzles, and submitted correct
answers to some of them, and that was considered "good enough" to fastforward them to the next part of the
speedrun.

(This is an aspect of writing hunts that can be a bit disappointing. Your teammates write these really cool
puzzles that you get to testsolve, but you don't get to testsolve most of them. Everyone testsolving
all puzzles is super inefficient, but most people need to be spoiled on puzzles to prepare for writing hint
responses and answering emails.)

The speedrun made it up to the last layer of Wyrm, and we told them "this is now a testsolve, please figure out
what to do". This was marked as the last part of the speedrun we'd do. The speedrun had run late, and had already
surfaced a bunch of site issues that we needed to fix. Between all three teams, there were only 7-8 people who
were unspoiled on the Wyrm metameta. Since we'd precommitted to only doing one test of the backsolving, we
decided it was okay to pool all the unspoiled testsolvers into one voice call to let them figure it out.

Two people in the spoiled group set privste messages about noticing the looping puzzle in the art, and the unspoiled
group ended up looking at Collage due to noticing the triangle pattern in all the previous metas. They only
noticed the looping art after backsolving ("oh god, we're dumb"), but I was mostly relieved that all the parts
of breakout and Wyrmhole had tested the way we wanted.


January 1-8, 2023 - An Exercise in Juggling
------------------------------------------------------------------------------------------

> There are decades where nothing happens; and there are weeks where decades happen.

As we get closer to the start of Hunt, I'm going to need to segment things more.

Puzzle postproduction and factchecking was continuing at a breakneck pace. I caught up on writing a bunch
of solutions that I'd been putting off for a while - Bingo, 5D Barred Diagramless, etc. But after doing so,
I did not do many more postprods or factchecks, because I had more important things to do with my time.

At this point I am just trying to go through the copious number of tech issues surfaced during the
speedrun. "Wyrm should save its zoom level", "I hit a 404 on this link that showed up",
"notifications are going off-screen", and so on. Each issue was prioritized and we went down them in order.
Meanwhile puzzle postprods and factchecks are getting merged in at an absurd rate. Thank goodness for
auto-postproduction, because boy were there just a lot of words words words to copy into code.

This was all in preparation of a second full hunt speedrun, to happen the last weekend before Mystery Hunt.
Not everyone was free for the first one (given it was in the middle of the holidays), and it wouldn't
hurt to have another round of testing.
For my part, I stopped working on the tech issues IDed last time to do another important bit of work: load testing.

This would be the 3rd time I'd done load testing for a puzzlehunt. Puzzlehunts are not *that* big, if anything
they're quite small, but load is very bursty at the start of Hunt and POST requests are coming in all the time.
In my experience, I've seen two classes of errors.

* Your HTTP handlers are overloaded. In this scenario, requests to the server will be replied to very slowly,
but the server and database will mostly not go down, it'll just be laggy. This is usually only a problem
at the start of Hunt, and you can either ignore it or just make the server CPUs more beefy.
* Your websocket handlers are overloaded. The websocket connection layer in tph-site is mediated by the database,
and having too many websockets open at once causes the number of active database connections to rise. If the
database connections go past a max limit, any DB query will fail and the entire site stops working. This
is a much worse problem, because it's a configuration problem, not a server-size problem.

The last two times, I load tested using [Locust](https://locust.io/), since there was example code
in the Puzzlehunt CMU codebase. This time, I decided to switch to [k6](https://k6.io/), because I was most
concerned about the websockets for chatting with MATE. The documentation for testing websockets in k6 was
a lot more complete, and I had example code from a Huntinality script. k6 also describes user behavior in
Javascript, and although I am more of a Python fan, the frontend of our site was in React, so there was
less work needed to translate our site's code into load testing calls.

Broadly, the way all these load testing libraries work (Locust, k6, etc.) is that you define the access
patterns of a hypothetical user. "This user will login, wait a few seconds, open the list of puzzles,
open a random puzzle, and submit a guess." Something like that. They will then provide utility functions
that let you spin up users for some period of time, and provide statistics at the end. The main work for
loadtesting is in setting up the mock requests.

The thing we were most concerned about for loadtesting was the collaborative jigsaw for breakout. Most
teams would go through it during Hunt, the collaborative cursors were going to send a lot of websocket
messages to sync cursor positions, and we wanted to make sure it wasn't too laggy. I set up some mock
behaviors for chat, Collage, and the breakout jigsaw. The chat users would connect to MATE and send
2 messages per second. The Collage users would load the puzzle, send 1 guess per second via HTTP, and verify
receiving team-wide updates on recent guesses via websockets. Last, the breakout users
would load the jigsaw, and attempt to move a random piece to a random location every 0.3 seconds.
I verified that the proof of concept worked for simulating 1 user locally.

VIDEO HERE

The load test scaled up to 10 users, then 25 users, at which point we noticed some slowdown. This was
expected though, our cloud machine was still small scale and we hadn't provisioned the full-size Hunt
machine yet.

For this speedrun iteration, a small group of people acted as a mock HQ, and some puzzle answers would say
"send a hint to get the answer", "request a physical puzzle pickup to get the answer", "oops this puzzle
is impossible, redeem a free answer". This was to get all those aspects of site behavior tested as well.

Once again, there was a secret objective: we would do a deploy to the site during the speedrun, to measure
the length of the deploy when there were active users, and the amount of interruption from the solver's
perspective. There was also an extra secret objective: load testing.
I'm pretty sure the only person who knew I was planning to do this was Ivan.

![The conspiracy](/public/mh-2023/50test.png)
{: .centered }

Essentially, I was concerned that our tech-only tests were a good but insufficient test of what a DDOSed
site would feel like. So I would run a 50 user load test in secret, and take it down only if unaware
testsolves complained.

I noticed some slowdown on entering guesses on the site, but nothing major. However, as teams solved the loading
puzzle and hit the collaborative jigsaw, the lag got a lot worse. It was to the point where one person would
drag a piece, and it wouldn't show up on anyone else's screen until several seconds later.
I confirmed I saw the same thing, then killed the load test. It was quite a sight to see 20 jigsaw pieces
all snap into place as the other load went away.

![Load test stats](/public/mh-2023/loadteststats.png)
{: .centered }

Brian was of the opinion that the stats would look much better if CPU load weren't at 100%, but he'd also take
a look at breakout and Collage to see if there were easy performance wins. He found some places to batch
websocket updates and add more caching, we tried another run of 50 users, and it did look better - but we
made a note to try again with the full-size machine.


January 9, 2023 - Four
------------------------------------------------------------------------------------

I had taken (another) week off work, but this time it was more justified. It's the week before Hunt,
even when I am not writing Hunt I can't get much done those days. I left the Bay Area in the morning, to
land in Boston in the evening.

I was still in the mode of "optimizing how I use my time", so I prepared a bunch of work that I could
do on the plane.

It turned out my plane had in-flight WiFi (praise JetBlue). I had trouble connecting to it from my laptop
for whatever reason, but I got my phone connected just fine, and ended up spending my flight doing code
reviews from my phone. As I tapped out a comment on my 8th code review of the flight, a part of me
acknowledged that trying to squeeze in Hunt to this many aspects of my life was really not a healthy
thing to do.

For the week before Hunt, teammate would have an informal HQ in an AirBnB we booked near campus. Around
half the team would stay there and the other half had booked hotels, had nearby friends, etc. Our
on-campus HQ would not be available until the Friday of Hunt, although we did get a classroom booked
the Thursday before. When I landed, I asked for directions to the AirBnB, and was told the best balance
of money and time was taking the Logan Express.

So, I stood at the Logan Express stop, checking directions on my phone, and wondering why it claimed
taking public transit would take 5 hours. A few searches later, I found that the last Logan Express
of the day had left 10 minutes ago.

Shit.

Luckily calling a rideshare wasn't too bad, and I got there in time to see Ivan complain about
responsive CSS and "why can't all computers have the same size screen". The Wyrm round art had a hardcoded
width/height that didn't fit on everyone's computer during the speedrun, and I had pawned off
fixing that to Ivan so I could focus on connecting Conjuri, Terminal, and other interactives puzzles
to my load testing script. For me this was the best trade deal in history, I will take infrastructure
over the complexities of frontend any day of the week, but I did feel bad about it.

I finished writing up the appendix that I had worked on during the flight, and sent it off
to postproduction, with the note that it would be really long. Then I went back to my hotel to
sleep.


January 10, 2023 - Three
-----------------------------------------------------------------------------------

This day was a bit quiet, since not everyone had flown in yet. Most of my time was spent
addressing factcheck feedback and revising puzzles in turn.

A few months back, we had set up Puzzup to auto-generate a factcheck template spreadsheet for each
puzzle. Factcheckers would fill it out and it would auto-generate a comment in Puzzup when done.
I'm not sure how helpful the comment was, we hit message length limits often, but the spreadsheet
definitely helped!

The complexity in how we displayed puzzles on the site definitely contributed to difficulty
of factchecking and postproduction. I think this could be useful, so I'm actually going to just
directly pull out the list of questions in our factchecking template.

Instructions: Fill out this checklist. Completeness and Correctness are the most important points to check
carefully. If it does not apply, note it can skipped.

Completeness and Correctness

* With the help of the solution, I solved the puzzle 100% (all clues, no nutrimatic, etc)
* By using the solution, I did not need to think very hard to solve the puzzle.
* If there is "uniqueness" invoked in the puzzle, uniqueness is checked. e.g.. uniqueness in a logic puzzle, answers to clues, etc.
* Make sure facts are still true. **List info that could change on or before 1/13/2023 under Other Findings and set the status to TIME.**

Puzzle

* Copy to clipboard matches puzzle in both Firefox and Chrome
* Second person (if there's any person at all), present tense narration
* Flavortext is italicized. Non-puzzle info (e.g. controls, accessibility, and contacting HQ) should come with the "info icon" (Post-hunt note: this
is the specific "i in a circle" icon that can be seen on Museum Rules)
* Interactive components work for the intended solve path, doesn't crash on incorrect/malformed inputs, any shared team state is tested
* No unintended source code leakage (inspecting code doesn't spoil puzzle)

Display

* Puzzle is displayed (post-prodded) correctly and tested on small/large window sizes, interactions tested on both Firefox and Chrome (or note which browser was tested)
* Puzzle is displayed (post-prodded) correctly on a mobile device and fully navigable, if interactive, via touch
* Act 1 only: Puzzle looks correct in the void, i.e. no white on light text (Post-hunt note: after solving Reactivation, teammate shuts down Mystery Hunt. Museum puzzles could still be accessed from the Puzzle Factory,
but were put in "dark mode" to signify that they were in a void "outside of puzzlehunt space", so to speak. Sometimes a puzzle would hardcode text colors that worked in light mode, but not dark mode)


Accessibility

* Do all images with puzzle content have alt text? If there's text in the image, can it be typeset with CSS instead of an image?
* Is the puzzle color-blind friendly? Does it rely on colors or can they be replaced with symbols or text?
* Do audio clips have transcripts when possible? Do videos have synchronized subtitles?
* For interactive puzzles, does it require very fine motor control? Is there an undo button if misclicks or typos will cause solvers to lose large amounts of work?
* Does the puzzle use inclusive language? If the puzzle talks about people, does it contain a representative sample (gender, race, age, etc.)?
* Print preview looks reasonable (can see all parts of crossword, clues, no unnecessary black backgrounds) on both Firefox and Chrome

Solution

* The answer is in Answerize format (bold + caps + monospace). All cluephrases are caps (not bold or monospace unless necessary).
* If puzzle involves images/visuals, they are included in the solution
* Solution is in first person plural ("we", "our", etc). It does not use terms like "the solver", etc
* I've reported typos and grammatical mistakes in the solution.
* Solution is displayed (post-prodded) correctly.

Depending on the puzzle, there could be up to four different "versions" of it to check (the displayed puzzle,
its display in dark mode, its display in print preview, and its copy-to-clipboard). I think almost every puzzle I helped on failed
its first round of factchecking, aside from silly ones like the Wyrm backsolve puzzles. So a lot of my time was
spent fixing CSS issues or adjusting clues.

When looking for suitable background music, I learned that Demetori had released a new album just 2 weeks prior.
(They're a Touhou circle and one of my favorite music groups of all time, mostly progressive metal.) It was pretty good
and I spent most of the week playing it on repeat. The Youtube comments taught me facts no one should know ("corgi" is
edit distance 2 away from "orgy"), but I couldn't help thinking about the album's closer - ["It's Better to Burn Out Than To Fade Away"](https://www.youtube.com/watch?v=VCHcSX3SX4Y).
It seemed like a fitting sentiment for the final push towards Hunt.


January 11, 2023 - Two
------------------------------------------------------------------------------------

The tech work continues!

There were a rather worrying number of features left to implement, including fixing print previews,
cleaning up more gaps in the interactions flow, and adding a Contact HQ + puzzle feedback flow.

If we were not implementing it so late, Contact HQ would have been done properly, but we were doing
it late and all bets were off on code cleanliness. Internally the Contact HQ page is implemented
by having an internal, unsolvable puzzle, that does not show up in the list of all puzzles,
starts with hints unlocked, and has no cap on number of open hints. Is this awful? Absolutely. But,
it was the fastest thing to set up and we were at the point where everything needed to be speed.

Speaking of speed, I was testing the speed on Conjuri under load. I had zero idea how the game worked,
but got some pointers from the main Conjuri devs for how to start games and move within them. The
load test was to login, connect to a Conjuri slot, then send a bunch of move commands (10/second,
roughly the speed they fire if someone holds down the arrow key). Along the way I found a small security
flaw that could have allowed teleporting anywhere in the map, including past locked doors (oops).
My load test script would sometimes enter monster fights, and then immediately get stuck because
you can't enter move commands in the middle of a fight, but I decided this was fine. The goal
was to test load, and the server still had to respond to move commands even if you couldn't do
them, so it was still simulating load of *some* kind.

Brian had provisioned the full-size hunt server, and I hit it with 250 users. This immediately
caused errors, but the site seemed fine, and the error message codes I was seeing looked
more like networking issues. Having done this before, I asked if our AirBnB WiFi could handle
250 fake people at once, and then immediately answered my own question "no of course it can't
you should have remembered this the first time". (When loadtesting Puzzles are Magic, I spent
two days trying to fix a bug that didn't exist, until I tried running load tests from
my work WiFi instead of my puny home WiFi.)

A teammate had a spare DigitalOcean machine they used for personal stuff, and we moved load
testing there. The new 250 user test went fine. Great! Let's scale that up to 500 users.
The CPU load looked fine, but we started seeing real site errors, ones that were breaking the site
for people factchecking puzzles.

Oh dear. I left it up for a bit so we could collect more stats, then turned it down. The errors
were identical to the ones seen when I loadtested Playmate 3 years back, caused by having too
many open database connections. We added back the same solution of using [PgBouncer](https://www.pgbouncer.org/)
to do connection pooling, and swapping our Redis layer in Django Channels to a RedisPubSubChannelLayer.
While Brain looks into how to do this, Alex Gotsis comes by with some news.

He explains that the site sends us an email whenever we have a server error. Sent emails
create and save an `Email` object to our database, to integrate with our email handlers later.
So, suppose your site has no database connections left. It'll have an error, and the server will send an email.
The Email object tries to get saved, but fails since the database has no connections left. So the site
has an error, and will send an email. That email will try to get saved to the database, and fail, so the server
will send an email. Which...you get the idea.

I said "Oh no" and he said "Don't worry I'm fixing this". So, that got fixed and we removed a
few thousand emails we spammed to ourselves. Phew.

After all the websocket improvements were in, I reran the loadtest and the site no longer
broke at 500 users. The Django Channels swap also appeared to cut our CPU usage by 3x, which
was cool. Between both of those, I was willing to declare the site battle-ready.

The one issue flagged was on Conjuri. All the Conjuri instances were running in the same
server as the rest of the Hunt. The codebase had evolved that way and we did not have the
time to both refactor it to a separate machine and adjust our deploys accordingly.
I gave my opinion that 500 users continually submitting guesses and moving 10 times per second
in Conjuri was likely a big overestimation. I was pretty sure our true load would be lower,
and Conjuri wouldn't be seen until late Saturday or Sunday, which would have less load than
Friday. That sounded fine to Conjuri devs - or at least, the TODOs on fixing game bugs were
more obviously important than hardening against load.



January 12, 2023 - One
-------------------------------------------------------

Thursday was the first day that we had a room on-campus. We were also
starting to spot people who definitely-looked-like-Mystery-Hunters
in nearby hotels.

We first had a too-long conversation about whether we should draw the blinds on
the windows for our room before we went into an incredibly spoiler-heavy
presentation, and decided that no that was overkill. We then had a presentation
about the story of Hunt, structure of the rounds, and different responsibilities
that would need to be filled to man HQ. The previous day, a spreadsheet
had been shared, and people were asked to fill it out, but today that request
was now a demand. "Sign up for a slot or we will *find* a slot for you."

I checked and had been pre-signed up for "tech on-call / contact", so problem solved.

We then had a training session for how to use the hint and interactions
interface, which I mostly tuned out because I'd been working on that flow for a while. The remaining time was allocated for people to finish up any
last minute tasks they had.

I went through the list of tech TODOs, which was now decidedly at the point
where we knew we would not finish all of it and would need to decide what we
wouldn't do. Solve sounds - cut. Easy way to see feedback sent during Hunt - cut.

This made some people on teammate quite sad, but there were just too many other things. "We don't have bandwidth to do this. If you have bandwidth to help on tech, we'd prefer working on these prioritized issues, but if you implement a deprioritized feature and send a PR someone will probably review it."
And that's why the Hunt has solve sounds, someone decided to do just that.

To more directly hit the broader point, Mystery Hunt is an enormous thing.
It is big enough that even individual people making it are not aware of
all the things going on. Nor should they be, the coordination overhead
would be too big if they were. But, the natural consequence of this is
that sometimes people's beliefs of what is getting done differs from what's
actually getting done. Tech is just one example of this. I
got passed copy of the MATE tutorial we'd link between kickoff and
puzzle release, and ended up rewriting a bunch of it because half of the
example commands didn't pass our chat parser.

Still, this is approximately the level of chaos I expected the day before Mystery
Hunt, so I wasn't too concerned. Logistics got ironed out, more bugs got
fixed, and the site was progressing well. We started organizing our tech on-call
shifts, and I signed up for the 1 AM - 7 AM shift originally, before reconsidering and asking if I could take the one ending at 1 AM instead.

As the last action item before Hunt, we did a "slowrun", where instead of solving
puzzles as fast as possible, a group of us would solve puzzles one at a time,
pause at every meaningful state change, and verify the site's behavior was
consistent. The original target was 7 PM, but it got pushed back, and back,
and back...

The delay was bad enough that we were going to lose our room booking. It was going
to be a long night. Everyone not involved in verifying the slowrun was told
to get some sleep before Hunt, while the rest of us headed towards the AirBnB
HQ.

The slowrun officially started around midnight, and over the course
of several hours we proofread all the site pages, checked that puzzles and round
pages unlocked properly, that the Puzzle Factory was gated properly, that
story updates and interactions appeared right when metas were solved, and so
on. We also checked display issues similar to the factcheck spreadsheet -
how the site worked at multiple screen sizes, were the Puzzle Factory monitors
swapping properly as metametas (feature requests) were unlocked, etc. We *also*
checked the logic that puzzles would start internally unlocking 1 hour before
puzzle release (to reduce server load), while still being blocked from loading.
We merged in some final visual touches as we went along the slowrun, and by the time
we finished it was around 8 AM and kickoff was in a few hours.

A few people started heading right to campus to prepare for kickoff, while I looked
for a corner to take an hour long nap before Hunt got started. I really should have just
gone to bed, but it's the first Mystery Hunt kickoff after two years of remote Hunts.
Proper sleep can wait.


Friday of Hunt
-------------------------------------------------------------------------------------

I arrived at Kresge a bit out of it, but intact enough to say hi to people lining up outside.
I didn't have too much to say - talking about Hunt is something you do after, not before.

Everyone was to wear their "Museum of Interesting Things" staff T-shirt. MATE was printed
on the back, but we would only reveal this when MATE was revealed at kickoff. Basically
this meant everyone needed to wear a long-sleeve shirt or hoodie that would hide the back.
I helped usher people into the auditorium, and got the sense it was about as full as
previous years, which I wasn't quite expecting. I figured there'd be less presence due to
COVID concerns, but it didn't look that way to me.

I saw kickoff for the first time with everyone else; I'd been too caught up in tech to
see any of the rehearsals. The stage was decorated with a bunch of random decorations
people owned or bought from Home Depot while getting construction supplies for set
creation. Based on all the people taking pictures after kickoff, it looked like a few hunters were trying
to image ID the decorations, just in case they were a puzzle.

Now, we unfortunately did not have our official HQ in Building 10 booked at the start of Hunt.
It would be available later that afternoon, so for now we'd be in two different spare rooms
until then. This made the start of Hunt a bit more complicated, just more communication
hurdles when people aren't in the same place.

Once we got to our rooms, we had a fun time trying to connect to the Wi-Fi. MIT had
planned a wireless network change to happen the Friday of Hunt. We were aware of this
beforehand, but MIT Puzzle Club both did not have the cachet to delay the change, and
was also assurred it would be fine.

Well, it wasn't totally fine, because two things happened.

* Anyone who'd connected to campus Wi-Fi before the transfer (i.e. literally everyone on teammate)
could not connect until they'd told their devices to forget and re-remember the Wi-Fi
network.
* Discord calls that worked yesterday didn't work today. We were able to figure out it
was a port issue - Discord uses a random UDP port from 50000-65535 for voice calls
and the entire range was blocked on the guest Wi-Fi.

Separately from that, I had the fun time of needing to enter a code that was sent to my phone
or email to get my laptop connected, when my phone had no cell signal and I had no Wi-Fi signal.
I was lucky enough to suddenly get a tiny bit of signal, and successfully bootstrapped from that to
full Wi-Fi connection.

People on the team started poking around the hunt HQ, and immediately found a bunch of errors
and exceptions as they clicked internal links. This was disappointing, but not too surprising, since our speedruns
and slowrun were all very focused on the solver experience, and didn't exercise our HQ backend
to nearly the same level of scrutiny. Tech immediately starts preparing "hour one" patches, but
they're definitely not getting deployed until after teams have solved for a bit.

Puzzles go live! And no guesses come in for a bit, which made us paranoid that our site had not
released puzzles properly. Then some guesses started coming in, and we breathed a sigh of relief.
T minus...we'll find out, until the coin was found.

I realize I have yet to describe what our Hunt weekend organization looks like. Here are the rough
list of camps:

* **Huntcomm:** People keeping an overall eye on the pulse of Hunt, comparing solve progress against
different forecasts, and in charge of decisions for deciding on interventions to either speed up
or slow down Hunt. Mostly made of people on hunt exec that are spoiled + have context on all the
puzzles.
* **Tech:** Self-explanatory. On-call to answer any questions about the site's functionality, from
either solvers or HQ members.
* **Physical Puzzles:** Handles giving physical puzzles to teams that ask for them.
* **Phone:** Answer call-ins are gone, but we have a phone number! In charge of answering the phone.
Duties were merged with other roles later in Hunt.
* **Dispatcher:** Tracking point of contacts for tech, huntcomm, etc. The person to talk to if you
are free and don't know what you can do to help run Hunt.
* **Runners:** In charge of moving things between rooms. Generally not a dedicated role and only
important at some hours of the day.
* **Logistics:** Similar to dispatcher, but not the same. Roughly, dispatcher knows where everyone
is, and logistics knows what those people are doing.
* **Hints, Contact HQ, Email, etc:** Basically, everyone else. In charge of replying to hints, forwarding
tricky emails to people with the right context, etc.

These are not rigid roles, and very often the work to be done is quite fluid and picked up by
whoever is free.

Now, remember me mentioning we're split between two different rooms? This means that tech is half-split
between rooms, huntcomm is partly split, and so on. Which makes things extra spicy when teammates
start reporting they're having trouble loading the site.

We try confirming whether this is an admin-only issue or a site-wide issue, and it starts to look like
a site wide issue. Then the phone starts ringing. The room goes quiet, and someone picks up.

"Hello?"

"Yes, we know the site is down. We're working on it."

("Do we have an ETA for a fix?" "No, but tell them to avoid spamming the site.")

"For now, please avoid refreshing the site too much, that'll make it worse."

\*click\*

Before Hunt, a logistics member told me a story. They went to MIT Puzzle Club's storage, and found 10 phones.
One of them was the correct phone for our phone number. The other nine were not. It took them a bit to figure
out which phone was the correct one. Good to know that wasn't all in vain.

(Why are there 10 phones in storage for Mystery Hunt? Well back when answers were called-in, phone calls
would go in and get multiplexed across the phones to handle simultaneous answer submissions. I guess teams
have always kept the phones, because why throw them away?)

It does not take long for us to figure out why the site went down despite our load testing. The default
behavior in tph-site is for all tabs from the same browser to connect to the same websocket. This is how
live updates to one tab auto-apply to all other tabs. Ages and ages ago, we changed this default to
do one websocket per tab, so that different tabs could persist different chat histories when talking to
MATE.

The load testing script modeled a user as opening one tab, spamming 10 requests per second for a bit, then closing
the tab. This is not the right model. The correct model is one user opening 100 tabs, then spamming many fewer
requests in just one of the tabs. So we are way over our margin of error on number of active Websocket connections.
After checking CPU usage, we are way under our server's CPU limit, so it looks like we can fix it by just
multiplying all our Websocket handling numbers by a factor of 10. If that causes CPU issues, we'll handle
those later.

The change goes in, we start deploying, and the site comes back up and stays back up. Phew, crisis avoided.

(I just want to briefly point out the counterfactual, where if we hadn't made the websocket optimizations and
email fix we did, we might have had much worse problems.)

I didn't see any projections of Hunt time, but it did seem like people were going through the first round
pretty slowly. Hints were originally planned to be opened in the evening, but huntcomm recommended we opened
them earlier. The way hints worked in our Hunt was that they would open when it was both past the global
hint release time, and a puzzle had been solved by at least N teams. The default setting for N was 25,
but it was always planned to change based on how the Hunt went. It was still early enough in Hunt that
teams had not separated much in number of solves, and releasing with the default N would have opened
hints on everything, which seemed like too much.

In spoilr, a bunch of hunt constants (including the ones for hint release) are saved in the database,
rather than in the code. This allows anyone with admin access to adjust the hint timing without needing
to redeploy the site. As the tech point of contact for the half of HQ I was in, I was asked to weigh in
on the plan of changing N to whatever value would make hints unlock for the first 3 puzzles of the Hunt,
which was N = 70 at the time.

I told them that yes, this would work, but if they did this, huntcomm would be signing up to continually
adjust N throughout the Hunt. Was that fine?

"Yes."

Okay then! But then the logistics person in the room immediately pointed out that we were about to switch
rooms to our official HQ location. Releasing hints at the same time as everyone moving rooms was a bad,
*bad* idea. Huntcomm really wanted to release hints *now*, so we pushed moving HQ back by an hour
and opened hints instead. It was a gamble that no one would actually kick us out right when our room booking
ended, and that was correct.

As hints came in, it exposed a few more bugs in our HQ pages for hint management, which I started looking into.
We also started getting our first emails from teams asking why puzzles were taking forever to load, which
we started responding to with "Thanks for reporting this, we'll look into it. It's puzzling why it's taking
so long to load." And then we got our first official breakout! Hooray! As the Puzzle Factory reveal cascaded,
we started cleaning up to move to the next room.

I was asked if I could carry the soda for Fountain (LINK). "It's in Matt.", they said, as they ran away.

"Sorry, what?"

We had named our temporary storage rooms Matt and Emma, and I did not know where either of them were.
Luckily someone else knew the way and after doing a few trips we'd moved everything to our building 10
HQ. With that, it was back to hint writing.

Well, for most people, it was back to hint writing. A few teams were starting to email us that the
collaborative jigsaw puzzle was stuck for them, with one piece missing, and we weren't able to
reproduce the issue. I suspect there's some screen resizing edgecase that lets people drag the piece
just far enough out of bounds to be unclickable, but we never figured it out, and we didn't have an easy
way to reset the position of a single piece because, well, we never thought we would need it. The
best solution we came up with was to skip the team past the jigsaw, by copying the
jigsaw state from the admin team, and manually adjust the story state to the one marking Factory
discovery.

We would get a new "stuck on jigsaw" email around every 2-3 hours, so I decided to write a
quick playbook for what to do (while everyone who knew the jigsaw backend was still awake).
We also started getting emails reporting some of the known issues that we'd deprioritized in
the slowrun - no volume slider on solve notifications, accessibility issues in navigating the
Puzzle Factory rounds, and so on. I alternated between replying to the emails acknowledging
the problem and asking other web devs how fixable the problems were. We did get a volume slider
added during Hunt, and fixed the wild CSS bug that was causing the Puzzle Factory page to
ignore arrow key inputs, but it turned out fixing the accessibilty bug introduced a new bug
that broke print previews for all the Factory pages. (That bug we didn't learn about until
reading devjoe's writeup a week after Hunt. In general, if people see a site bug in future
Hunts, please email HQ! If we'd known about it during Hunt we could have fixed it in 15 minutes.)

The first event happened and it sounded like solvers had fun. Huntcomm was still concerned about
the pace of hunt, so they decided to give extra event rewards for this event, and allow remote
teams to solve the event too. They asked if there was a way to see how many event rewards a team
had used, to see if teams were using them or hoarding them, and I realized this wasn't well
exposed on any of the HQ pages. I made a mental note to address this sometime.

After I got dinner, Alex Gotsis came by, asked me how much sleep I'd gotten ("...1 hour?"),
and basically relieved me of duty and told me to leave. I got back to my hotel room, but
decided to set up Discord alerts whenever an event reward was used or puzzle feedback was
sent.

![Me failing to sleep](/public/mh-2023/fridaysleep.png)
{: .centered }


Saturday of Hunt
---------------------------------------------------------------------------------

Sleeping so early meant I woke up much earlier in the morning than usual, and I arrived at
a mostly empty HQ. I caught up on overnight progress, then got back to answering hints.

In the Palindrome AMA, they'd mentioned the initially unlocked puzzles made up around 30% of
all hint requests, and that seemed accurate to us as well. When running a Hunt, it is very
common to get new teams that essentially want to be handholded through the entire solve. Of
course, we will answer the hints they send in, we're just trying to make sure those teams have
fun, but it can be a bit tiring when a team sends a hint every time they solve a new cryptic in
Inscryption or ID a new rule in Museum Rules. Luckily, there is a natural "get better with experience"
phenomenon where handling hints on those puzzles gets a lot faster over time.

Things were progressing, albeit more slowly than anyone had expected. Someone started drinking
some of the soda set aside for Fountain. The physical puzzle distributor stopped them, and
announced "Don't drink this soda, it's a puzzle", which is really the kind of sentence
you can only hear and believe at Mystery Hunt.

Running Hunt was simultaneously getting more boring and more exciting. Fewer things about the site
were on fire, which was good. More teams were solving the fun submission puzzles. Our first tirades
had come in Friday evening, but now a bunch of other teams were solving the tirades puzzle and
sending in even wilder speeches.

We did another round of team check-ins. Our first round was on Friday and went...okay. We sent out
three groups of people, one with Mystery Hunt veterans and two with mostly newer people. The
veteran group came back much later than the other ones. That group was much better at asking
questions that got teams to give real feedback, rather than generically saying hunt is fun or not
fun. The dispatcher (I think it was Edgar Chen?) had that group write up a guide for to run check-ins,
get more specific feedback, and support teams that were newer or feeling stuck. The 2nd round of
check-ins was told to read that guide before going, and I heard that round went a lot better. I
didn't get to go on check-ins because I was the only person on tech in the room at the time
and needed to be on-call.

Speaking of tech, we had designed this entire interactions flow for team check-ins. We'd bulk
unlock a check-in interaction for all teams, and have people mark a check-ins as done whenever they
visited a team. This reused the hint answering flow that was very battle tested by this point.
Unfortuantely, none of that check-in infrastructure got used. The main things missing from the
dispatcher point of view:

* Interactions were ordered by team name, but the important feature was ordering by location.
Check-in squads were directed towards all teams in a certain part of campus, and there wasn't
a great way to sort by that.
* Some teams had special instructions (call this number first, knock first, please mask up and
show us proof of a negative rapid test before coming into our room, etc.) and that wasn't
easy to retrieve from the interaction..
* Bulk opening a check-in for everyone didn't do a good job at ignoring teams that had
self-reported they had stopped solving Hunt.

So, a miss on tooling there. Check-ins were managed over Google Sheets instead.

There was an increasing drip feed of event rewards at this time, since teams were still behind
where huntcomm wanted them to be. Now that event reward alerting was up, it was clear that a lot
of teams were stockingpiling their rewards. Huntcomm tried a lot of alternate wordings on the
event page to nudge teams to use event rewards more aggressively, and none of them seemed to do
much. Eventually the event page was updated to say the Museum and Puzzle Factory were the first
8 rounds of Hunt (at a time where no team had made it past Reactivation), and that seemed to
be the right words to get event rewards used more often.

The event reward flood was also exposing some bugs in our event handling code. In testing,
I had never tried redeeming multiple event rewards at once. Puzzle unlock lazily in
tph-site, they unlock on first page load after you refresh. So if a team spammed free unlocks
without refreshing the page, they could be debited for many more rewards than they could
use. I changed my code to force unlocks on event redemption, gave back event rewards to teams that
reported the bug, and so on.

Remember me saying running Hunt was getting more boring? Yeah, that was stopping pretty quick.
Huntcomm continued to be concerned with the pace of Hunt, and had already shipped a bunch of
puzzle unlock changes to open rounds sooner, release metas earlier, have more puzzles open within
a round, and so on. They were now deciding that Hunt speedup needed to be even more drastic.
Anyone with an AI round puzzle was asked to brainstorm ways to nerf their puzzle before teams unlocked
it. If we didn't come up with a nerf, then huntcomm was going to do it for them.

All of these nerfs were going in with zero testsolving, which restricted them almost exclusively
to either flavortext nerfs, or complete removal of intermediate steps, rather than revisions to
existing clues or designs. Quiz Bowl Packet was marked unnerfable (would have required rewriting
clues). 5D Barred Diagramless was similarly hard to nerf, because understanding the game is hard
even when linked to a good tutorial.
But, some of the other puzzles I was
on had easier nerfs. Period of Wyrm originally had only the professions of Astrologer, Laborer,
etc. and we added the names. Walking Tour used to have an ordering step and we removed that
completely.

By now the hint threshold had been lowered to N = 15, and huntcomm was hesitant to lower it any
further. Partly it seemed potentially unfair to bring N closer to teams that could be in contention
to win the coin by Sunday, and partly it was unclear if we had the manpower to widen the hintstream
more.

(If every team was limited to one open hint at a time, shouldn't the number of total open
hints not increase by much? Well, yes, that's true. But it will increase by a bit, since not
all teams continually keep one hint open. More importantly, lowering N unlocks hints for
puzzles that did not have hints open before. Newer puzzles take longer to hint than older ones.)

I would say the biggest factor is that hints tend to be more fun for solvers, but they don't make
Hunt end as fast as free answers do.
One of the hypotheticals I've thought about after Hunt is what would have happened if we took N all the way
down to 0 to open hints to leading teams, and I believe the consequence would have been an
immediate bottleneck of the hint system as every top team sent in their partial work on the hardest
puzzles in Hunt. We'd spend 10-20 minutes figuring out a good hint to give, then would immediately get a
new hint the team wrote while waiting for their old hint. Essentially, we needed ways to push Hunt
forward that didn't require us to be in-the-loop after intervention, and hints were not one of them.

Saturday evening, huntcomm announced that we were going to change the Reactivation requirement
to 3/4 input metas instead of 4/4. (The incoming meta requirements were the Office meta, Basement meta,
Factory Floor meta, and Museum metameta.) We would also change the scavenger hunt to reward
free answers that were usable in AI rounds. Then huntcomm came over to tech and asked "okay, how do
we implement this?"

Changing unlocks around the midpoint interactions for Reactivation was especially scary...this
was probably the most complex part of our unlock structure, was the source of many bugs we fixed
during the slowrun, and now we needed to change it. Once we sat down to actually check
the details though, making it unlock on 3/4 was actually not too bad, since the majority of
the complexity was on what happened after Reactivation, not before.

AI round free answers on the other hand were a bit more complicated. Remember way back earlier,
when we said AI round free answers wouldn't be supported, because we wanted teams to break into
the answer gimmicks on their own? Yeah, well, now that was coming back to bite us, since we had
to start thinking through any important edge cases that could go wrong.

I found the event code I wrote a few months ago, and jury-rigged AI round free answers by
doing a ton of copy-pasting of the old logic. I did not want to touch any of the existing
logic if I could. Although we did not plan to release AI round free answers outside of the
scavenger hunt yet, I decided to implement the functionality for it just in case that changed
later.
The implementation got in around 30 minutes before the first team solved
Reactivation and unlocked the scavenger hunt.

Speaking of Reactivation, people were rightfully very excited to go do the first Reactivation
interaction. The way Reactivation worked was that when completed, teams would unlock a midpoint
conversation with MATE. In the middle of the conversation with MATE, teammate would shut down the
Puzzle Factory, and a few minutes later teammate would barge into that team's on-campus room
to tell them Mystery Hunt was cancelled.

*Logistically*, teams did not know we were going to do this, so teammate needed to know when
a team was close to solving Reactivation. The transit time from our HQ to other team's HQ could
be up to 15-20 minutes at the extremes, and although we wanted teams to have nothing but
a shutdown Puzzle Factory for a few minutes, we didn't want them to *actually* be stuck.
Our solution was to send a Discord alert when 3/4 Reactivation minipuzzles were solved (time
to prepare) and 4/4 Reactivation minipuzzles (time to barge into HQ). Kind of like a
"tornado watch" and "tornado warning" system.

![Taco warning](/public/mh-2023/tacowarning.jpeg)
{: .centered }

At 3/4, teammate members would leave HQ and nonchalantly hangout near the team HQ, trying
to not be too obvious. At 4/4, they would converge. Sometimes the gap would be very short,
like Cardinality going from 3/4 to shutdown in 5 minutes.

![5 minutes](/public/mh-2023/5minuteshutdown.png)
{: .centered }

Sometimes, it was not, like when TTBNL solved 3/4, half of hunt HQ immediately left, and they
didn't solve the 4th minipuzzle for another 50 minutes. Meanwhile the hints were still coming
in, except with half the staff available to reply. This made the logistics team very upset. They tried to get people
who weren't directly acting to stay behind in HQ and join a later run (there'd be several
throughout the weekend), but this seemed like a hopeless request.

It was a bit hectic, but once MIT campus closed for the evening, I decided it would be better for me to
sleep early so that I could stay awake through Sunday. Surely Hunt will end by then.

(Here, "early" means 4 AM, I stayed up to make sure physical puzzle pickup didn't need tech
help and to set up alerting on the new AI round free answers.)


Sunday of Hunt
-----------------------------------------------------------------------------------------

I did not get much sleep.

I woke up at 6 AM, planning to go back to sleep after checking the team Discord to see if
there were any issues. There were, with the AI free answer redemption.
As implemented, they were *only* usable in the AI rounds. In the 2 hours I'd been sleeping
huntcomm had decided they would send strong event rewards to everyone, but wanted those
rewards to be usable by all teams, including ones that had not unlocked AI rounds yet.

"How urgently do you need this, like should I get out of bed right now?"

"Yes please."

So I got up and spent the next hour making that happen.

Internally, the way I set up event rewards was the same way gph-site sets up hints. In gph-site,
instead of directly storing the number of hints a team has left, the database stores the total
number of hints a team has ever received. On every page load, the backend
computes how many hints the team has used so far, then displayes the difference to
the frontend. The advantage of this design is that you never need to worry about database
transactions or keeping counts in sync across models. A hint is used if and only if the team
has gotten back a useful hint response.

A disadvantage of this design is that the more ways you can use hints, the more complicated
it is to compute the number used so far. So similarly, the more overlap there is between
how you can use event rewards, the more complicated the accounting logic gets.

The new logic would iterate through any event reward usage the team had, compare it to their
total regular + strong rewards, and greedily use the "weakest" reward required. This did
require describing the logic really awkwardly on the event redemption page, but the idea was
to make sure a team could never hit a gotcha of using a strong reward to solve a puzzle
when a weak reward was available.

Tested it, got it reviewed and deployed to production, and I said I'd stay up until I saw the first alert of
a team using a strong reward. That happened about 10 minutes later and I told people I was
going back to bed.

I woke up a few hours later, decided that was as much as I'd reasonably get, and walked to
hunt HQ. You might think the mood was more somber, now that we knew Hunt was wildly
behind schedule, but it really wasn't. Maybe it was more panicky in huntcomm, but I thought
the rest of HQ was holding up well and still running smoothly in an emergency.

A group of people were going to [Tosci's](https://www.tosci.com/) to do a double-check for
Eat Desserts on Main before lead teams unlocked Conjuri. They said they'd do a group order for ice cream for HQ, which I immediately
jumped on. I don't remember what flavor I got but it was good. Also, now that we knew
around how many teams would unlock Fountain, we had way too much soda, so a portion was
set aside and declared "no longer a puzzle". A teammate on teammate complained that
they would only drink the soda if it were a puzzle, so we relabeled the soda as "still a puzzle
but okay to drink". They tasted the soda and described it as "perplexing".

There was a continuing dripfeed of site issues, but they were fairly minor. There was
an issue with Wyrmhole round links caused by colliding [React keys](https://react.dev/learn/rendering-lists#keeping-list-items-in-order-with-key).
The keys were based on puzzle name - when would the list of all puzzles ever need to
display the same puzzle multiple times? The realization that Wyrm was still causing
problems in our site code was incredibly funny, especially given how well it
echoed the in-Hunt story.

We were also starting to get stories and screenshots of teams first reaction to the complete
nonsense of Bootes round answers, which was pretty great.

IMAGE HERE?

The dripfeed of free answers continued, along with nerfs delivered in the form of errata.
I've seen a bit of speculation about why we used errata to deliver nerf. The deployment process
for Hunt was based on Docker and includes a step where Next.js needs to build
the application. Blah blah blah, tech tech tech, the short version is that finishing a deploy
takes around 15 minutes. Changing the puzzle content requires changing the site code, which
requires a deploy, which takes 15 minutes. Adding errata doesn't require changing the site code,
so it can be done in 0 minutes. It just looks a bit weird.
I remember not liking the nerf given for The Scheme, it felt like it was just removing the
puzzle entirely, but in retrospect it was important to push off the rabbit hole of reusing
the layout from The Legend.

Sunday continued on and on, with no team finishing Hunt. We did see teams spending one free
answer on the scavenger hunt to get two free answers in return, which was simultaneously
disappointing and delightful. I remember saying "We've created an economy", like we'd
made a free money hack in the universe that was actually free money. I mean, it was
not actually free money, but some teams just don't want to touch grass. What can you do?

The more pressing issue was some of the reports that Conjuri was lagging.
In load testing, we assumed not too many teams would be doing Conjuri at once, and that
our sims were a big overestimate on server load. Now, we both had many more open websockets
than expected due to the one-per-tab design, and a lot more teams were on Conjuri at once
since we'd both lowered the round unlock threshold and given a lot of free answers. It
was now causing real problems.

When we checked the server load, the CPU usage was still fine, so it seemed like it was
tied entirely to our websocket management, and wasn't fixable by just migrating to a bigger
server. There was a very brief tech meeting where I explained as much, and said
there wouldn't be an easy way to fix it unless we heavily decreased our websocket usage
or spun up another server.

I was not expecting us to do anything about it, so an hour later I was surprised to
see that we were in the middle of migrating hunt progress to a fresh server. A lot of
tph-site assumes it runs on a single server, so we would not be trying to run two machines
for everyone. Instead, we'd do it manually. Some teams would be given the URL to the new
server, and instructed to only use it for playing Conjuri, while the original server would
be the "official" marker of Hunt progress.
This went up, and I sent out an FYI that some Discord alerts would be duplicated, and
people should not worry about it.

As we approached the original Hunt end deadline, we got a complete flood of hint requests,
and immedaitely drafted an email telling people Hunt was still going to slow down the flood,
while hint repliers handled the backlog. At the Sunday MIT campus closure window, huntcomm was
now quite confident they could get a team past the finish line
by Monday morning. A decent number of teams had all AI round metas unlocked, with enough
feeders to solve, and they had hints of increasing severity that could be deployed over time.

I got back to my hotel, saw that some teams were on the clickaround (MATE's Team), and decided
to stay up until they finished. Meanwhile, team exec is preparing a slide deck for wrap-up,
and is asking if I have any GIFs of the full loop of the Wyrmhole round. I don't, but I tell them
to give me some time and I can get back to them with some options. I tried a few different options,
but none were as good as a screen recording of me clicking the zoom button a few times.

With that done, I decided to stay up until the first team finishes MATE's Team. Then I decide,
screw that, I am going to bed, there is still the runaround and that won't happen until MIT
campus reopens.


Monday of Hunt
---------------------------------------------------------------------------------------------

The coin is officially scheduled to be found. I get into HQ and keep answering more hint requests,
emails, and so on. It is also time to start looking into solution release, doing a last minute
check of site behavior when we cross the HQ shutdown point, and so on.

In betaveros's post Hunt post, he mentions that if you have to choose between a Hunt that goes
short and a Hunt that goes long, it seems like a lot of people prefer the former. That's definitely
true for me. One of the consequences of a long Hunt that is not as obvious is that you just have
less time to do the work between HQ close and wrap-up. In our case we had basically no
time at all, and a lot of time that would have gone into making solutions and stats releasable by wrap-up was
instead spent running Hunt.

We do a bunch of runarounds, grouping together teams towards the end as we try to fit > 4 teams
into 4 time slots. Then it's time to close up for real. I start reading some of the discussion but
there's really still so much cleanup to do.

Wrap-up happens, and if teammate sounded a bit out of it, well, that is the default state of Mystery
Hunt wrap-up. It does not go as badly as I thought it would. People are mad online but civil, friendly,
or reassuring in real life. I catch up with puzzlehunting friends that I haven't gotten to see all
week because I've been busy keeping the lights on, and am happy to hear that multiple teams had fun with
Interpretive Art. People slowly file out of the room and soon it is just teammate.

I go to get lunch with some other hunters, then collapse and take a nap so that I can be awake in time
for a team dinner at Spring Shabu-Shabu. This is mostly uneventful. The hunt retrospective is officially
postponed to a later date so people can catch up on sleep, but I head back to the AirBnB instead of my
hotel because I am pretty awake after my nap.

![Passing out](/public/mh-2023/passingout.png)
{: .centered }

Over a few hours we finish up the lingering tech work to get solutions out. Most of it is done, the
main change is updating stats to account for free answers and making sure those pages are accessible
even if a team has shutdown Mystery Hunt. The Hunt server itself is left up, with static conversion for
another day.

The official postmortem is not happening until later, but some parts of team leadership are doing an
unofficial one anyways, going through various puzzles with the air of "why did we write
so many teammate-friendly metas when not all teams are teammate, why was this puzzle made so long,
how could we miss this and this and this f\*\*\* f\*\*\* F\*\*\*". Some of this self-criticism is
fair and some is not.

(A few days later, we'd get some advice from previous Mystery Hunt
runners where they said they were sure we'd done a panicked reflection on every mistake we'd made, and how
that wasn't a good thing to do so close to Hunt.)

The world turns and life moves on. I fly out on Tuesday, after spending the morning doing a
[Boxaroo!](https://boxaroo.me/) escape room and playing board games.


February 2023 - Did You Think It Was Over?
-------------------------------------------------------------------------------------------------

The model you might have is that after MLK weekend, you can stop thinking about Hunt. This is sort of true?
It depends. If you took on a lot of work before Hunt it's more likely you'll have work to do after Hunt.

In our case, we have a big problem of figuring out first how to reveal all puzzles when our site makes so
many assumptions on team state on, like, literally everything. Then, on figuring out how to generate
the customary hunt stats page.

I start looking at feedback, but seeing feedback like "you did not mean for anyone to have fun with
these puzzles" when you have just worked a 100 hour week trying to make sure people had fun with the
puzzles is...pretty disheartening. I force myself to stop, telling myself that I signed up to make the
hunt stats page, and if I read any more feedback now I'll get depressed, and stats will be delayed by a
week, and nobody wants that. I'll look later.

First things first: figuring out what the *hell* we can measure in our Hunt. In a typical Hunt, solve times
and solve counts are pretty free, but in our case we have nonsense like Hall of Innovation. I don't find
a clear place where this information is saved in the database, it looks like our stats page are only
reporting the last time the team has solved the puzzle due to us literally deleting old solves on gizmo
save. I eventually figure out that I can get the information by writing a Discord script to crawl our
entire #guess-alerts channel with some regular expressions on top.

I also need to figure out how to merge solve data from the server spun up for Conjuri back into the main
server. Brian sends me some pointers to the database dump, and I figure out that all our existing documentation
only applies to setting up a Postgres server from scratch. I end up loading both dumps locally and do some
surgery to get solve data merged apporpriately.

That just leaves everything else. Most of the stats are not too bad to derive in Django shell after I
understand the data format. Data on when teams solved the collaborative jigsaw is partly incomplete for teams that
had to get fastforwarded past the puzzle, since they never got credited with an official solve. I spend too
long trying to reconstruct the full data from email logs before deciding it's not worth it. Would anyone
even notice the inconsistencies? Probably not.

The spoilr HQ page has some built-in solve charts based on chart.js, which solves many problems while
introducing other ones. First I get confused that the documentation doesn't match the code. Then I figure out
that the chart.js we use is out of date, and migrate past a breaking API change to get some features
I want in the codebase. *Then* I learn that actually [there is an undiagnosed performance bug](https://github.com/chartjs/Chart.js/issues/10073)
in the newest version that causes the charts to render 2x slower, and I sigh as I undo all my migration work.
I care about speed more than making the charts display nicely on mobile.

As I generate the team size graph, I find it very cute that you can almost see the vertical lines where
teams round their team size to the closest 5 or 10. It reminds me of an old Jon Bois video showing that
football plays are biased towards starting from multiples of 10 due to refs having some leeway on where
to place the ball. The lines of scrimmage are coming together nicely, and then Death & Mayhem is out
here having 133 people and breaking the illusion.

![Solves by team size](/public/mh-2023/solvechart.png)
{: .centered }

Over the rest of February, we figure out what our hunt handoff looks like, and the list of tech items to work
on to convert all the interactive puzzles to static ones. Then I stop paying as much attention.


March 2023
-------------------------------------------------------------------------------------------------

The site is still not static. It turns out it is a lot harder to work on Mystery Hunt 2023 when the world
has moved on from talking about Mystery Hunt 2023. It was a thing that made some people happy and some
people mad, but now it is over. Except it isn't.

I suppose that's the deal. Write a tech-heavy hunt that strongly assumes a server exists, and it looks cool
live, but it's annoying to wrap up later. Given how long it took to package up Teammate Hunt 2020 and
Teammate Hunt 2021, I am entirely unsurprised Mystery Hunt is taking a while to wrap up. But we do want it
to wrap up.


Thoughts on Hunt
-----------------------------------------------------------------------------------------------

Obviously, some things went wrong.

I think it'd be a shame if the only story of this Hunt in 10 years is "it was really hard". As someone who
spent a lot of time on a bunch of non-puzzle aspects of Hunt, a lot of the stuff I'm most proud of is
in the overarching structure of Hunt, and supporting its visual presentation. I never want to deal with
responsive SVGs ever again, but my God, we figured out how to make the Wyrm round concept work.

IMAGE

That being said, I did write a lot of puzzles too. I wrote more puzzles this year than I have in
the past 10. (That isn't saying much when there was a 7 year hiatus in between, but still.) If anything,
it makes the criticism easier to go through. I've seen people simultanously put my puzzles as their
favorite and least favorite puzzles of Hunt, and that makes it easy to tell it's not an attack on, like,
my character or self-worth as a person.

I am about to go into a big list of interventions that I think could have addressed the Hunt difficulty,
but before doing so, I want it clear that I'm still proud of 80%-90% of the work I did for Hunt. It was a lot,
and took over much of my life towards the end of the year, and I'm not sure I ever need or want to go
through the experience again, but I did it and a lot of it was cool.

At the same time, I think it is important and interesting to consider where things started going wrong,
because I think some of the errors were pretty subtle. It is not an easy explanation, like "teammate
was inexperienced". teammate is a younger team, but collectively I believe members of teammate have run
around 5-7 different puzzlehunts before this one. It's not quite "teammate did not account for being biased
towards their own puzzles". This bias was a big topic in puzzle variety discussions, and is the reason
the hunt is only half-littered with math and video game puzzles instead of completely-littered. There
were other things going on.


Calibration
---------------------------------------------------------------------------------------------------

If you forced me to pick one thing that went wrong, it would easily be calibration. Puzzles were harder
than their ratings across the board.

But, it's interesting that they were *consistently* harder. The usual symptom of a Hunt that is undertestsolved
is that some puzzles are much easier than expected and some are much harder than expected. When you don't
have enough testsolve data, errors tend to go both ways. Here the errors went one way.

In Puzzup, puzzles are rated 1-6 in difficulty, and those ratings were actually pretty consistent with
each other, they just weren't consistent with the stated rubric. A 3 on the scale is marked as "comparable
to a puzzle in The Ministry". Looking at some puzzles that averaged a 3.0, they were certainly easier than
a 4.0 puzzle, but they were certainly *not* Ministry difficulty. Like, even the inside baseball
solve times of teammate testsolving teammate's puzzles seems too long.

My sense is that when puzzle guidelines first came out, there was overextrapolation on what a full-size
Mystery Hunt team could do, relative to the 3-4 person testsolve groups that were assembled per puzzle.
So the recommended solve times for puzzles were started too high, puzzles got written to that bar, and then
the bar stuck.
I agree with [Eric Berlin's](https://www.ericberlin.com/2023/01/20/how-hard-is-your-hunt/) comments
on Hunt difficulty, but I thought it was interesting that he casually mentioned testsolve teams of
4-8 people. We would have killed to have a lower bound that started at four! Our lower bound was often
2-3 people, and getting 5-6 people was logistically challenging enough that it only happened for puzzles
that really needed it.

If we had ever revisited a Ministry puzzle around the middle of the year, when feeders were getting
written in earnest, it would have been very obvious. But why would you solve a puzzle from an old Mystery Hunt
when there are a bunch of puzzles to testsolve for the upcoming Mystery Hunt? The clock is ticking,
after all.

Maybe that is the answer - go back to old Hunts and *actually* solve 1-2 of those puzzles in the same
conditions as your other testsolves, and assume this will be worth your time? If you ever actually do this,
let me know, because I have never seen it done or heard of it being done. Usually you assume your
calibration is correct.


Calibration Part 2
----------------------------------------------------------------------------------------------------

One of the consequences of starting with a higher difficulty bar is that it's harder to design and
testsolve puzzles.

I think some people might have a mental model where making a puzzle is like placing one brick at a time,
and you grow from a good 5 minute puzzle to a good 10 minute puzzle and so on, until you hit your time target.
This is really not how puzzle writing works.
As an art form (yes I am saying puzzles are art, no I
am not taking questions), puzzles are partly defined by how well they fit together as a cohesive whole.
You can have a multi-step puzzle with very disconnected steps, and it can even be fun, but it's
not what people imagine when they think of a good puzzle.

Usually what happens is that you start with some idea, and you try to expand it, as if you're inflating
a balloon. Holes will appear and you'll tape over them, testsolvers will say it isn't fun and you switch
out the material, but eventually it comes together. And at the end, you've got a puzzle.

The challenge with writing longer, more difficult puzzles is that you need to start with a bigger idea
at the beginning. It has to be big enough that once it's filled to a complete whole, the completion will
be a long enough puzzle. Then each round of iteration takes longer, since it takes longer to do the grunt
work and testsolve. If you assume a constant number of puzzlemaking hours per Mystery Hunt year, and
every puzzle starts harder than it should, the self-consistent conclusion is that every puzzle also
gets fewer iterations of refinement than it should. You can't push one up without pulling the other
down.

A lot of the puzzles could have done with more editing, and many of them *were*, but the later in the
year it got, the less prominent editing became. Much of the drumbeat during the year was "we are
behind our puzzle projections", so editing became more perfunctory. If a puzzle got rated as fun,
it went in, even if it may have been too long. This is 100% the correct decision, every time. Part of
[following the fun](https://www.reddit.com/r/gamedesign/comments/hfqh8r/what_does_follow_the_fun_mean_how_do_you_follow/)
is to not throw out things that testsolvers have told you are fun. The problems arose when
a puzzle was rated as sort-of fun, where editing it would take a lot of time and require another
testsolve when testsolve capacity was low.
Puzzles that should have been edited got rounded to "let's
ship it and move on" due to time pressure. The provocative way to put it is that we knew we had some clunkers,
but we understood exactly how they were clunkers and needed warm bodies (puzzles).

(And then feedback comes in pointing out the same flaws that came up in testsolving, and you feel bad
because you knew it existed, even if it made sense to ignore it at the time.
This seems to be a common theme in not just puzzlehunts, but all creative endeavors:
authors of a work know most of the problems in their work. I remember Palindrome saying that a lot of their posthunt
feedback was framed as "I think you overlooked this problem" when they usually had not overlooked the problem.
Please don't let this stop you from pointing out flaws though. It's useful to see what
problems people consider important, even if they're already known.)

The Conjuri feeders were the last ones written in Hunt, and when the feeders were released, they
were specifically asked to be puzzles that would be easy to edit and iterate (read: closer to
well-known puzzle archetypes). The shift to encouraging such puzzles should probably have happened much
earlier.



Testsolve Affinity
--------------------------------------------------------------------------------------------------------


For a lot of the year, authors were responsible for soliticiting testsolvers for their puzzles, usually at
general meetings. This started taking up a lot of people's time, so we later had a "testsolving lead". Everyone
filled out a survey with their puzzle preferences and availability. Authors would describe their hour estimate,
what kind of puzzle it was, and how many testsolvers they wanted, and the testsolving lead would handle scheduling
testsolves and wrangling testsolvers.

This was pretty helpful, but it may have done too good a job of pairing testsolvers to puzzles. Math puzzles
were routed to math people, geography puzzles were routed to geography fans, and so on. I'm not saying every
puzzles was directly routed to people who'd have an easy time with the puzzle, but it was certainly biased that way.

There are other biases of the testsolving process. You are encouraged to stick with a puzzle, so that other people
don't need to be spoiled. You usually are not solving with as much urgency as a live hunt, but you are also not
as tired as a live hunt. You start with a group of people, rather than having one person start a puzzle and slowly pull
in people over time. When testsolving Moral of the Story, I thought it seemed okay, but we had four people doing
the first step and were able to speed through it pretty quickly. If you start the puzzle solo, you're going to have
a bad time grinding through the start.

Still, I think the main issue was large puzzles that reduced the number of testsolves doable to make
every puzzle shipped in time.
Every puzzle has inherent uncertainty to it, and you can think of a testsolve as reducting th
- testsolves never
exactly match real solvers, and this is especially true in Mystery Hunt where team sizes and makeups have
such a wide range of variance. A team has to choose when it's worth investigating an uncertainty, and when
you should leave it alone and assume it's not worth investing the time to make it more certain.


Complexity and Scope Creep
------------------------------------------------------------------------------------------------

teammate wrote some **wild** puzzles this year. Some wack metas, some crazy feeders, all sorts of stuff.
Let me go requote the team goals from the start of the year.

> 1. Unique and memorable puzzles
> 2. Innovation in hunt structure
> 3. High production value
> 4. Build a great experience for small / less intense teams

It's possible we overreached here. There's a certain amount of complexity budget you get, defined by your
available manhours, and we spent a lot of it early when deciding to create the AI rounds. Then we spent more
of it creating some more complicated metameta structures, and spent more of it connecting rounds
together between the Museum and AI rounds. I have already mentioned how in retrospect, I think adding a dependency
chain between Museum and the Wyrmhole was a bad idea, but I think there were some second order effects
on late feeder release and relative lack of "normal" puzzle answers at the start of puzzle writing.
The gimmicks also made it hard to shuffle puzzles between acts if they tested harder than expected.
In previous Teammate Hunts, I've been in testsolves for puzzles that got moved from intro round to main
round and vice versa. I'm not sure if that happened this year. You can't easily swap a Museum puzzle with
a Bootes or Eye puzzle, their structural requirements are different.

I'd say that in general, teammate does rise to the occasion, but not without a lot of effort and struggle,
and usually with some scope creep along the way. Teammate Hunt 2020 started design with one Playmate game
as the runaround, and ended with eight. Teammate Hunt 2021's design proposal for the Carnival Conundrum
said "7-9 intro puzzles, 7-9 main round puzzles per half, 6-8 pairs of puzzles", and the final hunt picked
9, 9, and 8 respectively, the top of each band. I was not in the design process for Hall of Innovation, but
I remember someone telling me that when that was first designed, it was certainly not an entire round.

My guess is that you can make a hunt with complex hunt structure, and you can make a hunt with complex
puzzles, but you should give some pause before trying to do both. It's a bit tricky, because one of the
common themes in survey feedback on what makes Mystery Hunt special is its grandiosity and scale. This is a
very, very consistent sentiment. I would rather people try to push Mystery Hunt than feel required to play it
safe.


Time Unlocks
-----------------------------------------------------------------------------------------------------

This year, rounds were time unlocked as a whole, up until Reactivation, and no time unlocks were done past that
point.
I've seen some speculation that we did not time unlock past Reactivation because doing so would have spoiled
the answers to Reactivation. Personally, I don't think that mattered. If a team wants to see more rounds
they can probably live with skipping one puzzle of Hunt.

Requoting myself, the way I view teammate's approach to story is:

> * Decide on the story you want to tell, then make sure as much of the Hunt as possible acts
> consistently with that story. This will take a long time to polish - do so anyways.
> * Tie changes in narrative or story state to actions the solvers take. These actions are usually
> solving puzzles, but they don't have to be.
> * Use bottlenecks to direct team attention towards the same point, then put the most
> important story revelations at those bottlenecks.

I'd say the main
reason we did not time unlock past Reactivation was because the story was integrated so strongly into the
Hunt that we didn't have a good answer for how to time unlock past it. The entire conception
of the story beat is that is is a breaking, irrevocable change in how solvers use the site, and the solvers'
story objectives. You can't really have a team suddenly cause Mystery Hunt to be cancelled because it
happens to be Sunday. This was similar to why we decided not to time unlock teams past the intro round
in Teammate Hunt 2021.

To a smaller degree, we also had zero idea how the site would function if time unlocked past Reactivation.
All across the codebase, there are differnt checks for solve progress and story progress. The two tracks
are separate yet interconnected (think of them as "entangled" if you'd like). We had pulled a literal
all-nighter making sure it worked properly for a team with no time unlocks and did not have time to
verify any behavior of what happens if a team has a post-Reactivation solve state without the corresponding
puzzle solves beforehand. Even the relaxation of 4/4 metas to 3/4 metas to unlock Reactivation caused some
problems around Hall of Innovation that we had to fix live.

I think the devil's advocate argument is that not all teams care about story, but teammate is a team that
*does*. We care about story enough to allow it to partially intrude on UX of the site. This isn't just about
the shutdown of the website, but also the Museum puzzle list not listing Puzzle Factory puzzles, because
from the Museum's perspective the Factory does not exist.

There was probably a way to make time unlocks work, if we made them self-service, gated them on
scheduling a team interaction with teammate where we could catch up on story beats teams were skipping, and did the tech
work required to verify site behavior and allow for skipping certain cutscenes / bottlenecks.
These are all solvable problems, that are not solvable in the middle of a Hunt that is going too long. I'm sorry
that wasn't set up beforehand, there were just too many other things to work on.


Difficulty Buckets
-------------------------------------------------------------------------------------------------------

One of the common points of criticism is that the intro round was not really an intro round. Some of the puzzles
were quite involved and difficult.

I think it is more accurate to say that the Atrium was not designed as an intro round, but it was perceived
that way. During writing, there were guidance for three difficulty bands: Museum puzzles, Factory puzzles,
and AI round puzzles. There was then no further guidance within that, and puzzles were to be ordered afterwards
by hunt exec. This has never been a problem in past Teammate Hunts, where the intro round was 8-9 puzzles.

There were 40 puzzles among the Museum rounds. If you don't try to funnel easy puzzles to one round, they'll
get distributed roughly evenly. This left a trilemma where not all three could be true at once:

* Puzzles are mostly ordered by difficulty.
* A meta for a round is unlocked only after a solid fraction of its feeders are solved.
* A meta is unlocked early in Hunt.

The puzzle order picked for Hunt chose the last two. If teammate had known how hard Hunt was going to be,
I think it's likely we would have picked the first and last instead. A number of our metas were not very
backsolvable or doable from partial information, and I think it would have been safe to unlock those earlier.
It's still not an ideal solution if a meta unlocks way before you can solve it but it's still psychologically
more exciting.

Now, the real fix is to make sure you are not in this trilemma to begin with.
One of the big conclusions of the internal retrospective was that
round ordering should have been locked in much earlier so that people could know if their feeder was towards the start or
end of Hunt. The curve would have been smoother if Atrium was easier for Museum puzzles and Wyrmhole
was easier for AI round puzzles.

(Just as one final point: although I am framing puzzle ordering as if you should order puzzles from easiest to
hardest, this is just one of many criteria people can use for puzzle ordering. Variety of currently unlocked puzzles,
avoiding too many grindy puzzles at once, avoiding too few grindy puzzles in case solvers want a menial
straightforward task, difficulty walls, and so on are also all important points. These criteria often compete
with each other, making optimizing all of them hard. I have literally been in 8 hour long meetings for puzzle ordering
and unlock tuning, and that was for Teammate Hunt 2021, which was 1/4th the size.)


Small Team Objectives
------------------------------------------------------------------------------------------------------

One of the experimental decisions we made all way back at theme proposal time was to have the discovery of the Puzzle
Factory be the equivalent of solving the 1st meta as a small team objective.

How well this worked is probably directly related to how cool you found the Puzzle Factory and what expectations
you had around Hunt. If you went into Hunt with the goal of solving 1 meta, you may not have been hyped to
unlock a new round with new puzzles instead of more Atrium puzzles.

We did a lot to funnel teams towards unlocking the Puzzle Factory, including gating the release of the Atrium
meta behind finding the Puzzle Factory. I believe it is literally impossible to get enough Atrium solves to
get the meta without solving the loading puzzle, since we wanted teams to find the Puzzle Factory before focusing
on a meta. The side effect was that the Atrium meta unlocked quite late.

Story-wise, teammate is trying not to draw attention to the Puzzle Factory, and teams were supposed to feel like
they were breaking the rules by exploring the Puzzle Factory. I think that created tension with how to make teams
feel like they had done something important.
There was a planned story interaction with MATE,
where MATE would tell off solvers for entering the Factory, but let them explore anyways. I believe it got
deprioritized, but it probably would have helped on this front.

I suspect future teams will not have this problem, because they will just make solving the 1st meta the objective
for new teams.



Advice for Future Writers
-------------------------------------------------------------------------------------------------

I mean...

In some sense, this entire post is advice for future writers. In some sense, I got too lazy to distill it down
into actionable advice, and decided to just put my thoughts on everything and let people draw their own conclusions.
I'll add some more things that I don't think were covered earlier though.

* Pay attention to the processes you're creating. Hunt is big enough that there are pretty significant benefits
to improving them early.
* Hunt writing is very much a marathon. Make sure you are not burning out too early.
* Disasters will happen, this is just what it is like to work on a big project. Usually they are fixable.
* You will not always get your way, this is also just what happens in big projects. Try to deal with it.
* Absolutely do not feel obligated to do as much tech and art as teammate did in this Hunt. We went all-out
on both because we have a lot of artists, a lot of software developers, and pretty recent experience for both.
Out of Mystery Hunts in the past 5 years, Galactic's and teammate's are easily the most tech-heavy. It is not a coincidence
that both Hunts were written by teams that had recently ran puzzlehunts with many interactive puzzles. I do not
think our Hunt site would have been possible if the tech team had not written puzzlehunt code in the same
codebase for two years prior.
* Make sure you have at least one round of easy puzzles!
* Mystery Hunt got a *lot* more popular during the pandemic. It's shrunk a bit, but not a lot. For reference:
the 2020 Hunt had 150 teams and the 2023 Hunt had 300 teams. The load for answering hints, emails, etc.
scales with the number of teams. Make extra sure your flows there are seamless.
* Mystery Hunt is also becoming more multicultural. There are a larger number of foreign teams where English
is their second language. Maybe this is PTSD from hinting non-native speakers through Inscription, but
consider having your intro puzzles not depend on fluency with English language or US culture. Otherwise something
like this might happen.

![Hints](/public/mh-2023/hints.png)
{: .centered }
* Although we had accessibility checks throughout our Hunt writing process, we didn't have an accessibility lead
and we think this was a mistake in retrospect. The benefit of having an accessibility lead is that it empowers
someone to make a fuss about accessibility issues. Consider if you want to have advocates for other groups as well.
During Hunt we had someone unofficially start advocating for small teams while huntcomm was making
decisions that would only affect top teams, and I think it was good that happened.
* Read all the post-Hunt posts you can to see how people react to Hunt, then don't take them too seriously.
The class of Mystery Hunt writers is usually biased towards top teams and doesn't reflect everyone.


My Future of Puzzle Writing
------------------------------------------------------------------------------

I do not have any burning puzzle ideas left to write, but I told myself this at
the start of Mystery Hunt. I said I did not expect Mystery Hunt to encourage me to work harder on puzzles, and
then it did. I have, very continuously throughout the year, asked myself if
this was all worth it, and the answer was not clearly yes.
Still, if you offered me $1000 to never write another puzzle, I wouldn't take the money.

So, uh, ask me later? I care about puzzles
a lot. I wouldn't have written for Hunt if I did not care. That continues to be true. Puzzles are cool.
I'm just tired and want to do other things in my free time. Maybe I will write again, but right now
I want a hobby, not a job. We climbed the mountain, and came back down.


Cut Paragraphs
----------------------------------------------------------------------

To put on my CS / machine learning researcher hat for a bit, a puzzle is something
that compresses very well. Random data cannot be compressed well, because there is
no underlying truth or explanantion. Scientific experiments compress a bit better,
because there are often underlying physical or mathematical principles that explain how the
world works. Puzzles are then at the very extreme end of the scale - they are designed
to have most of their data pointing towards a key idea. "Breaking in" or finding
that idea can be *hard*, but discovering that compression algorithm is what triggers
the dopamine rush that makes solvers want to keep doing puzzles.

If that's why people solve puzzles, why do people write puzzles? Well, some
people do so because they've had fun solving puzzles, and want to try something new.
I'd say that's why I first started.
solving puzzles, and want to try doing so. I'd say this is how I first got started.
Another reason is that you want to make a puzzle about one of your other niche
interests or hobbies, since puzzle solving lends itself well to pushing people to
research a topic in lots of detail. That's how I restarted writing puzzles.

In the long run though, I find I get the most motivation from figuring out how to
create an interesing solve path, and making the puzzle rhyme with itself as much as
possible. Designing a puzzle is itself a puzzle, where
you're trying to figure out what steps are interesting and guess what wrong turns
a solver could make. In doing so, you're naturally constrained in what information
you can give, because every bit of extra information is something that might point
in a *different* direction than the idea you want the puzzle to express. It's
challenging, but it can be very rewarding to find a way to solve design problems
within the constraints you've created for yourself.

On some puzzles I've made, I'd be satisfied
if just 1 person had fun solving it end-to-end, understanding its design to the same
level of depth that I did at construction time. I'd say this is my answer to
[why people spend so much time creating such ephemeral experiences](https://mitadmissions.org/blogs/entry/two-hundred-puzzles-4/#fading-together).
I get a lot of self-satisfication even before the puzzle goes live. Blogging is
similar for me. There are things I get out of the writing experience that I'd
still get if no one read my blog.

And, in the same way as blogging, I still want to share my puzzles, because as much
as I get from doing it for myself, there are things I get from broadcasting that
I don't get anywhere else.

Solver Constructor Tension
-------------------------------------------------------------------------------------

There is a core tension between puzzle solver and puzzle constructor. Puzzlehunts
are at their best when their solutions are not obvious.
The solutions *have* to be indirect. You want to leave enough space for the solvers
to work with the puzzle and discover the answer for themselves. If you don't give
that mental room, and railroad solvers from start to finish, then it's still an
object with an answer, but it's not a puzzle anymore. It's more following directions.

Or, put another way: if it's impossible to get completely lost, then it's not a puzzle.

> The key to making a detective game fun to puzzle out is that you have to give the player as many opportunities as possible to be wrong. If you steer them through finding the clues and give away the answer anyway then they can never be wrong. If you give them three dialog options to pick from then it’s pretty easily brute forced. Meanwhile, [Return of the Obra Dinn] has you fill in multiple blanks that all have multiple possible entries, and there could be hundreds if not thousands of wrong combinations. And you can’t brute force that, your only recourse is to actually be smart enough to figure it out.

([Yahtzee Croshaw, "Great Detective Games Let You Fail Miserable"](https://www.escapistmagazine.com/great-detective-games-let-you-fail-miserably-extra-punctuation/))
(: .centered }

(Side note: Return of the Obra Dinn is excellent, even if you aren't "a video game person",
definitely recommend.)

This, though, is where you have the tension. If it's possible to get completely lost in
a puzzle, not everyone is going to get the full experience.

but if it's possible to get completely lost, not everyone is going to get the full
experience.

Take the duck konundrum. It's literally a series of directions. They're hit-or-miss
as a puzzle type, but when they hit, it is usually because following the directions is
hard for some reason. What if we misread a line?
What if we have to start over? That tension is the chance for failure that makes a
puzzle a puzzle.

(does the section above even fit here)

Aphorisms About Writing MIT Mystery Hunt
---------------------------------------------------------------------

Although there are a lot of stories about Hunt, the main one is about its difficulty.

I don't want the difficulty of Hunt to overshadow everything else, but I'd be lying if I tried
to downplay the way it affected the solve experience. There is no one reason that Hunt went long. Like
most engineering postmortems, it's more like there were many small things that accumulated, which either
went unnoticed or were noticed at a point where it was too late to change.

Perhaps the easiest way to tell that story, is to present my theory on what it's like to write Mystery Hunt,
and we'll see how it played out.

**Writing Mystery Hunt is fundamentally a game where you run out of time.** It seems like every Mystery Hunt,
without fail, has lots of work happen in the week leading up to Hunt. That's not because every construction
team sucks at time management. It's because the amount of work that *could* be put into a Mystery Hunt is
essentially unbounded. You could always try to get more testsolve of a puzzle to get more data about
sticking points, or replace a puzzle that has low fun ratings, or address a website bug, clean up an art
asset. But you get one year to write Hunt. You don't have time to make things perfect, you have to ship.
Move fast, and try not to break things. A team has to choose where it will not fully resolve things for
the sake of getting other things done.

**Decisions you make early can have big repurcussions later.** One example is theme selection, which is
always done first but influences basically the entire year afterwards. The more notable example is the
puzzles themselves. The writing order is metametas -> metas -> feeders, and
they have to be done serially. Each metameta or set of metas is written simultaneously, so you're
picking how many puzzles the Hunt will have long before they exist. It's not impossible to adjust puzzle counts
later, but it's usually pretty hard to, especially if you're considering cutting a round with a meta
that someone worked hard on.

**Team member availability will always shift.** Some people will spend less time than they expected, some
will spend more, and in general, there are always people who will have to drop out for good reasons.
Everyone starts excited and motivated at the start of Hunt, when you are scoping things, but by the middle
some people will have checked out.

Very soon after getting a copy of [spoilr](https://github.com/Palindrome-Puzzles/2022-hunt) in March,
the tech team had a meeting to decide what we wanted to do for our tech stack. We saw three options:

(Explain silenda here too.)

* Use tph-site, reimplementing things we liked in the spoilr code.
* Use spoilr, reimplementing things we wanted from tph-site.
* Combine both codebases together into one Django project.

Oh, I haven't described what Django is! Uhhhhh, okay, super quick crash course.

Django is a Python library for building web applications. You define Python classes (normally
called models) that represent what you want the database to look like. You then define views,
Python functions that take incoming requests, do processing that might query from or save to the
database, and return a response. Finally, the output
responses get rendered to the user in the frontend. I first learned Django 10 years ago and somehow
every website I've worked on has used it in some way. All the tech stacks I've mentioned use Django,
including tph-site.

Where tph-site differs from the other sites is on its frontend.
Puzzlehunt CMU, gph-site, and Spoilr all use HTML template files that are rendered server side based
on the database. This is what the Django tutorial recommend. Meanwhile, tph-site uses Django to drive
a React + Next.js based front-end.

Oh, I haven't described what React or Next.js are either! Uhhhhh, okay, round 2.

React is a Javascript library whose organizing principle is that you describe your page in components,
that each either have internal state or state passed from whatever creates the components. Each
components describe what it ought to look like based on its state, and whenever the state is updated,
anything that could depend on that state is entirely rerendered. This adds extra layers between your
code and the resulting HTML, but also makes it easier to build dynamic or interactive web pages.
You can do the same with raw Javascript, but it'll involve more boilerplate and state management
on your end. Next.js is then a web framework that makes it easier to pass React state from the server,
rendering pages server-side when possible to make load times faster for the users. This is especially
useful for puzzlehunts, where you want to do as many things server-side as possible to prevent spoilers
leaking to the front-end.

The tph-site fork exists because teammate devs had experience with React, and for Teammate Hunt 2020
implementing the Ninteamdo Playmate without React was declared a non-starter. Since then we've had
multiple years of experience working with tph-site. Given the combination of story goals and familiarity
with the code, we elected to stick with tph-site. At the same time, we liked parts of spoilr, so we
decided to merge their spoilr HQ management code with the tph-site code.

Like most software integrations, the end result is a bit more complicated than both in isolation, and
tph-site was already more complex than gph-site thanks to us merging Django code with React code. As of
the writing of this post, we're still working on cleaning up the code into a releasable state.

In general, one of teammate's strengths is that we have a lot of tech literacy and software chops, so
we're able to manage the higher tech complexity that enables the interactive puzzles and websites that
we want to create. I'm not sure what TTBNL plans to use for their Hunt, but in general, I would only
recommend tph-site if you have a need for lots of interactivity. Otherwise, using a purely Django
setup like gph-site is fine. The various Galactic Puzzle Hunts prove that you can do plenty of
interactive puzzles in that setup too.
