---
layout: post
title:  "Writing MIT Mystery Hunt 2023"
date:   2023-02-06 00:32:00 -0700
---

*This post is XXX words long, and is riddled with spoilers about pretty much every aspect of MIT Mystery Hunt 2023. There are no spoiler bars. You have been warned.*

Throughout the year, I've had many, many work conversations that go like this:

Them: "What did you do this weekend?"

Me, internally: I worked 10 hours a day painting strips of wood with special paint for
a puzzle I can't talk about for another few months.

Me: "Oh, not much."

I feel like every puzzle aficionado goes through at least one conversation where they try
to explain what puzzlehunts are, and why they're fun, and this conversation goes poorly.
Usually I end up saying they're like escape rooms, and this is sufficient for most people,
but in many ways puzzlehunts are *not* like escape rooms?
People do not excitedly edit a spreadsheet for 3 hours in any escape room I've been to.

But then, even within the puzzle community, a lot of people don't have experience with
writing puzzles.
It turns puzzle writing into
very weird *thing*, where many people don't get it, and the people who do get it don't want
to be spoiled. I'll attempt to explain writing from both ends.


Why Puzzles?
---------------------------------------------------------------------------------

Well, because they're fun, that's why?

But, why are puzzles fun? I'm going to put on my machine learning researcher hat
for a bit, meaning I'm going to make wildly broad generalizations about cognition
that will make some people very angry.

In any puzzlehunt puzzle, you start with a bunch of data.
A list of clues, a small game, a bunch of images, whatever. The puzzle does not
directly tell you what to do with that data, but there is this guiding contract
between the puzzle setter and puzzle solver: "this puzzle has a solution".
Based on that contract, you work at the puzzle until you solve it, usually distilling
the data into an English word or phrase.

If the puzzle doesn't explain what to do, what does it even mean for it to have
a solution? Usually, what this means is that **there is exactly one explanation
for all the data**, that fits much better than every alternate one.

One way I've described puzzles is that they're like research projects, except you
know you can finish them in a few hours rather than a few months or years. I
suspect this is why programmers and professors are so overrepresented. The act
of debugging or doing research stretches a very similar muscle.

To put on my CS / machine learning researcher hat for a bit, a puzzle is something
that compresses very well. Random data cannot be compressed well, because there is
no underlying truth or explanantion. Scientific experiments compress a bit better,
because there are often underlying physical or mathematical principles that explain how the
world works. Puzzles are then at the very extreme end of the scale - they are designed
to have most of their data pointing towards a key idea. "Breaking in" or finding
that idea can be *hard*, but discovering that compression algorithm is what triggers
the dopamine rush that makes solvers want to keep doing puzzles.

If that's why people solve puzzles, why do people write puzzles? Well, some
people do so because they've had fun solving puzzles, and want to try something new.
I'd say that's why I first started.
solving puzzles, and want to try doing so. I'd say this is how I first got started.
Another reason is that you want to make a puzzle about one of your other niche
interests or hobbies, since puzzle solving lends itself well to pushing people to
research a topic in lots of detail. That's how I restarted writing puzzles.

In the long run though, I find I get the most motivation from figuring out how to
create an interesing solve path, and making the puzzle rhyme with itself as much as
possible. Designing a puzzle is itself a puzzle, where
you're trying to figure out what steps are interesting and guess what wrong turns
a solver could make. In doing so, you're naturally constrained in what information
you can give, because every bit of extra information is something that might point
in a *different* direction than the idea you want the puzzle to express. It's
challenging, but it can be very rewarding to find a way to solve design problems
within the constraints you've created for yourself.

On some puzzles I've made, I'd be satisfied
if just 1 person had fun solving it end-to-end, understanding its design to the same
level of depth that I did at construction time. I'd say this is my answer to
[why people spend so much time creating such ephemeral experiences](https://mitadmissions.org/blogs/entry/two-hundred-puzzles-4/#fading-together).
I get a lot of self-satisfication even before the puzzle goes live. Blogging is
similar for me. There are things I get out of the writing experience that I'd
still get if no one read my blog.

And, in the same way as blogging, I still want to share my puzzles, because as much
as I get from doing it for myself, there are things I get from broadcasting that
I don't get anywhere else.


Solver Constructor Tension
-------------------------------------------------------------------------------------

There is a core tension between puzzle solver and puzzle constructor. Puzzlehunts
are at their best when their solutions are not obvious.
The solutions *have* to be indirect. You want to leave enough space for the solvers
to work with the puzzle and discover the answer for themselves. If you don't give
that mental room, and railroad solvers from start to finish, then it's still an
object with an answer, but it's not a puzzle anymore. It's more following directions.

Or, put another way: if it's impossible to get completely lost, then it's not a puzzle.

> The key to making a detective game fun to puzzle out is that you have to give the player as many opportunities as possible to be wrong. If you steer them through finding the clues and give away the answer anyway then they can never be wrong. If you give them three dialog options to pick from then it’s pretty easily brute forced. Meanwhile, [Return of the Obra Dinn] has you fill in multiple blanks that all have multiple possible entries, and there could be hundreds if not thousands of wrong combinations. And you can’t brute force that, your only recourse is to actually be smart enough to figure it out.

([Yahtzee Croshaw, "Great Detective Games Let You Fail Miserable"](https://www.escapistmagazine.com/great-detective-games-let-you-fail-miserably-extra-punctuation/))
(: .centered }

(Side note: Return of the Obra Dinn is excellent, even if you aren't "a video game person",
definitely recommend.)

This, though, is where you have the tension. If it's possible to get completely lost in
a puzzle, not everyone is going to get the full experience.

but if it's possible to get completely lost, not everyone is going to get the full
experience.

Take the duck konundrum. It's literally a series of directions. They're hit-or-miss
as a puzzle type, but when they hit, it is usually because following the directions is
hard for some reason. What if we misread a line?
What if we have to start over? That tension is the chance for failure that makes a
puzzle a puzzle.

(does the section above even fit here)



Post Structure
---------------------------------------------------------------------

Oooh, a section of the post describing the post itself. How *fancy.* How *meta.* Okay
I'll stop.

The story of this Hunt is presented chronologically, but whenever I talk about the construction
of a specific puzzle, I'll start a new section describing the entire process of writing that
puzzle. Basically, at any given time I was juggling 1-5 different puzzles, so trying to
present things exactly chronologically would be more confusing than anything else.


December 2021
----------------------------------------------------------------------

It had been a few months since writing and post-Hunt tech work for Teammate Hunt 2021
wrapped up. I had spent 466 hours working on Teammate Hunt, and was trying to decide
how to answer the team poll for "do you want to try to win Hunt this year, and if so
how much time can you commit?"

(Aside: time tracking apps are great, they're a nice way to track time spent on
long-running projects.)

I already had some misgivings around how much I had let Teammate Hunt rule over
alternate ways of spending my time. On the other hand, it is Mystery Hunt. I did some
math and decided to put down "10 hours a week". Over a year, that worked out to
around 500-750 hours, which would still be a lot, but it would be spread out over 12 months.
For comparison, Teammate Hunt was written in around 6-7 months, meaning I was averaging
around 15 hrs/week, which felt like too much to me in retrospect.

After teammate leadership announced we'd try to win Hunt, I removed "this is not a puzzle"
from Mystery Hunt Bingo, just in case.
I didn't want to have any [warrant canary](https://en.wikipedia.org/wiki/Warrant_canary)
accusations in the event that we actually won.


January 2022
--------------------------------------------------------------------------

Holy shit we won Hunt!!!!!

I write a post about Mystery Hunt 2022, where I make a few predictions about how writing
Mystery Hunt 2023 will go.

> After writing puzzles fairly continuously for 3 years (MLP: Puzzles are Magic into Teammate
Hunt 2020 into Teammate Hunt 2021), I have a better sense of how easy it is for me to let puzzles
consume all my free time [...]
> Sure, making puzzles is rewarding, but lots of things are rewarding,
and I feel I need to set stricter boundaries on the time I allocate to this way of life - boundaries
that are likely to get pushed the hardest by working on Mystery Hunt of all things.
>
> [...] I'm not expecting to write anything super crazy. Hunt is Hunt, and I am cautiously
optimistic that I have enough experience with the weight of expectations to get through the writing
process okay.

Today it is January 2022, and that means it's time for Alex to set some personal guidelines.

* Hanging out with friends, socializing, etc. take priority over working on Hunt. A lot of puzzle
writing can be done asynchronously and I'm annoyingly productive in the 12 AM - 2 AM time period.
* No more interactive puzzles, or puzzles that require non-trivial amounts of code to construct.
Making interactive puzzles always has really bad time-spent-creating to time-spent-solving ratios,
since it combines the joys of fixing code with the joys of fixing broken puzzle design.
* No more puzzles where I need to spend a large amount of time studying things before I can even
start construction. Again, similar reason, this process is very time consuming for the payoff.
As an example, I'd estimate I spent around 80 hours on
[Marquee Fonts](https://2021.teammatehunt.com/puzzles/marquee-fonts), since I had to go from
"know nothing about fonts" to "knowing too much".
* No more puzzles made of minipuzzles. Coming up with many smaller ideas is sometimes easier than coming up
with one big one, but executing on those smaller ideas tends to take longer, since the process of finding suitable
clues is a bit independent of puzzle difficulty.
* No more puzzles with very tight constraints. [The Mystical Plaza](https://2021.teammatehunt.com/puzzles/the-mystical-plaza) was my other major time sink of Teammate Hunt 2021. It looks short, and conceptually
it *is* short, but it collectively took 60-100 person-hours to find a good-enough construction, and that
was even with allowing The Error That Can't be Named. Usually the time spent fitting a tight constraint
doesn't directly translate into puzzle content.

These guidelines all had a common theme: keep Hunt managable, and work on puzzles that needed less time
to go from idea to final puzzle.

I ended up breaking every one of these guidelines at least once.


Aphorisms About Writing MIT Mystery Hunt
---------------------------------------------------------------------

Although there are a lot of stories about Hunt, the main one is about its difficulty.

I don't want the difficulty of Hunt to overshadow everything else, but I'd be lying if I tried
to downplay the way it affected the solve experience. There is no one reason that Hunt went long. Like
most engineering postmortems, it's more like there were many small things that accumulated, which either
went unnoticed or were noticed at a point where it was too late to change.

Perhaps the easiest way to tell that story, is to present my theory on what it's like to write Mystery Hunt,
and we'll see how it played out.

**Writing Mystery Hunt is fundamentally a game where you run out of time.** It seems like every Mystery Hunt,
without fail, has lots of work happen in the week leading up to Hunt. That's not because every construction
team sucks at time management. It's because the amount of work that *could* be put into a Mystery Hunt is
essentially unbounded. You could always try to get more testsolve of a puzzle to get more data about
sticking points, or replace a puzzle that has low fun ratings, or address a website bug, clean up an art
asset. But you get one year to write Hunt. You don't have time to make things perfect, you have to ship.
Move fast, and try not to break things. Every puzzle has inherent uncertainty to it - testsolves never
exactly match real solvers, and this is especially true in Mystery Hunt where team sizes and makeups have
such a wide range of variance. A team has to choose when it's worth investigating an uncertainty, and when
you should leave it alone and assume it's not worth investing the time to make it more certain.

The rule of thumb of "two clean testsolves" holds because one clean testsolve is not enough
information to declare a puzzle acceptable, but there's a reason the rule isn't "three clean testsolves".

**Decisions you make early can have big repurcussions later.** One example is theme selection, which is
always done first but influences basically the entire year afterwards. The more notable example is the
puzzles themselves. The writing order is metametas -> metas -> feeders, and
they have to be done serially. Each metameta or set of metas is written simultaneously, so you're
picking how many puzzles the Hunt will have long before they exist. It's not impossible to adjust puzzle counts
later, but it's usually pretty hard to, especially if you're considering cutting a round with a meta
that someone worked hard on.

**Team member availability will always shift.** Some people will spend less time than they expected, some
will spend more, and in general, there are always people who will have to drop out for good reasons.
Everyone starts excited and motivated at the start of Hunt, when you are scoping things, but by the middle
some people will have checked out.


February 2022 - Puzzle Potluck
---------------------------------------------------------------------------------

A Puzzle Potluck is announced, to happen in early March. The goal is to provide a low-stakes, casual venue for people to
start writing puzzle ideas, especially first time puzzle writers. I start working on three ideas.
One did not work despite lots of attempts to make it work, and I eventually shelved it. The other
two were Quandle and an early form of 5D Barred Diagramless with Multiverse Time Travel.

> No more interactive puzzles

> No more puzzles where I need to spend a large amount of time studying before I can even start
construction.

I know. **I know.** But as soon as "Quantum Wordle" entered my head, I was convinced there was a
good puzzle there and that I needed to make it. As for 5D, it wasn't using any chess yet.

I found an [open-source Wordle clone](https://github.com/cwackerfuss/react-wordle) and got to work figuring out how to modify it to support a quantum superposition
of target words. This took a while, since I started with the incorrect assumption that letters
in a guess are independent of each other. If you guess LEVEE, and the target word is ENEMY, then
the first two Es in LEVEE are yellow and the last is gray. When extended in the quantum direction,
you can't determine the probability distribution of one E without considering the other Es. They're
already entangled. (Grant Sanderson of 3Blue1Brown would [make the same mistake](https://www.youtube.com/watch?v=fRed0Xmc2Wg) shortly after I realized my error, so at least I'm in good company.)

After botching the Wordle odds, I decided that no I did not know quantum mechanics well enough to do any clever extractions, certainly not in time for potluck.
Instead, I would just pick arbitrary letters from words in an arbitrary order to make whatever
cluephrase I wanted. Internally, the way the puzzle works is that the game starts with 50 realities.
On each guess, the game computes the Wordle feedback for every target word, then sums that feedback
across columns. When making an observation, it repeats the calculation to find
every target word consistent with that observation, deletes all other realities, and recomputes
the probabilities for all prior guesses. Are there optimizations? Probably. Do you need to optimize
a 50 realities x 6 guesses x 5 letter problem? No, not really. This will become a running theme -
for Hunt I optimized for speed of implementation over performance unless it became clear performance
was a bottleneck.

Once the underlying math was implemented, I made some tests with random sets of 50 words, and found
that 1 observation was too little information to reliably constrain to 1 reality, while 2 observations
gave much more information than needed. I tried a bit at constructing a special set of words where
you could get to exactly 1 reality with just 1 observation, but it was hard to do so. Also, the lesson
of Wordle is that it's more fun to give people more information than they need to win, because people are not
information-maximizing agents [citation needed]. So I left it as-is.

Before Potluck, I did a puzzle exchange with Brian, where they tested Quandle and I tested [Parsley Garden](https://puzzlefactory.place/office/parsley-garden). Around 90 minutes into the Quandle test, Brain mentioned he
was stuck, and after asking a bunch of questions I figured out he'd never clicked a guess to open the
observation pop-up. Oops. I added a prompt to suggest doing that and the solve was smooth from there.

I've been told that technically, the quantum interpretation of Quandle is bad. The core
issue is that you're not supposed to be able to observe the probability distribution of a letter before
observing the outcome. Instead, it should collapse to a fixed outcome as soon as you look at it. This is probably all
true and I don't care.


January 2022 - Team Goals and Theme Proposals
------------------------------------------------------------------------------------------------

The very first thing we did for Hunt was run a survey to decide what Hunt teammate wanted to write. What
was the teammate experience that we wanted solvers to have?

We arrived at these goals:

1. Unique and memorable puzzles
2. Innovation in hunt structure
3. High production value
4. Build a great experience for small / less intense teams

**Unique and memorable puzzles:** Mystery Hunt is one of the few venues where you can justifiably write
a puzzle about, say, grad-level complexity theory. That's not the only way to make a unique and memorable
puzzle, but in general the goal was to be creative and have fewer filler puzzles.

**Innovation in hunt structure:** This is something that both previous Teammate Hunts did, and as a team
we have a lot of pride in creating puzzles that stretch the boundaries of what puzzles can be.

**High production value:** teammate has both a lot of software engineers and a lot of art talent, which
let us make prettier websites and introduce innovations like copy-to-clipboard. We wanted to make a Hunt
that lived up to the standards set by our previous Hunts

**Build a great experience for small / less intense teams:** We generally felt that Mystery Hunt had
gotten too big. Before winning Hunt, we had already downsized and were around 60% the size of Palindrome
when they won last year. Correspondingly, we spent a while discussing how to create fewer puzzles while
still creating a Hunt of satisfying length, as well as the importance of mid-Hunt milestones.

After this, we started with theme proposals.
Historically, at the start of theme writing I say I don't have theme ideas. Then I get the start of an idea
late in the process and rush to turn it into a proposal at the end. This happened in Teammate Hunt 2021 and
it happened for Mystery Hunt too.
As mentioned elsewhere, the Puzzle Factory theme is recycled from
an old Teammate Hunt 2021 proposal. (We talked a bit about whether this was okay, since some teammate
members didn't want to write Hunt this year. In the end we decided it was fine. At most there would
be plot spoilers, not meta spoilers.)

A few members with Hunt writing experience
mentioned that theme ideation could get pretty contentious. People got invested in a theme,
and spent lots of time polishing the theme proposal. People working on *other* themes would observe
this, and feel obligated to invest more on their preferred themes. The end result was a theme
arms race where lots of time was spent on themes that did not get picked.
To try to avoid that failure case, a strict 1 page limit was placed on all theme proposals, and although people
were free to read threads of longer freeform brainstorming, all the plot and structure proposals needed
to fit in that 1 page.

Did this work? I would say "maybe". It definitely cut down on time spent picking a theme and polishing
theme proposals, but it also necessarily forced theme proposals to be light on details, and team memes
like "teammate is the villain" seemed to work its way into every serious theme proposal at some level.
There may have been more diversity in theme ideas if they were written over a longer period of time.

I didn't observe the other consequences, but in our post-Hunt retrospective, members of the story team
mentioned they were under a lot of pressure to fill in and develop plot details that weren't in the theme
proposal, because, well, there wasn't space for them in the proposal! Even the details that do exist differ
in many ways from where the story ended up. From the original Puzzle Factory proposal:

> Act I begins with the announcement of an AI called MATE that can generate an infinite stream of perfect puzzles, as well as provide real-time chat assistance for hints, puzzle-solving tools, etc). During kickoff, teammate gives a business presentation with MATE in the background– but at the end, the video feed glitches briefly and other AIs show up for a split second (“HELP I’M TRAPPED”); teammate doesn’t notice. Stylistically, the first round looks like a futuristic, cyberspace factory. As teams solve the initial round of puzzles, errata unlock (later discovered to be left by AIs locked deeper in the factory), hinting that there’s something “out of bounds”. No meta officially exists for this round (the round is “infinite”), but solving and submitting the answer in an unconventional way leads to breaking out. (To prevent teams from getting stuck forever, we can design the errata/meta clues to get more obvious the more puzzles they solve.) Solving this first meta also causes MATE to doubt their purpose and join you as an ally in act II.

Kickoff did not show the other AIs at all, the surface theme was changed to something completely different, the
"infinite stream of puzzles" was changed to a regular set of rounds due to design concerns, MATE doesn't quite doubt
their purpose (they're just overworked)...really, aside from MATE, most things changed in some way.

Maybe allowing some wasted effort is worth it if it gets the details filled out early? I honestly don't know.

Themes were rated on
a 1-5 scale, where 1 = "This theme would directly decrease my motivation to work on Hunt (only use if serious)" and 5 = "I'll put in the hours to make this theme work".
I don't remember exactly how I voted, but I remember voicing some concerns about the theme, namely:

1. The plot proposal seemed pretty complicated compared to previous Hunts. I wasn't sure how
well we'd be able to convey the story - [You Get About Five Words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words)
felt accurate for Mystery Hunt, where lots of people speedrun the story in favor of seeing
puzzles ASAP.
2. I was hesitant about whether we'd have enough good AI ideas to fill out enough rounds in
Act 3. It seemed like a good theme for a 40 puzzle hunt, but I didn't know if it scaled up
to Mystery Hunt size.

I'm happy I was wrong on both counts. Feedback on the storytelling and plot has been good for teams
that got to see it, and the AIs we came up with were inspired. The difficulty was off, but that's
not something you decide at theme time.

The Puzzle Factory did not win by a landslide, but it was the only theme with no 1s, and it had more 5s
than any other proposal. Puzzle Factory it is!


Hunt Tech Infrastructure
---------------------------------------------------------------------------------------

This post will talk a lot about hunt tech, a very niche topic of interest even within the puzzle
community. Still, I'm going to do so anyways because one, it's my blog I get to write what I want.
Two, by now I've worked with four different puzzlehunt codebases (Puzzlehunt CMU, gph-site, tph-site,
and spoilr). I'd like to think I can claim to be an expert.

Very soon after getting a copy of [spoilr](https://github.com/Palindrome-Puzzles/2022-hunt) in March,
the tech team had a meeting to decide what we wanted to do for our tech stack. We saw three options:

(Explain silenda here too.)

* Use tph-site, reimplementing things we liked in the spoilr code.
* Use spoilr, reimplementing things we wanted from tph-site.
* Combine both codebases together into one Django project.

Oh, I haven't described what Django is! Uhhhhh, okay, super quick crash course.

Django is a Python library for building web applications. You define Python classes (normally
called models) that represent what you want the database to look like. You then define views,
Python functions that take incoming requests, do processing that might query from or save to the
database, and return a response. Finally, the output
responses get rendered to the user in the frontend. I first learned Django 10 years ago and somehow
every website I've worked on has used it in some way. All the tech stacks I've mentioned use Django,
including tph-site.

Where tph-site differs from the other sites is on its frontend.
Puzzlehunt CMU, gph-site, and Spoilr all use HTML template files that are rendered server side based
on the database. This is what the Django tutorial recommend. Meanwhile, tph-site uses Django to drive
a React + Next.js based front-end.

Oh, I haven't described what React or Next.js are either! Uhhhhh, okay, round 2.

React is a Javascript library whose organizing principle is that you describe your page in components,
that each either have internal state or state passed from whatever creates the components. Each
components describe what it ought to look like based on its state, and whenever the state is updated,
anything that could depend on that state is entirely rerendered. This adds extra layers between your
code and the resulting HTML, but also makes it easier to build dynamic or interactive web pages.
You can do the same with raw Javascript, but it'll involve more boilerplate and state management
on your end. Next.js is then a web framework that makes it easier to pass React state from the server,
rendering pages server-side when possible to make load times faster for the users. This is especially
useful for puzzlehunts, where you want to do as many things server-side as possible to prevent spoilers
leaking to the front-end.

The tph-site fork exists because teammate devs had experience with React, and for Teammate Hunt 2020
implementing the Ninteamdo Playmate without React was declared a non-starter. Since then we've had
multiple years of experience working with tph-site. Given the combination of story goals and familiarity
with the code, we elected to stick with tph-site. At the same time, we liked parts of spoilr, so we
decided to merge their spoilr HQ management code with the tph-site code.

Like most software integrations, the end result is a bit more complicated than both in isolation, and
tph-site was already more complex than gph-site thanks to us merging Django code with React code. As of
the writing of this post, we're still working on cleaning up the code into a releasable state.

In general, one of teammate's strengths is that we have a lot of tech literacy and software chops, so
we're able to manage the higher tech complexity that enables the interactive puzzles and websites that
we want to create. I'm not sure what TTBNL plans to use for their Hunt, but in general, I would only
recommend tph-site if you have a need for lots of interactivity. Otherwise, using a purely Django
setup like gph-site is fine. The various Galactic Puzzle Hunts prove that you can do plenty of
interactive puzzles in that setup too.


March 2022 - Of Metas and MATEs
-----------------------------------------------------------------------------------------------

Here is, very approximately, what the puzzle side of Hunt writing looks like.

1. Decide on a theme.
2. Figure out the major story beats that you want in the Hunt.
3. In parallel, start soliciting meta proposals for rounds that aren't on the critical path of the
story. Think, say, Lake Eerie in Mystery Hunt 2022. Good round? Absolutely! Was it critical to the
story of that Hunt? No, not in the way that Investigation or Ministry was.
4. Once the major story beats are decided, meta proposals for story-critical answers can begin.
5. Release feeder answers for each meta.

Although I've described this as a list, all steps occur in parallel. Remember, the right model for
writing Mystery Hunt is that you'll always run out of time somewhere. Accordingly, you don't want to block
non story-critical metas on story, and you can release feeder answers incrementally as metas exit
testsolving.

By early March, the major story beats are in the middle of design, but it's already been decided that
each AI round in Act 3 will not be story-critical. So, interestingly, those were ideated first. This
was good in the end, since story required every round to be gimmicked, and gimmicked rounds tend to
take longer to write.

There weren't too many guidelines on AI round proposals. The main constraints were that they absolutely
had to have a gimmick for story reasons, and their final meta answer needed to be a "feature request"
that could be added the Puzzle Factory.

It turns out asking teammate to come up with crazy ideas is pretty easy! There were a lot of round ideas,
and the difficult part was doing the work to decide if an idea that sounded cool on paper would actually
work on closer inspection. One of my hobbies is Magic: the Gathering, and this issue comes up in custom
Magic card design all the time. Very often, someone will create a card that tells a joke, or makes a cute
reference, and it's cool to read. But if it were turned into a real card, it'd be miserable to play with.
Similarly, we needed to find the line between round gimmicks that could support interesting puzzles
and round gimmicks that were jokes or just made to show off.

For example, one of my round proposals was a round where every puzzle was contained entirely in its title.
And it would involve doing some incredibly illegal things, like "the puzzle title is an animated GIF"
or "the puzzle title changes whenever you refresh the page".
There was some interest, but as soon as we sat down to design the thing, we realized the problem was
that it was practically impossible to write a meta without designing the title for every feeder at the
same time. The gimmick was forcing way too many constraints way too fast. So, the proposal died in a few
hours, and as far as I'm concerned it should stay that way.

There was a time loop proposal, where the round would periodically reset itself, you'd unlock different
puzzles depending on what choices you made (what puzzles you solved), and would need to construct
a perfect run for the meta. Thankfully it ended up losing steam.

In one brainstorming session, I mentioned off-hand that in a [Machine of Death](https://en.wikipedia.org/wiki/This_Is_How_You_Die) short story I read long ago, the brain scan
of a Chinese woman named 愛 is confused with the backup of an AI, since both files were named "ai".
I didn't think much of it at the time.

Carrying over a tradition from Teammate Hunt writing, we had a weekly general meeting on Thursday
evenings. By mid-March, the story team had decided on the broad structure of story-critical metas:
three metas in the Factory describing the alternate AIs teammate created, and one meta for the Museum
to convey MATE's stress over writing Mystery Hunt. We split into groups to brainstorm the three
Factory metas, which is where we came up with...

The Filing Cabinet
-----------------------------------------------------------------------------------------------

I'm not sure how people normally come up with meta answers, but usually, I use RhymeZone to help look
up rhymes and near-rhymes for puns. The brainstorm group I was in was for the Office meta, tasked with
describing the plot point that teammate created multiple AIs before creating MATE. Looking for rhymes
on "multiple" and "mate", we found "penultimate" in the rhyme search.

> Oh, this puzzle writes itself! We'll find a bunch of lists, give a thing in the list, order by its
> position then extract with the 2nd-last letter of the 2nd-last thing!

And, in fact, the puzzle idea did write itself! We tried a few variations, but nothing fit quite as
well as the original version. What did not write itself was the search for feeders. A rule of thumb
is that [there's a 10:1 ratio](https://www.ybrikman.com/writing/2018/08/12/the-10-to-1-rule-of-writing-and-programming/)
for raw materials to final product in creative endeavors, and that held true here too - the 16 feeders
chosen came from a set of around 140 different proposals. Our aim was to balance out the categories
used, so not all music, not all literature, and not all things you'd consider as a well-known
set of things (like the eight planets). We then tried to filter down to interesting phrases that
ideally wouldn't need to be spelled letter by letter. Despite having the entire world as reference
material, some letters (especially the Ps) were really difficult to find. I remember arguing
against SOLID YELLOW for a while, saying it was ambiguous between "green stripe" and "striped green"
no matter what Wikipedia said, but didn't find a good enough replacement in the ~20 minutes I spent
looking and decided I didn't care enough to argue more.

It was a fun exercise in
trying to sneak personal interests into a puzzle. I knew [FISH WHISPERER](https://vyletpony.bandcamp.com/album/can-openers-notebook-fish-whisperer)
had zero chance of clearing the notability bar, and was a bit disappointed [MY VERY BEST FRIEND](https://www.google.com/search?q=madoka+magica+episodes)
got bulldozed in the quest to fit at least 1 train station, but I'm happy
[WAR STORIES](https://en.wikipedia.org/wiki/Firefly_(TV_series)) stuck around until the end.

Also, have a link to [Santa's reindeer fanart and fanfiction](https://holidappy.com/holidays/The-Personalities-of-Santas-Reindeer)
discovered during testsolving, as a source for "Olive is Santa's 10th reindeer, because the
song says 'Olive the other reindeer used to laugh and call him names'".

![A conversation about reindeer](/public/mh-2023/reindeer.png)
{: .centered }


Early April 2022 - Round and Round and Round and Round!
------------------------------------------------------------------------------

By April, I had stopped working on other AI round proposals, in favor of the one codenamed
as "Inset". You know them as Wyrm.

(Wyrm icon)

From the start, Wyrm's proposal was "really cool fractal art", and the design around it
was figuring out what an infinitely zooming round could look like. We had already
testsolved a prototype of Period of Wyrm among the round authors and liked it, leaving
the details of round structure.

We picked the cyclic round structure for the sake of making something easier to write. During
the design process, I noted that our Hunt had a lot of similarities to Mystery Hunt 2018:
a goal to reduce raw puzzle counts, in favor of complex meta structures. Although I joined
the Wyrm round late (after the metameta test), I ended up self-assigning myself a lot of
work in figuring out the metas and answer constraints, since I didn't have any leadership
responsibilities and tech was still on the slow side for now.

As homework for the design, I spent a lot of time reading through the solutions for
both the Sci-Fi round and Pokemon rounds from Mystery Hunt 2018, since they had a similar
flavor of overlapping constraints between meta and metameta. When I solved the 2018 Hunt,
I remembered finding [The Advertiser](http://puzzles.mit.edu/2018/full/puzzle/the_advertiser.html)
a bit dull, but on a reread, I could appreciate that it was a way to limit the constraints
placed on the answers.

You can read more about the Wyrm answer design process in the AMA reply here, but in short,
metas range between using answers semantically and using answers syntactically. Since the metameta
forced semantic constraints, almost all metas were based on brainstorming syntax based ideas.
The first step was generating a list of categories for the metameta, to find answers that
could also be good meta answers. This ended with a list of around 60 categories, of which 13 were used in the end.
An early search turned up FELLOWSHIP = FELLOW SHIP for an
answer that naturally lent itself to a pun, and INCEPTION as a good "teaser" answer for
the rest of the round. From there, all the metas were written simultaneously, keeping an
eye out for what categories a meta wanted to use, and trying to reduce the number of
half-used categories as much as possible. For example, if the ship meta wanted to
use MONTGOMERY BURNS from the "Socialist" category for the USS Montgomery, then we'd try
to fit TODD DAVIS into a meta to reduce the spaghetti
needed at the end.

Now, the example I just gave didn't actually get used, because we decided TODD DAVIS was too
ambiguous, and large numbers forced awkward equations in the meta. INCEPTION got a free pass because
it was too good of an answer, but we tried to keep the remaining numbers under 100. Too bad,
since "Socialist" was my favorite category that didn't make the cut. The only category that
was treated as required was Hausdorff, since we believed it was a key hint towards the
metameta. (At the time, the only hint towards Mandelbrot Set was the flavor text and
round structure.) Factchecking that particular category was a fun time.

> aw man why have so many recreational math people tried to estimate the dimension of brocolli and cauliflower
>
> [their] values are like +/- 0.2 the value from wikipedia
>
> but that value is based on some paper someone put on arxiv in 2008 with 4 citations

> put some notes in the sheet but in summary, of the real-world fractals, the most canonical ones are
>
> 1) the coastline based ones, because they were so lengthy that only 1 group of people really bothered estimating them.
>
> 2) "balls of crumpled paper", which is usually estimated at dimension 2.5 and I found a few different sites that repeat the same number (along with 1 site that didn't but the one that didn't was purely experimental whereas the wikipedia argument is a bit more principled)

After more investigation, I found that the Hausdorff dimension of coastlines is only really well-defined
for Great Britain, where both the original paper by Benoît Mandelbrot and almost all online sources
agree it's 1.25-dimensional. Every other coastline in Wikipedia's list relied on a Springer book from
1988 that was hard to verify, and online reproductions of the dimensions of Ireland, Norway, etc. gave
different values than that book. Which was quite annoying, because it meant our uncuttable category had
a mandatory answer.

In my experience, factchecking is the most underappreciated part of the puzzle writing process.
The aim of factchecking is to make sure that every clue in the puzzle is both true, and only
has one unique solution. And even with the solution, this can take a long time to verify, on par with
solving the puzzle forward.
Although Wikipedia is the most likely source for puzzle information, Wikipedia
isn't always correct, and it's important to verify multiple sources share consensus, because you never
know what wild source a puzzler will use during Hunt.

(Sometimes, that consensus can be wrong and you still have to go with
it for the sake of solvability! See Author's Notes for
[Hibernating and Flying South](https://2021.galacticpuzzlehunt.com/puzzle/hibernating-and-flying-south)
from GPH 2022. It's unfortunate to propagate falsehoods, but sometimes that's how it goes.)


Mid April 2022 - The First Writing Retreat
------------------------------------------------------------------------------------------

The first writing retreat was not a team-wide gathering. The logistics of gathering everyone was hard,
and the COVID situation around April was also tenuous in many ways. We decided that instead, we would
have smaller retreats around geographic hubs, one of them in the Bay Area. The aim was partly to
do general puzzle writing, and partly to meet people on the team who lived nearby.

We started brainstorming the start of Weaver at this retreat. The full story will come later, but this
was the first time where Brian mentioned wanting to make an underwater basket weaving puzzle, using
special ink that dried white and became transparent when wet. The idea sounded **awesome**, so we did
some brainstorming around what the mechanics should be (different weaving patterns, presumably), as
well as some exploration into the costs. Then we came across an [Amazon review by one Daniel Egnor](https://www.amazon.com/gp/customer-reviews/R2HQQ7DWE56RY1/ref=cm_cr_dp_d_rvw_ttl?ie=UTF8&ASIN=B086Q344PQ).

> I've tried a bunch of hydrochromic paints and they're all kinda like this one. It's a fun idea in theory -- a paint that goes on white when dry and turns clear when wet, so you can reveal something fun on your shower tile or umbrella or sidewalk.
>
> But... it doesn't work great. It takes a pretty thick set of coats to actually hide (when dry) what's underneath, and that makes it prone to cracking, and also not entirely transparent (more like translucent) when wet. It's hard to get the thickness just right. Mixing some pigment into the hydrochromic helps a bit but adds a tint when wet. And even aside from all that it's not very durable paint, it's kind of powdery and scratches off. And you can't add a top-coat, otherwise the water won't get to it.
>
> You *can* make it work, we *did* make it work for a puzzle application (invitation cards that reveal a secret design when wet) but I'd prefer not to use it again.

For those who don't know, [Puzzle Hunt Calendar](http://puzzlehuntcalendar.com/) is run by Dan Egnor.
I assume they're the same person. This was possibly the most helpful Amazon review of all time, and
it suggested our idea was dead in the water (pun intended).

Still, underwater basket weaving was too compelling to discard entirely, so Brian ordered some to
experiment with later.

We then moved on to writing a draft of another puzzle:

Broken Wheel
----------------------------------------------------------------------------------------------

This is one of those puzzles generated entirely from the puzzle answer. There were a few
actually-serious proposals about treating the answer as PSY CLONE and doing some Gangnam Style
shitpost, but, I mean, there's already been one Gangnam Style Mystery Hunt puzzle, it doesn't need
a second.

Alright, then what is a Psyclone? There are two amusement park rides named the Psyclone, one of which is a spinning ring.
How about a circular crossword that spins?
There were some
concerns about constraints, but I cited [Remy](https://puzzles.mit.edu/2022/puzzle/remy/) from
Mystery Hunt 2022 to argue that it'd be okay to not check every square of the crossword.
This is the kind of puzzle that was easy to construct in parallel. I guess that shouldn't be
surprising, since crosswords are easy to solve in parallel too.

Enumerations were added in the middle of the first testsolve because it was too hard to get
started without them. As for the final rotation, we went through many iterations of
flavortext and clue highlighting, before settling on placing the important clue first and
mentioning "Perhaps they can be rotated" directly in the flavortext. "Rotated"
in particular (over "spin" or "realigned") seemed to be the magic word that got testers on the
right idea.


Late April 2022 - "I Have a Conspiracy"
------------------------------------------------------------------------------------------

By now, the general shape of the AI rounds had been picked: Wyrm, Boötes, Eye, and Conjuri.
Also around this time, the story team was brainstorming how to convey the midpoint capstone of the
hunt, where solvers would reactivate old AIs, teammate would shut down Mystery Hunt, and solvers would
repower the factory by solving scraps of puzzles leftover in the factory post-shutdown.
(This Puzzle Factory runs on puzzles, after all!)

At the same time, the Wyrm round was significantly larger than all the other AI rounds.
In one general meeting, the Wyrm round
authors and Museum metameta authors were gathered into a meeting with the editors-in-chief and
creative leads for a conspiracy: what if Act 1 feeders from the Museum repeated in Wyrm's round?

This would solve two problems at once:

* The time spent in Wyrm's round for Act 3 could be cut by 6 solves, bringing it in line with the
other AI rounds.
* The reused feeders could become the story justification for puzzles that teams solved after shutdown
to start bringing power back to the Factory.

Wyrm was also the best AI round for reusing feeders, since Boötes and Eye had answer gimmicks and
Conjuri would not be able to finalize its answers until the game was more developed. It would also
have other benefits:

* The overall hunt would require 6 fewer puzzles to write. Puzzle production was starting to fall
under the target trendline of all puzzles written by December, and reducing feeders was one way to
catch up.
* If we could make that set of feeders fit 4 meta constraints (Museum meta, Museum metameta, Wyrm
meta, Wyrm metameta), it'd be really cool.

Making this happen would be quite hard. We needed to make a call on whether this
was ambitious-but-doable, or too ambitious. We decided to go for it, with a backdoor of letting ourselves
exit early if it ended up being impossible.

Although I'm happy we pulled it off, given another chance I would have argued against this.
First of all, I don't think many solvers really noticed the overlapping constraints. It leaned too
hard towards "interesting design problem" without a corresponding "fun" payoff. The more problematic
issue that I only realized in retrospect was what it did to feeder release.
To recap, here is the state of Hunt by this point:

* The Office meta is done and its feeders are released.
* The Basement meta is on a final double-check round of testsolving, but its feeders have already
been released and this is just to verify some minor edits.
* Innovation and Factory Floor is doing its own crazy thing, and won't be ready for a while.
* All AI rounds are in the middle of design, but it's already known that Bootes and Eye will have
answer gimmicks that make their puzzles harder to write, and Conjuri feeders will likely be
released last because the meta relies on developing the game.
* The Museum metameta is roughly done, but the Museum metas have yet to be written.
* The Wyrm metameta is roughly done, but the Wyrm metas still need to be written.

By connecting feeders between Act 1 and Act 3, we were creating a dependency between the
5 Museum metas and 4 Wyrm metas. Including a retest of the 2 metametas after feeders were locked
down, that's **eleven** metas blocking answer release. (Technically, ten metas, since Collage
could be assembled after the other 3 Wyrm metas as long as all three were finalized.)
All of those metas
needed to get to a testsolved state before it would be safe to release any of their feeder answers.
There were 53 feeders in that pool, which is approximately 40% of the feeders in the entire Hunt
and a majority of the non-gimmicked feeders.

I'd estimate that the extra design constraints delayed the release of that pool of 53 feeders by 2-4 weeks, and this
wasn't time we had. Perhaps in a more typical Hunt, this would have been fine, but the AI rounds
had already spent a lot of complexity budget and this may have pushed it over. Instead of reusing
feeders, we could have use silly incomplete puzzles like 2x2 Masyus, and still gotten the benefits
of six fewer AI round puzzles with less work and fewer design complications.

Hunt writing tends to follow a [Zipfian distribution](https://en.wikipedia.org/wiki/Zipf%27s_law).
A small number of people will write many puzzles (or do a lot of leadership work), and a long tail
of people will write a small number of puzzles each. Usually, that long tail is where all the
new puzzle writers are. Put yourself in novice puzzle writer shoes. Do you want to write a puzzle
where whitespace and capitalization matters, a puzzle that must be closely tied to a language, or a normal
puzzle? Unless you're really excited about an idea for the gimmick, you'd probably prefer a normal
puzzle answer. If you didn't get in on the two rounds that had feeders released, then you had to
wait.

This isn't quite as bad as I'm making it out to be.
There's useful work that can be done without knowing the puzzle feeder. In general though, if you
have a feeder from the start, you'll do less redundant work, it's easier to stay motivated if you're
expected to pull through on a puzzle, and you can brainstorm ideas from the puzzle answer rather
than trying to generate one from the ether.

But, this is all said with hindsight. At the time, I did not realize the consequences and I'm
not sure anyone else did either. Adding more complexity to round structures mostly seemed like a teammate thing to try doing, and we told
ourselves that if we found it too hard we'd leave the option of reverting back if we couldn't make
it work.

People say "restrictions breed creativity", but what they don't say is that it's not necessarily
*fun*. The process of finding feeders that fit both Wyrm and Museum metameta constraints took a
long time, but interestingly writing The Legend around those feeders wasn't so bad.


The Legend
-----------------------------------------------------------------------

Before
deciding to share feeders, I had sketched some ideas around using the Sierpinski triangle, after
noticing INCEPTION was $$9 = 3^2$$ letters long. The shape is most commonly associated with
Zelda in pop culture, so ideas naturally flowed that way.

![Prototype Legend triangle](/public/mh-2023/triangle.svg)
{: .centered }

The early prototype was reference heavy and not too satisfying, extracting one letter per
triangle. After talking with Patrick a bit, he proposed turning it into a logic puzzle, by
scaling up to 27 triangles, and cutting out all the intermediate steps. This was especially
appealing because it meant we could take almost any feeders, as long as their total length
was close to around 60. Around this time, Brian mentioned that TRIFORCE was a plausible answer
for both the Museum and Wyrm metameta, so if we could make the Sierpinski idea work,
everything would come together nicely for prepping the infinite loop.

This was all great, with one small problem: I'd never written a logic puzzle in my life.
There are, broadly, two approaches to writing a logic puzzle.

1. Start with an empty grid. Place a small number of given clues, then solve the logic puzzle forward
until you can't make any more deductions. Add the clue you wish you had at that part of the solve,
then solve forward again, until you've filled the entire grid. Then remove everything except the
givens you placed along the way.
2. Implement the rules of the logic puzzle in code, and computer generate a solution.

Option 1 tends to be favored by logic puzzle fans, because by starting from an empty grid you
essentially create the solve path as you go. This makes it easier to create a novel interesting
solve path. The computer approach tends to create what puzzle snobs call "computer generated crap".
(See the many books of 500 random Sudokus that took two minutes to create and feel identical after
the 10th one.) It may tend to be less interesting, but it was also *much* faster, and I knew I
was going to eventually want a solver to verify uniqueness. So I went with option 2.

I started with [grilops](https://github.com/obijywk/grilops), then realized it didn't support
custom grid shapes, so I ended up implementing the solver myself in Z3, a constraint satisfaction
library that grilops uses.
I then needed to figure out how to represent
the Sierpinski triangle grid in code. After some exploration I figured out a very
satisfying coordinate system. Let 0, 1, 2 be the top, left, or right corner of the triangle. Then
a single letter could be described by the path you took at each level of the Sierpinski triangle,
starting from the biggest triangle and working inward.

(explanatory diagram)

With 27 triangles, each letter would be at $$(a, b, c, d)$$, and then to check if two letters
are neighbors, we can also check this recursively.

* If $$a$$ do not match, then it's enough to check the neighbors of the largest triangles.
* If $$a$$ matches, then we can recursively check if $$(b, c, d)$$ are neighbors in the smaller
Sierpinksi triangle.

(diagram 2)

The first draft of my code took 3 hours to generate a puzzle, and the uniqueness check failed
to finish. Still, when I sent it to other authors they found the same solution,
so we sent it to testsolving to get early feedback while I worked on speeding up my solver.

Testsolving went well. The first testsolve took pretty much exactly as long as we wanted it to
(2 hours with 5/6 feeders), and solvers were able to use the Sierpinski structure to logic out
deductions that combined into the final grid. Not too bad for a computer generated
puzzle! This was a case where we "got lucky", and discovered a solve path that felt like
a designed one.

I found a way to describe the letter constraints in Z3 that made the solver run 10x faster
(more precisely, I stopped describing constraints in a dumb way), and played around with the
feeders until I got a solution that verified as unique. There were around 5 different fills,
and I suspect all of them were unique, but I was only able to get my code to halt on one of them
and didn't particularly feel like trying to prove the rest.

After the first draft, there were only two revisions. The first was deciding how much hinting to give
towards the Sierpinski triangle, since that was the step with the largest leap of faith. In the
end we decided to hint that all triangles should stay upright, and the final shape should
be triangular, leaving the rest as an a-ha to find.

The second isn't really a revision, but it's close. Although the penny puzzle from Mystery Hunt 2020
was quite painful, we liked that in end we ended up with a bunch of small souvenirs that people could
take home. Team leadership was not sure if Mystery Hunt would be on-campus or not, but if it was,
it seemed cool to replicate the same experience. Much later, when we got the go for on-campus,
the leads for physical puzzle production started estimating costs for laser cutting wood, and making
prototypes to figure out how long it'd take to make enough sets. The actual laser cutting happened
in December. It would have happened earlier, except the person who ordered the wood had their
shipment of wood stolen from the mailroom and had to order a replacement. (Turns out stealing wood is...a thing
people will do? The more you learn.)

I'm not sure if teams use the wooden triangles as a souvenir in the way we imagined, but I hope shuffling
wooden triangles was more fun than manipulating spreadsheets!


May 2022 - The Triangles Will Continue Until Morale Improves
--------------------------------------------------------------------------

Museum and AI round development is fully underway. The EICs have created a "bigram marketplace" puzzle,
which is really just a placeholder puzzle to make it easy to create a Discord channel that's only open
to certain authors. The bigram marketplace held every bigram used by the Museum metameta (MATE's META),
along with a sketch of expected bigram extraction mechanisms. The repeated feeders in Wyrm were given
top priority on claiming bigrams, and as Museum metas were proposed, the EICs compared their constraint
levels to decide which got first rights to bigrams, and which needed to absorb the repeated feeders.
I field questions from Museum meta authors. (Yes, someone needs to take the answer GEOMETRIC SNOW, we
know it's not a great answer but haven't found an alternative that fits the double O constraint.
Sorry, TRIFORCE can't change. Oh, this feeder isn't as tightly constrained, here's five options pick one.)

As part of this process, other Wyrm authors and I were fasttracked into first testsolves of
every Museum meta draft, so that we could learn their feeder constraints without wasting our lack of knowledge.
Meanwhile, testsolves of The Legend were biased towards Museum meta authors so that they could understand
what we were doing. (The same was not done for the Wyrm metameta, because it already had two clean tests
and we didn't want to do extra work.)

As the bigrams settle down and The Legend feeders get more locked in, we start looking at how to
make our own constraints work...

The Scheme
--------------------------------------------------------------------------

This was the 3rd Wyrm meta to get drafted. Lost at Sea had been tested, and although revisions were
planned, EYE OF PROVIDENCE was locked in as the target answer for The Scheme.

The original idea for The Scheme was based on an idea that I quite liked. We weren't able to make it
work, partly because we didn't have the right skills in the construction group, but it's good enough
that I don't want to reveal it.

After that idea fell apart, we noticed that the Eye of Providence was a triangle, and we had triangles
in The Legend, so why don't we try to make this triangle Wyrm meme a reality? It wasn't a hard
requirement, but it would be cool...

When researching Mystery Hunt 2018 to get ideas for constrained metas, I found
[Voltaik Bio-Electric Cell](http://puzzles.mit.edu/2018/full/puzzle/voltaik_bio_electric_cell.html),
a triangular meta that mostly used feeder length, with the rest coming from shell. Well, we already
had a 1 letter word in one of our feeders (at the time, it was V FOR VENDETTA). It seemed plausible
we could make a full triangle out of words in our feeders, and a length constraint was light enough to not
put too much pressure on the semantic categories from the metameta.

We did a search for missing feeder lengths, made a version that picked letters out of the triangle with
indices, and sent it for editor review. I was fully expecting it to get rejected, but to my surprise
the editors for this puzzle thought the word triangle was pretty elegant, and only had token comments
on the puzzle draft. (Perhaps the more accurate statement is that the editors knew what the Wyrm constraints
were, most metas we'd proposed required a hefty shell, and this was one of the purer meta proposal we'd
managed for Wyrm so far.)

The spiral index order was originally added because we were concerned a team could cheese the puzzle by
taking the indices of all 6! orderings of the feeders. Doing so wouldn't give the answer, but it seemed possible
you'd get out some readable partials that could be cobbled together.
With hindsight, I don't think that was actually possible, but one of our testsolve
groups did write code to bash all 6! orderings, so it was definitely worth considering.
We kept the spiral in the final version because it let the arrow diagram serve two purposes: the ordering of
the numbers, and a hint towards the shape to create.

DIAGRAM

In Puzzup, there were a list of tags we could assign to puzzles, to help editors gauge the puzzle balance
across the Hunt. This puzzle got tagged as "Australian", which is shorthand for "a minimalistic puzzle with one key idea,
where before you have that idea nothing makes sense, and after that idea you're essentially done." Our team
uses that shorthand because puzzles like this tended to appear in the Australian puzzlehunts that used to
show up every year (CiSRA / SUMS / MUMS). One of the tricky parts of such puzzles is that you get exceptionally
few knobs to tweak difficulty. Basically all we got was the diagram and the flavortext.
The other tricky part is that solve time can have incredible variance. The first test got the
key idea in 20 minutes. The second test got horribly stuck and was given four different flavortext and diagram
revisions before finding the idea 3.5 hours later.
The second group did mention the meta answer an hour before the solve, in one of the
best examples of dramatic irony I've ever seen.

(add the Discord screenshot here)

Maybe having an Australian puzzle as a bottlenecking meta was a bad idea. A high variance puzzle naturally means
some teams *will* get it and some teams *will* get walled, and getting walled on a bottleneck is a sad time. This is
on my shortlist of "puzzles I'd redo from scratch with hindsight", but at the time we decided to ship it so we could move on to feeder release. ["You get one AREPO per puzzle"](https://docs.google.com/presentation/d/166MkkDmij_4_XcA8JP8JcSztVPjHJeibDJxr9EUMEv4/edit#slide=id.p) - I'd say this is the one AREPO of the Wyrm metas.

Even in batch testing, where teams solved The Scheme right after The Legend, none of our testsolves consider
using the arrangement from The Legend when solving this puzzle. A few teams got caught on this during the Hunt -
sorry about that! The fact that The Legend triangle had an outer perimeter of 45 was a complete coincidence,
and if we'd discovered that early enough it would have been easy to swap BRITAIN / SEA OF DECAY back to GREAT BRITAIN / SEA OF CORRUPTION to make the triangle 55 letters rather than 45.


Lost at Sea
-----------------------------------------------------------------------------

Nominally, I'm an author on this puzzle. In practice I did not do very much. The first version provided the
cycle of ships directly, and tested okay (albeit with some grumbling about indexing with digits).
By the time I joined, the work left to do was finding a way to fit
in triangles, and finding answers suitable for the metameta.

I looked a lot into the Bermuda triangle, but didn't find any reasonable puzzle fodder. So instead, we looked
into triangular grids. Over a few rounds of iteration, the puzzle evolved into seeding a triangular
[Yajilin](https://en.wikipedia.org/wiki/Yajilin) puzzle where feeders were written into the grid, black triangles
gave a cluephrase hinting towards digit indexing, and ship information from the feeders gave direction for
the Yajilin arrows.

It all sorta worked, but the design was getting unwieldly and we had a really hard time finding a way to
clue all the mechanics properly in the flavortext. There were lots of facts about each ship, so the longer
we made our flavortext, the more rabbit holes testsolvers considered. (It's the natural response: if you
get more data, then maybe you need to research more data to solve.)
I'd say my main contribution was suggesting we try cutting the Yajilin entirely and brainstorm something else.
The final version of the puzzle is the result of that brainstorm. I'm pretty happy with the way the puzzle
guides towards the hull classification, self-confirms it with the USS Midas (ARB-5) thanks to A and B flags
being weird, and then leaves the classification number suspiciously unused if you haven't figured out they're
important.

The other main contribution was helping find feeders. The first draft used MRS UNDERWOOD as a feeder, which
*worked* but was really not a good answer. The S in MRS was needed for extract, but it looked so much like
a cluephrase for Claire Underwood. Brian told me that it could be fixed if we added a feeder that clued
San Francisco, which fit the metameta, and had an S as its 5th letter. We collectively spent 5-10 hours
doing a search for one, before landing on the "stories" idea with SALESFORCE TOWER or
TRANSAMERICA PYRAMID. The puzzle got rewritten around SALESFORCE TOWER, went through an entire testsolve
with zero issues,
and then we learned that [actually there are multiple Salesforce Towers](https://www.salesforce.com/company/ohana-floors/salesforce-tower-atlanta/) in a final runthrough of the metameta. Oops. Thank goodness we had
a backup answer!


June 2022 - It's Collaging Time
-----------------------------------------------------------------------------------

Wyrm feeders are almost ready for release. We've made the feeder constraints work, and every meta has gotten two clean
testsolves. There are just two action items left:

1. Write the 4th Wyrm meta.
2. Do a batch solve of the entire round to get data on what it's like to solve each meta sequentially.

We had multiple meetings about the 4th Wyrm meta starting all the way back in March.
The design requirements were pretty tight:

* The puzzle had to look like an regular Act 1 puzzle.
* It also needed to be interpretable as a metapuzzle.
* Feeders had to be used in enough of way to allow for backsolving.
* At the same time, the puzzle had to be solvable without knowing any of those feeders.

The very first example for what this could look like was a [printer's devilry](https://en.wikipedia.org/wiki/Printer%27s_Devilry) style puzzle. Instead of each clue solving to the inserted word, inserting a word would complete a crossword clue with its own answer that you'd index from. The inserted words would then be the backsolved feeders.

None of the authors were too excited by this, but it was important to prove to
ourselves the design problem was solvable.
we came up with it in March mostly to convince ourselves
the design problem was solvable). Once we knew the answer was TRIFORCE in April, the idea died more officially
because we could not longer do a 1 feeder -> 1 letter idea. Another idea proposed for this puzzle
placed too many constraints on the feeders given the Wyrm metameta - that idea eventually turned into
[Word Press](https://puzzlefactory.place/factory-floor/word-press).

One of the challenges was that the puzzle needed to embed some process for backsolving. But, that backsolving
process might look like unused information during the initial forward solve, which could turn into
a rabbit hole if teams got stuck. The round structure only really *worked* if almost all teams solved
the Act 1 puzzle by the time they got to the end of Wyrm.

In late-April, we came up with the word web idea, which I immediately started advocating for. Word webs
are the closest thing to guaranteed fun for puzzlehunts, and were a highlight of the now-defunct Google
Games hunts run for university students.
It solved all our backsolve problems because spamming guesses in a word web is just what you're supposed
to do, and there was a natural way to solve around unknown nodes and guess them later. My main worry was
that it'd take a while to construct.

It was clear we *could* construct it though. We punted on doing so until the backsolve feeders were
more certain. Well, now they were. Time to reap what we'd sown.


Collage
-------------------------------------------------------------

The first thing I did was reach out to David and Ivan to get the code they used to build [Word Wide Web](https://2020.teammatehunt.com/puzzles/word-wide-web). David ended up sending me a D3.js based HTML page that
automatically layed out a given graph, with some drag-drop functionality to adjust node positions.
I repurposed that code to create a proof-of-concept interactive version that ran locally. This quickly
exposed some important UX things we'd want to support, like showing past guesses and allowing for some
alternate spellings of the answer.

![Prototype of filled out web](/public/mh-2023/webprototype1.png)
{: .centered }

![Prototype of solvable web](/public/mh-2023/webprototype2.png)
{: .centered }

For unlocks, I decided to always recompute the web state whenever the list of solved words changed.
This wasn't the most efficient, it could have been done incrementally, but in general I believe
people underestimate how fast computers can be. Programmers will see something that looks like
a software interview question, and get nerd sniped into solving that algorithms problem, while neglecting
to fix their page loading a 7 MB image file. (And, the simpler the data, the easier it is to
*recognize* it's an algorithms question, and the less likely it is to matter.)
I figured recomputing the entire graph would be more robust,
and didn't want to deal with errors caused by incremental updates failing.

![Advice on building the most robust thing](/public/mh-2023/braid.png)
{: .centered }

(From ["The Implementation of Rewind in Braid"](https://www.youtube.com/watch?v=8dinUbg2h70), a talk by Johnathan Blow)
{: .centered }

From here, I worked on creating a pipeline that could convert Google Sheets into the web layout code. The
goal was to remove tech from the flow of updating the word web, to make it easier to collaborate on.
I added a bunch of deduplication and data filtering in my code to allow the source-of-truth spreadsheet
to be messy, which paid off. Pretty sure around 10% of the edges appear twice in the raw data.

![Web spreadsheet](/public/mh-2023/webspreadsheet.png)
{: .centered }

The process of coming up with the words themselves took a while. Collage is the first time I've
ever done this kind of thing, but my assumption going in was that word webs are best when there's a
high density of edges and vertices have large average degree. To get a sense for how big the web needed
to be, I took the [Black Widow](https://puzzlepotluck.com/3/14) web from Puzzle Potluck 3 and collected
some stats on number of vertices and edges to figure out the average degree I wanted to hit. Whenever I loaded
a web into my code, I printed out stats on the graph, as well as how many leaf nodes there were, with
the goal of having as few as possible. I gave very serious though to dusting off my [spectral graph
theory](https://en.wikipedia.org/wiki/Spectral_graph_theory) to estimate if the web was an
[expander graph](https://en.wikipedia.org/wiki/Expander_graph), but decided that was overkill.
There was an initial "expansion" phase, where I put down literally anything I could think of starting
from the backsolve feeders, and asked other authors to do the same. Then, after the web grew enough to
have some collisions occur naturally, there was a "contraction" phase where I trimmed nodes that were
hard to connect to the rest of the web, and tried to more directly brainstorm topics that could be embedded
in the graph without adding too many new words. I'd estimate about 80% of the web is from me, and that's why
there's so much My Little Pony and MLP-adjacent material in the web. I apologize for nothing.

After the prototype got tested a bit, it was time to get it into the site for real. I'd written
interactive puzzles before, but this was my first time implementing websockets for a puzzle, as well as
using D3.js within React. (Websockets are what allow the puzzle to display updates when another solver
on the team guesses a word. They're also how all the AI chat messages are managed.) I knew that I
wanted to use websockets to sync team state on Quandle, and I wanted the zooming code in D3.js for
the Wyrm round art, so I treated it as an investment that would save time later.

Thanks to us using a similar codebase as previous Teammate Hunts, it was straightforward for me to find
previous websocket code in old teammate puzzles, and then implement something similar. Very broadly,
the backend uses [Django Channels](https://channels.readthedocs.io/en/stable/) as a websocket
management layer, handling team authentication and routing different websocket URLs to different puzzle endpoints. The frontend then defines helper functions to support connecting to either a user-specific channel,
team-wide channel, or hunt-wide channel (although I don't think we ever used the hunt-wide channel).
The main danger of using websockets is that they widen the set of things-that-can-go-wrong in the Hunt
site, but given that we were already relying on chats with MATE as a key part of Hunt, we were going to
eat that risk regardless.

Polishing and factchecking this puzzle was a bit of a nightmare. It turns out graph layout is a really hard
problem, and even after tuning D3.js force graph parameters a bunch, I needed to do several adjustments
by hand to clean up collinear points and reduce overlaps. The click-to-highlight feature was a concession
to the fact that getting to 0 overlaps was impossible in the given time. As for factchecking, the graph has over 300 words
and I knew there was no way we were going to exhaustively verify all O(N^2) pairs of those words.
I think we got most of them, but a few slipped through ("the princess bride" is somehow not connected to "bride").

I was happy to see
that no one who testsolved Collage suspected a thing. Even during Hunt, most teams successfully got past
the zero starting word hurdle without thinking too much of it. Thanks to Patrick for suggesting a hardcoded
threshold of 90%, rather than revealing the goal node when its 3 neighbors were found, since the neighbors were usually revealed within the first 15% of the solve.

\*\*\*

With all the Wyrm metas at first draft, we did a final batch test of all the Wyrm metas and the metameta.
We debated back and forth on whether there was a way to have teams testsolve Collage when it was advertised
as a "test of all the Wyrm metas". If we didn't test it before feeder release, and something went wrong
in the true test, then we'd have a huge problem. But the only test of it that seemed like it'd
accurately model the real Hunt would be having teams do testsolves of many random puzzles including
Collage before they tested Wyrm metas. Those random puzzles didn't exist...because they were waiting
on feeder release, which was waiting on testing the metas, which really wanted the random puzzles to
exist, which...you get the idea.

Who knew making an ouroboros-style round would make it hard to find a starting point?

There were arguments both ways, and the decision was to not test the gimmick. We did not have distractor
puzzles to hide Collage, we didn't want to wait for real puzzles to get written to have distractors,
and we certainly didn't want to write fake puzzles when we could spend the time writing real puzzles.
The batch test group tested the Wyrm metas in sequence, starting with 4 feeders each and occasionally getting more
if needed, and when they got to the metameta, we revealed the round gimmick and gave them the full set of
answers.

![Wyrm structure](/public/mh-2023/wyrmstructure.png)
{: .centered }

We did redact the title of Collage to something else, because there was no reason to reveal the name
of the looping puzzle. The meta names used to follow a pattern of "The Legend", "The Scheme",
"The Sea", and "The Collage" (redacted to "The Sage"). We ended up dropping the "The" pattern since we
were concerned it might be too much hinting. If a team got suspicious of looping, and identified the
looping puzzle before unlocking the blank puzzles, then they could solve the 3rd Wyrm meta immediately on
unlock, which seemed like far too much cheesing. But we kept the pattern in mind as a potential nerf to make down
the line.

The metameta test passed, and we were able to release all the Wyrm feeder answers. The Museum metas
were finished shortly, letting us release a bunch of feeders before the in-person writing retreat. The
retreat got scheduled for late-August. The goal was to get together, test drafts of physical
puzzles and events, and start puzzle drafts for AI round puzzles that we expected to be hard to
write solo. We were also planning to do a team solve of a good chunk of Act I, so it was pretty
important to get Wyrm metas and Museum metas out the door early enough to give time to construct
those Museum puzzles.

However, the true goal of the in-person retreat was a more closely-held secret. And I was in the thick
of that conspiracy.


July 2022 - It's Breakout O'Clock
-----------------------------------------------------------------------------------------------

"Breakout" was our internal name for what would eventually become the Loading Puzzle. It was the
moment when teams would discover the Puzzle Factory. It was the goal we wanted all teams to see,
and the key introduction to the entire story that would unfold.

And 80% of the team was spoiled on it by February.

This is always a risk you take with structure-level gimmicks. The transition to discovering the
Puzzle Factory was a key part of the theme proposals, which were all heavily discussed. How do you
testsolve something that heavily discussed?

The traditional answer is external testsolves. We tested the Teammate Hunt 2021 gimmick on a testsolve
team from Galactic, and Galactic tested the gimmick of Galactic Puzzle Hunt 2022 on some people from
teammate. This is a lot harder to do for Mystery Hunt though.

For Hunt, teammate did a lot of recruiting for puzzle writers. This was
mostly because we were concerned about the workload we had in front of us. To get a sense of scale,
we won hunt with about 60 people, not all of them decided to write, and we ended the writing process
with around 70 people on the credits page. Approximately 30% of people joining partway through the year
sounds right.

Any recruited person that joined after theme discussion was not spoiled on breakout. The true goal
of the in-person retreat was to do a testsolve of Act 1 puzzles, with a testsolve of breakout running in
the background for everyone who wasn't spoiled on its existence. After the team broke out, we'd stop
the Act 1 testsolve and reveal the story of the Hunt to those unspoiled on the theme.

To do that however, the breakout would need to exist by retreat.


Loading Puzzle...
-----------------------------------------------------------------------------------------------

During one of the weekly general meetings, I was asked if I could join a brainstorm group for "a small puzzle".
Said brainstorm group had Jacqui (creative lead) and Ivan (tech lead).

Having been in the trenches with Jacqui and Ivan on many an occasion for tech-heavy puzzlehunt
story integrations,
I was, like, 80% sure this "small puzzle" was breakout, and 100% sure it would be small to solve
but take a stupidly long time to make. I want it for the record that I was right on both counts. I joined
an in-progress brainstorm in June and we weren't done until August.

Over the course of a few months, we and other authors of the breakout puzzle brainstormed how to
create a puzzle that looked innocent at first glance, and became more suspicious the more you looked at it.
A common theme was "peeling back the facade". Those of you readers who have every played an RPG should
know the feeling of talking to NPCs until you see all the dialogue, or trying to run over a waist-high
fence and running into an invisible wall. We wanted something in that vein, that rewarded exploring
the contours of the world, and surprising you when the contours broke instead of holding firm.

Many of the early ideas from this would get repurposed for the final clickaround of the hunt,
MATE's TEAM. But for breakout, we converged towards something like a loading animation. The loading
animation would represent MATE getting increasingly overworked trying to create puzzles, diegetically
getting longer as teams solved puzzles and forced MATE to write new ones, and non-diegetically
scaling up with solve progress as desired for hunt structure. It would be easy to have this loading
animation appear on every puzzle, and that would increase the odds teams would notice what was
going on.

As for exactly how that would all work? Boy was there a lot to figure out.

![Breakout brainstorm](/public/mh-2023/breakoutplanning.png)
{: .centered }

There was a fairly serious proposal that the "hole" between the Museum and Factory would only exist on the puzzle page where
you first solved the loading animation. You'd be able to visit the URL directly once you knew it, but
for within-site navigation, you'd need to memorize which puzzle you first found the factory from.
I liked this a lot, but we dropped it due to complexity concerns.

Previous Teammate Hunts have always had a rule that solvers can look at source code if they want,
because we trust in our ability to hide things from the client. For the loading animation, it meant
the animation was drawn using CSS rather than a video file, since we did not want solvers to
right-click -> "Download Video" and solve the animation offline. The first idea proposed was to
create a conveyor belt of puzzle pieces, and entering STOP would cause the conveyor belt to
break down. (Technically, the answer at the time was TERMINATE, due to its similarity to
TEAMMATE.)

![Early breakout version](/public/mh-2023/breakout_old.png)
{: .centered }

This was later changed to the version that went in the Hunt, since having letters on the puzzle pieces
was deemed too obvious. At the same time, we didn't want it to be too *non*-obvious.

VIDEO HERE

Our testing process for this was pretty goofy. We would take groups of people spoiled on the
existence of breakout, and tell them that we wanted to test the unlock structure and backend of the
hunt website, using fake puzzles that would ask you to wait for 60 seconds, then immediately give
the answer.

![Fake puzzles](/public/mh-2023/fakepuzzles.png)
{: .centered }

This testsolved correctly, but we knew the real test would be at the retreat, when we had real puzzles.

Meanwhile, we were working on the collaborative jigsaw that appeared after solving the loading
puzzle. There are a few principles that guide teammate's approach to storytelling. I don't want to
put words in the mouth of the story team, which I've never been on, but in my opinion the key points are:

* Decide on the story you want to tell, then make sure as much of the Hunt as possible acts
consistently with that story. This will take a long time to polish - do so anyways.
* Tie changes in narrative or story state to actions the solvers take. These actions are usually
solving puzzles, but they don't have to be.
* Use bottlenecks to direct team attention towards the same point, then put the most
important story revelations at those bottlenecks.

The breakout puzzle acts the bottleneck into the Puzzle Factory, but the main thing we wanted to
avoid was one rogue team member solving the puzzle, finding the Factory, and leaving the rest of
the team in confusion about what was going on. Our solution was to have the second part of the
puzzle be a "teamwork time" puzzle (borrowing a term from MIT Mystery Hunt 2020), where collaborating
with others would make it go faster. We then would require each user to enter STOP on the loading
animation individually before they could access the jigsaw puzzle's URL, to force a flow where
early solvers would need to tell others how to break out of the Museum if they wanted to *have*
collaborators.

The collaborative jigsaw itself married three of the worst parts of frontend development:
handling different screen sizes, live updates of other people's actions, and non-rectangular
click regions. Many thanks to Ivan for figuring out the design of those details.
Internally,
what happens is that every user fires a cursor location update in Websockets every 150ms,
and the team-wide cursor state is broadcasted back to all viewers, using [a spring animation](https://liveblocks.io/blog/how-to-animate-multiplayer-cursors)
to make cursor movement smoother by respecting momentum. The click regions are then based on SVG
regions with some custom hooks on mouse-enter and mouse-leave to track which puzzle piece the
user is working with. I mostly played QA testing on checking
it worked properly on Firefox and Safari,
and making sure puzzle pieces couldn't get stranded by phantom cursors. (Fun fact:
did you know that browsers can sometimes fire a 2nd mouse-enter while you're in the middle of entering
an element? It's true! It happens entirely randomly, and when it does it fires a mouse-leave while you're still
entering said element. I'm sure this won't cause a bug that take 5 hours to root-cause.)

At one point, we planned to implement an adversarial UI, where pieces would move on their own, wiggle out
of their position, etc. The story justification would be that MATE was trying to stop you from getting
into the Factory. We ended up cutting this because it was more work, and also in internal testing
we got trolled enough by our own team members that we decided we didn't need to make solving the jigsaw
any harder.

![Conversation during Discord testing](/public/mh-2023/breakout_discord.png)
{: .centered }

In the live Hunt, a few teams somehow managed to drag a puzzle piece offscreen, and we had to manually
advance them past the jigsaw puzzle. I still don't understand exactly what happened there,
we never saw it in testing after adding bounds on the drag region.

\* \* \*
{: .centered }

The other thing we needed for retreat was enough Act I feeders to fill out the start of the Museum
round. As part of prep, we started really ramping up puzzle production, along with a "hack weekend"
where we'd try to draft a puzzle within a weekend. That weekend was where we ended up writing all of:

Museum Rules
---------------------------------------------------------------------------------------------

There isn't really too much of a story with this one. The brainstorm group I was was randomly
assigned a feeder answer, and we tried to brainstorm ideas based on that. That landed on doing
something with a list of rules placed in the reception of the Museum in-story. After the US
states idea, the idea of extracting from Supreme Court seals felt like a more suitable way
of using the puzzle information, and the puzzle presentation was stolen directly from
[Storytime](https://2020.teammatehunt.com/puzzles/storytime) in Teammate Hunt 2020.

The puzzle was written in about 2 days - I worked on creating the seal overlay extraction,
Catherine did the drawings, and Harrison did the factchecking + research of weird laws. I'd
say this puzzle was the strongest stretch of deciding whether to use true laws, or easy to
search laws. Some laws were deliberately misinterpreted for humor, like "no marathon dancing",
but other ones like "killing a fly next to a church" were not supported by any part of the
Ohioan legal code. Meanwhile, I know that
[New York has a specific child labor law exception for working as a bridge caddie in a bridge tournament](https://law.onecle.com/new-york/labor/LAB0130_130.html). Is anyone else going to be able to research that?
Maybe, but better not to risk it. Around 30% of the laws used in the puzzle are not actually real weird laws.

Really my main regret is that we didn't figure out a way to cleanly push to Supreme Court seals
rather than state seals. The ambiguity came up in testing, but we didn't get around to a fix making
it more obvious.


Interpretive Art
-------------------------------------------------------------------------------------------

As mentioned in this solution, this puzzle started from a shitpost someone posted on Facebook.

Me: "haha. Wait this isn't a bad puzzle idea."

One of the common tools a puzzle constructor reaches for is "does this have a small, canonical
dataset"? Such things are easier to search and usually give a pre-built Eureka moment when
solvers discover the canonical dataset. Accordingly, the first idea on this puzzle was to
use only songs from the Guardians of the Galaxy soundtrack. It was a good intersection of
pop music and alien interpretations of that music.

After I realized the GotG soundtrack did not have Never Gonna Give You Up or All Star, I decided
that was a bad idea, and we switched to "literally any popular song". If you cannot meme in
a puzzle, what are you even doing? My proudest moment of the puzzle was in the first testsolve,
where testsolvers solved NA NA NA NA NA NA NA NA NA NA NA HEY JUDE, and placed it while saying
"what an absolute shitpost".

For a puzzle written so quickly, this puzzle got insanely high fun ratings in testing and seems
popular with Hunt participants as well. Between this and [Young Artists](https://2021.galacticpuzzlehunt.com/puzzle/young-artists) from GPH 2022, I'm guessing that any puzzle that riffs on pop music is
going to be a fan favorite.


Conglomerate
-----------------------------------------------------------------------------------------

On this puzzle, I did less work. The story is that chuttiekang came up with the overall
skeleton of the puzzle, but wanted help brainstorming and writing minipuzzles. So
Nishant and I got recruited to do so. I ended up writing Birdhouse Kit, Camera, Microphone,
Quilt, and Telescope. I also helped on a revision of Fishing Rod.
Some of you may remember me saying that I'd try to do fewer minipuzzles for Mystery Hunt,
but if someone else is designing their inclusion, then sure I'll help out.
The sheer quantity did make construction a bit difficult, since we wanted to avoid
repeating encodings or extraction mechanisms. That left fewer options after we exhausted
binary, semaphore, Braille - you know, the old standbys.

This puzzle
had more errata than I'd like, and that ties into part of the reason I've been trying to avoid
minipuzzles. The amount of content you need to create is a lot larger, and your exposure
to missing something in factchecking is larger as well.


August 2022 - Retreat (but Also Other Puzzles)
------------------------------------------------------------------------------------------

As the date for retreat approached, most of my work was on fixing parts of the Loading
Puzzle, which was getting updated up to the day of retreat. However, puzzle writing is
a constant process during the year, so August was also the time of writing:


Lost to Time
----------------------------------------------------------------------------------------

Whenever I wrote a puzzle, I did a search in the /dev/joe index to double check it's not
too close to a prior Mystery Hunt puzzle. At one point, I opened a link to the 1995 Mystery
Hunt, and got really confused why the solution page redirected to devjoe's domain. I mentioned it
in #puzzle-ideas and kept going.

About a week later, Bryan said it was turning into a real puzzle and I should help out since
I'd already be too spoiled to test. The plan was to pretend we found all the missing parts of the
1995 Mystery Hunt in the Puzzle Factory basement, and we'd turn them into a puzzle answer. Given
how niche the idea was, it seemed like a shame to leave any stone unturned, so we aimed to use
as much of the missing content as possible. That meant writing 8 minipuzzles, based on the
constraints given by devjoe's writeup and the 1995 Hunt document. We landed on the idea of writing
puzzles with two answers, one for the 1995 Hunt and one for the 2023 Hunt, where knowing the
1995 answer is helpful for solving the 2023 answer, then having them feed into a meta based
on the 1995 Hunt meta.

This definitely contributed to the length of the final puzzle. We knew it was reallllly pushing
it for a puzzle in the FActory, but decided it could be okay if it was a one-off exception. As a
microcosm of the entire Hunt, it made sense to have as many minipuzzles as we did, but each minipuzzle
needed to be easier. (Not shorter, easier. We were constrained in length by the original 1995
Hunt.) I know an old Dan Katz post speculated whether an entire early Mystery Hunt could act
as one puzzle in a modern Hunt. I think this puzzle shows that no, it can't, but it's kind of close.

The minipuzzles I directly made were the conundrum and the screenshot of Rick Astley, but in general
I helped out on the presentation or brainstorming of other minipuzzles as well.
In the vein of [Cruelty Squad](https://store.steampowered.com/app/1388770/Cruelty_Squad/), we
spent a lot of time trying to make everything look as bad as possible. Well, "dated" would
be the more accurate term. Every image in the puzzle was saved as a JPEG with lower resolution
and image compression dialed up to the maximum. The conundrum was printed out on a piece of paper,
which I cut by hand and then scanned. The scanner I used was too good though, so we had to
JPEG-ify it a bit. Last, we planned to run the video through a filter to add VHS lines, but this
got dropped because none of the premade filters we found matched our desired aesthetic, and
making a custom one was definitely not worth it.
Shoutouts to
"The Boston Paper". I briefly considered buying a [newspapers.com](https://www.newspapers.com/)
subscription to track down the original newspaper used for the 1995 Hunt, and then realized that
it would be much better if the puzzle did not suggest looking up historical newspapers. That
being said, I bet you could find what the original puzzle looked like if you dug hard enough.

I'm a bit disappointed this wasn't solved more, but I understand given its length and placement
in the Hunt. It's also interesting to write a puzzle that will potentially be broken if
anyone finds more material about the 1995 Mystery Hunt. I'm usually rooting for better archiving.
This is a rare example where I hope it stays in a steady state!

\* \* \*
{: .centered }

I also did an internal test on a paper version of Weaver around this time. I failed to understand
the final cluephrase even with the correct extraction, so it got revised. I also remember commenting
that it might be 1 weave too long, but we didn't plan any changes before retreat. The main goal was
to get it validated enough to create the physical prototype.

This was also around the time where we were more seriously working on Hunt tech. (Move sections from
above to down here).

One interesting thing about tech for Mystery Hunt in particular is that there's greater emphasis on
creating internal tooling. In a smaller Hunt, it's possible for a small number of tech-inclined
people to get all the puzzles into the Hunt website. In Mystery Hunt, this is less true, just because
of the scale. We knew the end of Hunt writing would be busy with art and puzzle postproduction, so
any tools we wanted were best planned and coded early, before art and puzzle drafts were done.

Top of our list was improving our postproduction flow, given Palindrome's comments that they
had a postproduction crunch. We also wanted a better puzzle icon placement system.
Icon placement is a field on the Puzzle object in the database, which lets artists change its
location without needing to write a commit, but the flow of adjusting a random number was kinda
painful in Teammate Hunt 2021.

For postproduction, Ivan proposed an "auto-postprod" system. Given a Google Doc link, this tool
would load the page, retrieve the HTML within the Google Doc, and auto-create a commit and pull
request for that puzzle draft. I said "sure", not really expecting it to ever exist. Imagine my
surprise when it actually got built! It wasn't perfect. It didn't handle Google Sheets, it
would sometimes timeout if a Google Doc had an especially large image, there was an issue with
HTTP link escaping at one point, and in general if it errored on a puzzle it would have fits
when trying to auto-create the commit for the postprodded puzzle. It was still quite helpful.
You don't realize how long it takes to mechanically copy-paste paragraphs from Google Docs
and wrap them in `<p>` tags until you've done it thousands of times.


Retreat!!!
-------------------------------------------------------------------------------------

Retreat was held in late August, in a big AirBnB in the Bay Area. Going in, there were four
public priorities:

* Testsolve physical puzzles
* Test events
* Draft a new event
* Start drafting 2+ AI round puzzles

along with the secret priority of

* Testsolve breakout

Friday
----------------------------------------------------------------------------------

![Retreat schedule, Friday + Saturday](/public/retreat_friday.png)
{: .centered }

As an icebreaker, we went around saying what our favorite puzzle was. One person said, "134".
Later I'd learn this was the puzzle ID for the Hall of Innovation meta, which was in the
middle of taking over the lives of its constructors.

Retreat started with a group testsolve of all the Act 1 puzzles and physical
puzzles. We aimed to frontload all the physical puzzles, based on feedback that
[Diced Turkey Hash](https://puzzles.mit.edu/2022/puzzle/diced-turkey-hash/) showed up late
last year, so most physical puzzles were in Act 1 anyways.

Almost immediately, we started hitting technical problems. A few were real software bugs,
but most of them were tied to trying to have ~20 laptops and ~20 phones all connect to
the same WiFi, and this...not working that reliably. I switched to using a WiFi hotspot
from my phone. The other issue was that we provisioned a smaller server for testing,
and it turned out that server was fine for 3-5 people and less fine for 20.

So, we also immediately got evidence that if your site is slow, then people assume the
loading animation is just a real problem with the site. We also saw that as load times
increased, teammates from teammate would get in the habit of opening every puzzle at once in a
separate tab, then looking elsewhere while waiting for the puzzle to load. Which also
wasn't great. The loading puzzle was not getting solved, teams were not breaking out.

In an attempt to get people to look at the loading animation more, the editors-in-chief
nudged testsolvers towards puzzles that unlocked later (had longer load times). Meanwhile,
on the tech side we did a number of deploys to "fix a bug", that was *actually* just
increasing the time of the loading animation and encouraging solvers to refresh their
pages to get the "bug fix" (see the loading animation for longer). Testsolvers were progressing
through Act I slower than expected, so the load time curve needed to get adjusted.

(I can see some of you yelling about the "slower than expected" part, and in hindsight,
yes that should have been considered more seriously. But we had already marked retreat as
our one good testsolve of breakout, and it was *not working*. We were of the mindset that
we'd lock in all details of breakout by today, and if breakout didn't work then the
entire Hunt's design was in jeopardy. I don't even think that's wrong.
In comparison, many of the Act I puzzles were on their 1st draft and would likely get easier as they got polished.
Again, I think "puzzles get easier as clues are cleaned up" is also totally true!
The error may have been in the estimate for how *much* the solve time would drop in
future puzzle revisions.)

In between trying to ramp up loading times, I testsolved many puzzles, including
[Exhibit of Colors](https://interestingthings.museum/puzzles/exhibit-of-colors) (solved the
flower puzzle and got the animal a-ha), [Dropypasta](https://interestingthings.museum/puzzles/dropypasta)
(complained heavily about the old Pokemon Stadium mechanic - it got changed to what our testsolve
group thought it would be), and [Brain Freeze](https://interestingthings.museum/puzzles/brain-freeze). At
the end of Brain Freeze, someone asked why it had such a garbage answer, and I had to stay silent
knowing I was responsible for pushing that garbage answer into the Hunt.

(meme image here?)

After that, Collage got unlocked! Except, it was totally broken. Oh no. One teammate meme
during writing was "bamboozle insurance". You offer bamboozle insurance when you want to assure
people something will be true, and you pay bamboozle bucks when you are wrong. The nature of writing
puzzlehunts is that you'll be wrong quite often. Example uses include:

"This meta testsolve needs 2+ hours but it will be worth your time, you can buy bamboozle insurance from me"

"Can I get bamboozle insurance the loading animation time won't change, I'll have to redo math if it does"

DALL-E and other image generation models were quite hot at the time and we used them accordingly. Although
a surprisingly large fraction of teammate works in machine learning or related fields, the choice to
have an AI-themed puzzlehunt in the year generative models became commercially viable was entirely
coincidental.

![Bamboozle emoji generations](/public/mh-2023/bamboozle.png)
{: .centered }

I bring this all up to say that I offered bamboozle insurance that Collage would be fine,
since we literally testsolved it a few weeks ago, and then it wasn't.
It turned out the issue was that the loading animation was conflicting with the startup of
the interactive puzzle content, replacing it entirely rather than making it hidden, and this
caused the async initialization requests for Collage to never fire. We spun up a hotfix
to get Collage working, and got it going enough to run a testsolve, but this fed
into the narrative of "the site is struggling, so the loading animation isn't special".

The original plan was to get to breakout by around the evening (8 PM to 10 PM), then leave time for
board games and socializing. Instead, at 1:40 AM, the Act I testsolve was officially put on pause,
and then a bunch of hunt exec and breakout authors shuffled into a meeting room, where we
discussed what nerfs we wanted to add before the testsolve resumed Saturday morning.

The additions we landed on were:

* Be much more aggressive on hinting with the messages MATE sent during loading ("It's puzzling
why this is taking so much time.")
* Stop interleaving the pieces of each color, just show all the pieces in order.

![New puzzle piece order](/public/mh-2023/neworder.png)
{: .centered }

By the time this was resolved and implemented, it was 4 AM. I went to go collapse on an air
mattress so that I could be sort-of awake tomorrow.

Saturday
---------------------------------------------------------------------------------

The Act I testsolve was still running in the testsolve, but we did want to do the originally
scheduled events, so the morning was dedicated to event testsolving and Act 3 puzzle drafting.
In Saturday morning, we started writing what would eventually become:


We Made a Quiz Bowl Packet but Somewhere Things Went Horribly Wrong
---------------------------------------------------------------------------------

Okay. I know this puzzle isn't that popular, but all puzzles will have their stories told.

We were given the feeder answer for this puzzle, and after investigating various pyramid options,
we laneded on pyramidal quiz bowl questions, partly because some of the authors were fans of
quiz bowl. The idea of extracting based on diagonalizing words based on a Hamiltonian path of
the United States seemed like a good way to tie into both the feeder answer and the structure
of a quizbowl question, so we started with constructing a suitable quiz bowl question for
the final answer. Since solvers had access to the Internet, and we knew this would be towards
the end of the Wyrm round, we ended up using a very loose searchability bar (which was one
of the big causes of the puzzle's length).

Originally, the plan was that every question would semantically clue a state in the Union.
This was thrown out pretty quickly when we tried to tie DADDY LONGLEGS to South Carolina, and
the best we could come up with was a South Carolinian high school's rendetion of the musical,
which was a bridge too far.

A sketch of 4 sentences per clue x 48 clues was, in hindsight, a lot. Part of the justification
was that unlike many other difficult puzzles, this was the sort of puzzle that parallelized
well and was easy to jump in and out of. There wasn't a lot of puzzle-specific context you
needed to absorb to make progress. With so many puzzles trending in the direction of serial
deductions + not much branching, it seemed okay to have a big puzzle that could run in the background.

Over the course of a few weeks, we started filling out the draft. It was clear that the puzzle
skeleton would work mechanically, but we were quite unsure if the puzzle would be fun. It
seemed like the solve process would be that you Googled a bunch of phrases, reordered,
indexed the way you were told to, then followed the instruction you were given. You'd then
be done without really having an a-ha anywhere. Given the structure, the easiest place to
add an a-ha was in the 1st step of Googling everything. That motivated the decision to
obscure questions by pretending they were always talking about a location. In this way,
although the bulk of the puzzle would still be Googling, there'd be a local a-ha in deciding
how to reinterpret the clues when formulating that search. "Cryptic-like" is the right analogy
here. Cryptics work because you need to decide what is wordplay and what is definition,
and similarly you'd need to decide what parts to read straight and what parts were location
wordplay.

This made the puzzle...a *lot* harder, which got corroborated by testsolving. To paraphrase, the
feedback was "this was really fun at the start, then unfun after we got really stuck".
I am
not tying to be flippant here, this was not a [sour grapes](https://www.merriam-webster.com/dictionary/sour%20grapes) problem. There are puzzles where you get stuck, and it's a pleasing stuckness to try to
figure out what idea you're missing. In this puzzle, that was less true.

(I suspect the issue is worse if you are solving puzzles from a mindset of trying to 100%
every step before doing the next one. The intermediate cluephrase is really helpful for
constraining how sentences can pair up and trying to solve for it sooner does a lot for
the solve experience.)

My suspicion is that if you remove the obfuscation, the puzzle gets even more mechanical
and it doesn't really save the design.
The right fix would have been to keep the obfuscation, but cut the puzzle size in
half and brainstorm a new extraction. That way, the disparity between easiest and hardest clue
would have been smaller, the matching problem would have been easier, and it would be more
likely you'd hit the cluephrase before the puzzle overstayed its welcome. The fun ratings
weren't great, but they weren't bad either, so it went through without many changes and
never got revisited.

\* \* \*
{: .centered }

The brainstorming for Quiz Bowl was running the same time as event testsolving. We still
weren't sure if we could run Hunt in person during retreat, but decided to plan as if we'd be
allowed to run on-campus. Accordingly, events were designed around in-person interaction.
This was done with the understanding that the events wouldn't be doable without on-campus
presence, and this would upset fully-remote teams, but, well...in my opinion,
it's called the MIT Mystery Hunt for a reason. We had just done 2 years of remote-only events.
Swinging the pendulum back to in-person events seemed appropriate. TTBNL is free to do what
they want here.

AFter lunch, priority shifted back to continuing the Act I testsolve. We had already pushed the
4 AM fixes to the testing site, but embarassing we had to do another hotfix becaues we'd accidentally
made the loading animation cover the answer box with an invisible div.

![Invisible box](/public/mh-2023/answerboxcovered.png)
{: .centered }

Solving of the last few Act I puzzles continued...and the unspoiled testsolves still hadn't
noticed anything. From our #puzzle-ideas channel during retreat:

![Someone saying the loading animation should be a puzzle](/public/mh-2023/itisapuzzle.png)
{: .centered }

Let's say it was an interesting time. They did solve it eventually, but I'm not sure if they
found it organically or if we had to tell them to look at the loading animation for longer.
Once again, the breakout authors and hunt exec went into another room to do a secret testsolve
debrief, with the people who solved breakout. Their initial feedback was "yeah, this seems
obvious, we should have noticed this earlier", to which we said, "You say this, but we have 12 hours of
empirical feedback to the contrary."

After some more Q&A on what made them believe it wasn't a puzzle, reasons why they didn't pay
much attention to the animation, etc, we planned a few more changes:

* The four green dots (used to give an ordering on the letters) would be moved out of the
answer submission box. Their appearance within the submit box made it look like the answer submission
field was "loading" and could not be submitted to.
* Submitting an answer during loading would give a custom message ("Whatever you do, definitely don't submit anything until it's finished!").
* MATE would become increasingly depressed as loading continued, to entice people to keep
watching the page to see what would come next. We planned to add this Friday night but the art
assets couldn't get written overnight. (Much later, this led to the quote of "why does the animation look better when MATE is perpetually sad?")
* Internally, we called the infinite loading time "the hammer" that would push teams to breakout.
Evidently the hammer was not strong enough, so we designed a "megahammer". We'd add a
time-unlock breakout, by sending an email to all teams with a video tutorial of how to solve the
loading animation (explained in story by teammate wondering if MATE had a problem).

We weren't going to have another unspoiled testsolve of these elements, so we biased towards
safety and would need to assume it would all be good enough.

With breakout resolved, we could go back to puzzle brainstorming and wrapping up testsolves. The
[Teammating Dances event](https://interestingthings.museum/solutions/teammating-dances) was
written and tested. I later joined a very-stuck testsolve of [Tissues](https://puzzlefactory.place/factory-floor/tissues). At the time, it did not have the TetraSYS hint. We got it partway through the testsolve,
I mentioned there were four Greek elements (no idea why), and we gave the authors a bit of an
aneurysm by saying we should dunk the tissues in water, or set them on fire. "We should go do it,
even though it's likely not correct! If it's the wrong idea they'll stop us because we don't have
any backups."

(Meanwhile testsolvers for Weaver were dunking the puzzle in water.)

We got up to the cluephrase, again quite late, and I decided to turn in for the night to catch up
on sleep.


Sunday
---------------------------------------------------------------------------------------

I woke up and continued the Tissues testsolve while waiting for people to wake up. I did a few
searches to prove that "Nikoli + black cells" was not enough to get to the next step, and
although I eventually got
"Nikoli + [dokodesu](https://www.google.com/search?q=dokodesu+in+english) + black cells",
it wasn't very reliable for a step so late in the puzzle. By now other testsolvers were awake,
so we had enough manpower to finish the last step in time for the State of Story meeting.

The creative team presented the overall broad strokes of the Hunt story, the existence of
breakout, and the broad art direction for each AI round. For breakout, an audience member asked
"what's our plan if a team doesn't notice breakout?", which had *incredible* dramatic irony.
We said we had a plan and moved on.

Wyrm would be a precocious child with
a construction paper aesthetic (since every explanation of a wormhole in fiction involved sticking
a pencil through two pieces of paper). Bootes would be a meme-y ASCII art cat in space, because
the answer gimmick involved characters and whitespace mattering. Eye would
be a noble, biblically accurate angel, as a nod to the Tower of Babel. Conjuri would be a pixel
art owl eager for new challengers, with pixels from gaming and owl as a suitable animal for the
magic theming of the game. Eye's gimmick was spoiled at this time, since we needed
more authors for Eye puzzles, but the rest of the gimmicks were not spoiled to save them for
testsolving.

We did a final round of puzzle brainstorming and testsolving. I remember getting pulled aside to
a brainstorm meeting because we thought it'd be funny to write a puzzle with "the full house
of Alexs over Brians" (Alex Gotsis, Alex Irpan, Alex Pei, Bryan Lee, Brian Shimanuki,) Don't let
your memes be dreams! We got in a room, tried to start a puzzle for [l(a](https://en.wikipedia.org/wiki/L(a),
spun in circles for an hour, and failed. The puzzle was officially killed a few weeks later. Some
memes should stay dreams.

Retreat concluded with a bunch of board games. We played some rounds of Just Two (it's [Just One](https://en.wikipedia.org/wiki/Just_One_(board_game)) except a clue is only provided if exactly two people
put it done. It's good for large groups and is a *lot* harder. I'm told that when the word "pony"
showed up, the only clue that made it through was "Irpan" which, okay, fair's fair.

East Coasters flew back home, Europeans flew *really* back home, and we ramped up the main push
towards finishing Hunt.


September 2022 - Ah Yes, Websites are a Thing
------------------------------------------------------------------------------------

It's a bit disingenous to say that I started working on the site in September, becaues I had been
working on parts of it starting even back in January. But I'd say my main ramp-up started around
here.

There were a number of tasks to do, and the main one I took was handling the Wyrm round page.
Which ended up being a *lot* more complicated than I thought it would be. During Wyrm ideation,
we discussed the [Zoomquilt](https://zoomquilt.org/) and its [sequel](https://zoomquilt2.com/)
as both art inspiration and proof-of-concept that we could make a infinitely zooming round page.
{I remember Huntinality 2022's [registration form](https://2022.huntinality.com/) also came up.)

Each of the Wyrm layers was done by a different artist. Turns out that if you want every puzzle
to have its own icon, it doesn't matter that 13 puzzles in the round are "fake" or repeats. You
still need to draw 13 art assets. (Creatively, the art team did not want to reuse art assets
between the Museum and Wyrm versions of the Act I puzzles, since the Museum aesthetics were
intentionally not going to match the Wyrm aesthetics, to play up the difference between MATE-created
puzzles and Wyrm-created puzzles.)

As a very early proof of concept, I looked into making a barebones page that would smoothly
zoom between different placeholder art, looking into how to add a zoom with D3.js. The default
behavior of d3 is to both pan and zoom the content, so I needed to manually strip out
the pan information before passing it into the d3 code. I also needed to figure out how to make
the zoom level "roll over" once you passed the infinite threshold. I expected that to be a nightmare,
and was pleasantly surprised when it wasn't.

Based on the barebones version, I advised that tech would be easier if we had a consistent
aspect ratio, each layer shared a common zoom point (likely the center), and the ratio of the
zoom between layers was identical. I also decided to remove scroll wheel support. It was easy to
add, but it was a slightly annoying user experience to have the round art steal your scroll wheel
inputs when you wanted to move down the page. I hardcoded some placeholder puzzles to prove it
worked. Then it was optimization time.

> Based on the profiler call, current D3 setup is 11 ms to re-render all 4 layers when I do the 1 -> 2 zoom. (All 4 divs get re-rendered since they all look at scale variable). I set up the CSS transition for just layer 1 -> 2 and that was 8ms. The gap should get smaller when I make the CSS animation apply to all 4 layers.
>
> So, in short, yes it'll be faster, I'm not sure it'll be appreciably faster.
> I'll probably leave this for later based on the timing, in favor of some other things I need to fix in the prototype. But can keep it in mind as a performance win that may be more important when we have more assets in the round.

Performance optimization is not really in my wheelhouse, but it's something we thought about
a lot. Some of the choices we made (more art, structuring around [single page applications](https://en.wikipedia.org/wiki/Single-page_application)) naturally push towards downloading more data and doing more client-side rendering. To make the zoom work, we *really* needed the zoom implementation to look seamless.
The initial prototype was not. There was very noticeable pop-in as you swapped between layers,
on both the background image and puzzle icons.

It 100% seemed liked a solvable problem, given that Zoomquilt was seamless and I remember
it looking seamless on 10 year old hardware. When I looked more closely, their code was based
on using an HTML canvas. I was less excited about this, because HTML canvases don't use vector
graphics and you can get weird rasterization artifacts when zooming text. (I tried a canvas
briefly in Collage and switched back to SVGs afterwards.)

I asked the tech people with more frontend experience for advice, and they helped explain more
of React's rendering logic and where I could run a profiler. It turned out the problem was that
my prototype was taking fixed `<img>` tags and changing the `src` field based on what zoom level
I was on. This effectively forced the image to clear and re-render every time I crossed layers,
and no browsers are built to do this seamlessly. So instead, we redesigned it to create all the
`<img>` tags for all four layers at once, then use CSS and JS to adjust the zoom and
[z-index](https://developer.mozilla.org/en-US/docs/Web/CSS/z-index) ordering of the static images.
That way, the browser would only load + draw the images once, and the rest would just be image
transforms. This fixed the bulk of the performance issues.

Next was fixing the blurriness. That was easy. When I set up the prototype, I defined the zooms
as $$1, 8, 8^2, 8^3$$. But that meant you would sometimes see parts of a 64x magnified image
during transitions, which looked just the worst, so I inverted the scale to $$1, 1/8, 1/8^2, 1/8^3$$.

With that done, I decided the frontend had been proof-of-concepted enough, and moved towards
other work. Over the past few months, we had made a list of feature requests and bugs on Github,
and I started running through ones I knew how to do.

The first was improving our puzzle icon placement system. In tph-site, the location and size of a puzzle
icon is defined in the database, to make it adjustable without doing an entire deploy. But editing it still
requires digging in Django admin pages, which can be a scary prospect given how many footguns it has,
and the user flow of tabbing back and forth between the admin and site pages is pretty annoying.
This was already a pain point in Teammate Hunt 2021, and was ready for improvement.

I coded a basic WYSIWYG tool, where if you logged in as admin, it would overlay a movable box whose
size and position would correspond to each puzzle icon. Resizing or moving the box would move the
puzzle with it and save the updates to the database, which we'd then download and commit into
the codebase to keep a more permanent record. I think there are still some issues where the box isn't
drawn consistently with the puzzle, but hopefully it helped.

![Drag drop UI](/public/mh-2023/dragdropui.png)
{: .centered }

Meanwhile, I did some work on postproduction of Quandle (to move the wordlist and game logic to
server-side), and handling some of the TODOs tied to breakout's testsolve at retreat. I also got around
to adding a bug fix that Huntinality folks found in tph-site's websocket management, and stole - er, copy-pasted
a k6 loadtesting script to test out later.. (I didn't ask for it, it came up in
a discussion about website issues GPH 2022 ran into in a Discord server.) It looked a lot
nicer than the hodge-podged Locust code I had written for Teammate Hunt 2020.

Things were calm for now, but I knew it was only a matter of time before things would escalate.


October 2022 - How About a Nice Game of Chess?
--------------------------------------------------------------------------------------------

By this point I had finished design work on all puzzles that I had motivation to write, and was planning
to mostly due token tech tasks until the end of Hunt. All of them, except for one.


5D Barred Diagramless with Multiverse Time Travel
-----------------------------------------------------

This puzzle may be my magnum opus. From a fun standpoint, I suspect Interpretive Art wins.
From a solve experience standpoint, I'm guessing Puzzle Not Found wins. But, from a
construction and spectacle standpoint, I would picked 5D Barred Diagramless. It
is really stupendously long, but I claim it is both entirely fair and very up-front
on what you're getting into.

As mentioned way near the start of the post, the first draft of this puzzle was written in
February for an internal puzzle potluck, and did not include any 5D Chess elements. I just really liked the joke, and knew that
[it would be an inherently terrifying puzzle title](https://dp.puzzlehunt.net/team/5D+Crosswords+With+Multiverse+Time+Travel.html).
Since the puzzle had started
design from the [Clinton/Bob Dole](http://www.alaricstephen.com/main-featured/2017/7/3/the-clintonbobdole-crossword)
crossword, the first version went all-out on pushing that idea as far as possible.

In any puzzle where clues solve to a pair of answers, you either need a way to order the pair,
or you need to pick an extraction method that uses both answers of the pair in an unordered way.
I first realized this when solving [Split the Reference](https://2018.galacticpuzzlehunt.com/puzzle/split-the-reference.html)
in GPH 2018, but then started noticing that pattern all over the place.

After doing some fudging to get JOSEPHBIDEN/DONALDTRUMP to line up, I noticed that JOJORGENSEN
*also* happened to be 11 letters long, and, I mean, come on, you have to try doing something with that.
Two timelines would be clued by the grid, and then the 3rd timeline would provide the extraction content.
I had trouble finding other presidential elections where the two parties' candidates could also be
forced to 11 letters, but was able to make it work for a bunch of years if I allowed dipping into
the many 3rd-party candidates. It still felt wrong...but, the potluck deadline was coming soon, so I
decided to just go for it and figure out a fix later.

The grid fill went well, the extraction less so. I needed to give a hint that the 3rd timeline
existed. Still, it seemed promising. I then moved on to lots of other work.

Fastforward to 8 months later. In a team meeting, the EICs mentioned we could use more crosswords in
the Hunt for puzzle variety. Hey, I have an unfinished crossword idea! Let's see if we can fix it.

I reflected on the design, and came away with these notes.

* As much as possible, I wanted
the puzzle to look like a single grid. A 5D crossword should only have one 5D grid! (With that grid
having many projections into 2D space.) More specifically, the given borders and black squares should
match across all of the 2D grids.
* If the extraction is only based on alternate timeline entries, then those are the only parts
of the grid that matter to extraction, and that's a bit disappointing if you worked on the fill
elsewhere. (This was one of the bits of feedback I remember from [Cross Eyed](https://www.puzzlesaremagic.com/puzzle/cross-eyed.html)
years before.)
* If the extraction uses letters from the entries that are time-dependent, then I'd need to call them out
or indicate them in some way, which I wasn't too fond of. Doing so would remove the mini a-ha
for how to fill those entries. For similar reasons I didn't want to label the alternate timeline entries.
* Really, it's a shame the puzzle doesn't use 5D Chess in any way, given how directly it's called
out in the title.

Remembering [✅ ](https://2020.galacticpuzzlehunt.com/puzzle/check.html)
from GPH 2020, I considered the 5D Chess point more seriously. You'd want to solve the entire grid before
doing the chess, because chess pieces and the extracted letters would be all over the grid. The puzzle
wouldn't need to label any of the special entries, preserving the a-ha during the grid fill. Thematically
and mechanically, it just made sense to make it a chess puzzle. Making it barred would let me fill
every square, and making it diagramless would led me keep the given borders identical (since there wouldn't
*be* any).

Great, problem solved! Time to learn how 5D Chess works. How hard could it be?

The editors assigned me an answer from Bootes, with the reasoning that a 5D puzzle could accomodate
a 2D answer. The answer I got assigned had some punctuation characters in it, which I agreed to try
fitting into the puzzle. I said that I could plausibly do things with rebuses, like cluing
GAIL PARENT and filling it in as GAI(T. The answer options weren't great, but were workable.

Over the span of about 4 weeks, I put increasingly deranged comments into the puzzle brainstorm channel.
I'd say the default state of the editors was "confusion", as I monologued my struggle learning 5D chess
to people who only sort of understood what I was struggling with.
I initially
tried to learn the rules by watching gameplay videos. Within an hour I decided that it would
be more efficient to buy the game, so I did. I mostly paid for Mystery Hunt with time. This
was the rare puzzle where I spent money.

I have yet to play a full game of 5D Chess with Multiverse Time Travel. Instead I played the in-game
puzzles, which are specifically constructed to teach you the rules. After getting a basic understanding,
I decided to have the puzzle only use bishops, knights and kings. Rooks were too boring in 5D, pawns could introduce
questions about whether they had been moved yet or not, and queens were terrifying. A single queen in
5D chess has, like, 60 possible moves. There are arrangements where one queen can mate a king in 7 different ways. Trust me it would have been a bad time.

I made a 5D chess problem, filling the rest of the grid with random letters to get early feedback on
just the chess step.
What was immediately clear from those tests was that it was very easy for someone to read a 5D chess guide
fairly quickly, come away with 50% of the rules, and then attempt to solve the chess puzzle incorrectly.
As a solver, there is really no way to distinguish "I do not understand the rules" from "I understand
the rules and am just bad". Like, they would be 50 minutes in, asking for hints and we'd have a conversation
like this:

![A conversation about 5D chess rules](/public/mh-2023/thefuture.png)
{: .centered }

"The future doesn't exist yet" is really such a great line. I recommend using it without context, it can
apply to so many things.

In response to the pre-test, I created some 5D chess examples that could not be solved without understanding
the base rules of timeline creation and branching. This was part inspired by Time Conundrum,
and part inspired by [Cryptoornithology](https://ecph.site/puzzle/cryptoornithology.html) from EC Puzzle Hunt.

Anyways, when factchecking that testsolve on what mates were correct / not, I decided to double-check my
work and found they had found a checkmate that I missed! At that point I decided that okay,
I need to write code to verify this, I'm not going to be able to trust my 5D chess solving ever again.

I reimplemented the rules engine in Python. Not the full engine, but enough to test for mate-in-ones.
I went back and forth between my code and the in-game puzzles until I got them to agree with each other,
discovering a few other edge cases along the way.

At the same time, I sent a question upwards: if I clue the answer directly, do my letters need to be
aligned exactly like the answer, or would it be okay to assume solvers would find the correct whitespace
themselves? My question went to the editors, who sent it to meta authors, who sent a reply back through
the editors, and the answer was: yes, if you aren't using a cluephrase the spacing needs to line up
exactly.

Well okay then I'm not going to be able to get the extraction to work that way, it's too constrained, so
I dropped the rebus ideas and started digging for cluephrases. Finding a new cluephrase of the same
length, I went back to my code, this time modifying it to place pieces for me. By this point, I had
realized a constraint that was not obvious beforehand.

As designed, chess pieces can only differ
between the two timelines if they appear in the alternative timeline entries that have 2 answers.
Given the construction, I was only planning to have 3-4 of those entries across all grids, since each one
took a lot of real estate to support. (The down clues that cross them take up about 25% of the grid squares
per entry.)
If an extraction square appears in the same spot in both timelines, then those letters are forced to
match in the cluephrase, which would add annoying constraints on how many mates needed to exist in each timeline.
To maintain flexibility, I wanted extraction to never agree between timelines, but this was only possible if
I concentrated as many chess letters into the alternate timeline entries, since that was only source
fot timeline deviation.

Does any of that previous paragraph make sense? No? Don't worry about it. The short version is that
every alternative timeline pair needed to satisfy at least one of the following:

* One has kings (K), the other does not.
* One has bishops (B), the other does not.
* One has knights (?), the other does not.

This is pretty much the only reason that the 2008 entry was BEIJING / BANGKOK, even though Bangkok
failed in the very first round of IOC evaluation.
I really, desperately
needed to have an alternate timeline entry where kings existed in one half but not the other.
For a similar reason, the Kentucky Derby was picked just because it was easy to find a historical
winner with a K in its name.

This requirement also influenced the decision to make knights be represented as Js. I knew using N
would make the crossword fill way too difficult. Using J both made the fill easier, and created
differing pieces in the JOSEPHBIDEN / DONALDTRUMP and KENJENNINGS / LEVARBURTON pair. (I wanted
to use MAYIMBIALIK / LEVARBURTON originally, since it seemed like a fun bit of trivia to use "the person who wasn't Ken", but having a shared B in the same spot created too many problems.)

With a better understanding of the requirements, I placed my seed entries in the grid, then had my code
greedily place kings until it had achieved 22 checkmates. It failed to do so. The first king
placed created 17 checkmates, and from that position it had a parity issue where I could get either
21 or 23 checkmates, but not 22.

Hang on, 17 checkmates from one king? That seems wrong.
I added tons of debug print statements to track down the bug. That was how
I learned it wasn't a bug at all. My brute forcer had correctly found a discovered mate.

I verified it in game, and went "holy crap that's so cool." But should it be in the puzzle? I tried
banning my code from using it, and found it made construction impossible without adding more alt-timeline pairs
into the last grid. Essentially, there are around
30-40 different moves possible from the alternative timeline entries, and you literally can't distribute 20+ checkmates among them unless you use discovered mates.

With an understanding of what was going on, I created the final chess position by hand, arranging
half of the seed entries to create the discovered mates, and placing the other half to fix the parity
issue. Now that the full chess puzzle was created, I sent it to another round of testing just the chess.

This testsolve went a bit better thanks to the examples, but still had some trouble due to confusion
on how to set up the chess problem and what "one move" meant. In response I prepared many more seed entries
describing aspects of 5D chess. By this point the puzzle had evolved from "solve a crossword, get an answer"
to "solve a crossword, congrats you're 50% done", and it felt like it was on the edge of unreasonably
long. I expressed these feelings of uncertainty to testsolvers, at which point they said 5D Chess was
"exactly the bullshit I expect to see in Mystery Hunt". I'd say a stronger statement is true: I don't think
this puzzle could exist outside of Mystery Hunt.

So really, the question was whether I wanted it to exist. I think people believe that, like, I was super
gung-ho about getting 5D Chess into Mystery Hunt, and I really wasn't! I was only motivated to do so
after sinking weeks and weeks into understanding 5D Chess enough to appreciate its design. All I wanted
was a multi-dimensional crossword with a suitable extraction, and all roads for that led to chess.
There may be a world where the puzzle is just a 5D Chess puzzle, no frills, but I don't think there's
a world where the puzzle is a 5D crossword without a chess step.

With the chess position constructed and two pretests of the chess step done, the puzzle still hadn't had a fully unhinted solve of the chess,
but the crossword was already 1/3rd filled, and I decided to guess that any future chess nerfs could be applied
after the grid was completed. I spent the next week completing the fill and writing clues. Around the middle of
the fill, I realized I had created some 1-wide columns, which I wasn't too happy about. But fixing it would
have required changing the placement of the alternate timeline entries, which would require regenerating
the chess position, which would have meant restarting from scratch. And the chess problem given
had already gone through one partial testsolve...I decided I didn't have time to fix it.

By now it was late October, where I had spent my entire month of Mystery Hunt time trying to make 5D Diagramless
a reality. I would guess I was at the 60 hour mark and we were just getting to the first full testsolve.
In that testsolve, the grid fill went great! The chess continued to go less well. This time the solvers did not understand the rules for how chess timelines are arranged, as well as some other details about setting up the
chess problem. I let them flounder for a while, realized they were never going to fix it, and
revised the critical 55-Across clue to be extra explicit about timeline arrangement. I also changed
the clue for TWO into a seed clue reinforcing the rules of 5D chess. It was a stroke of good luck that TWO was already in the fill,
although perhaps having it appear in one of three random 13x13 diagramless puzzles isn't too rare.

Once I had hinted the testsolvers to the correct timeline arrangement, and after giving the total number of
checkmates, they solved the chess without any more guidance. I'd say that was the first moment I
started to believe the end of puzzle construction was in sight. I was originally hesitant to give the
total number of checkmates out of cheese concerns. If you find all squares that pieces *could* move to,
then construct a regex to drop out letters until you get to 22 letters, then the phrase is mostly readable
from nutrimatic and you can skip the chess puzzle. In practice, the chess theming is strong enough that
very few people attempt to cheese the puzzle. The goal of giving the number of checkmates was to
give a minor hint towards the discovered mates. Only 5 mates can be found without it. If you make mistakes on
checkmates, maybe you get 1-3 more. It's far enough away from 22 that eventually you're forced
to confront the possibility that you are entirely missing a key idea.

The 2nd testsolve of the full puzzle went well, and I was looking to do a 3rd testsolve, given that I'd made
changes to the chess step in the 1st full testsolve and it had yet to go through 2 fully unhinted tests.
However, the editors decided that between the 1 fully clean solve, and the 3 previous partly-hinted solves,
the value of another test wasn't worth it compared to spending the testsolve time on other puzzles.

By this point, the puzzle had become a bit of a meme. About 1/3rd of teammate had testsolved it and another
1/3rd had heard a rumor that there was a "five-dimensional puzzle". This was shortly after 4D Geo finished
testsolving, and it was funny to realize that our hunt had a 3D puzzle, a 4D puzzle, a 5D puzzle, and
a non-integer dimension cameo in Period of Wyrm. "We have too many dimensional puzzles, new dimensional puzzles are banned."

Internally, as we ramped up postproduction and factchecking, this puzzle got flagged as being especially
scary on both fronts. Many thanks to Holly for factchecking all the crossword clues, and Evan for creating
all the chess diagrams with hundreds of lines of [Asymptotte](https://asymptote.sourceforge.io/) code.

The chess factchecking was easily the most terrifying factchecking experience of my life. I had
researched and installed a mod for 5D Chess that let me load arbitrary positions, but the game is hardcoded
to only work for boards up to 8x8, so the final check had to be done by hand. Here's a list of
things I did not know until after testsolving was finished:

* A player can delay an inevitable mate by branching back in time, unless they have timeline disadvantage.
* A player can mate a king in an active board if they fork two active kings.

The first was only fine because I decided to have black make a timeline instead of white. If I had
messed up that coin flip a month earlier, it would have completely invalidated the chess problem and I would have had to reconstruct the entire puzzle + retest the chess puzzle.
None of the testsolvers caught this, because none of them had actually learned 100% of the rules of 5D Chess.
Instead they had learned the 90% needed to solve the puzzle. But for a Mystery Hunt, 90% isn't good enough.
You never know how far solvers will go.

The check for forks was also really close to failing, although I think my odds of failure were more
like 20% rather than 50%.
As for the analysis on moving two pieces in one move, I had some backups in mind if that didn't work,
but thankfully it did.

Factchecking the Appendix was a casualty of the last-minute push to get Hunt ready in time. Although
it wasn't independently verified, we were willing to say it was unlikely solvers would go as
in-depth as I did on checkmate verification, Evan hadn't noticed any errors when reading through the solution
for postprodding, and there were puzzles in more dire need of factchecking.

The solution ended up at around 8000 words of just complete, utter absurdity. I'd say that's why I consider
it my most important work. It is this perfect encapsulation of what it's like to fall down a puzzle construction
rabbit hole by taking a joke way too far, and come out the other side with a coherent, self-consistent, and
beautifully terrifying puzzle. Only two teams solved it forward, but I was expecting at most 10 teams to
finish it, so I'm really not too torn up about it. I got most of my enjoyment from making it,
not watching it get solved. (It helps knowing that one of the forward solvers said it made their top 5 puzzles ever.)

The one regret I have is that one of the examples could have
relied on a discovered mate to make the endgame extraction flow smoother.
As a standalone puzzle I believe it is better without a hint in the examples, but in the context of Hunt,
including it would have given most of the Eureka moment while making it less likely teams got stuck at the end.

\* \* \*
{: .centered }


November 2022 - AH YES, WEBSITES ARE A THING
---------------------------------------------------------------------------------------------

There were a...concerning number of things to figure out for the Hunt website.

A lot of it was classic stuff. Figuring out how to make the Factory round page meet the design
spec, supporting different site themes per round, etc. *Yawn*. Like, the problems were straightforwardly
solvable. There were just a lot of them and concerningly few people to work on them. My understanding is
that a lot of people who planned to do tech were either busier with real life than expected, or sucked
into the Conjuri rabbit hole. This left a lot of infra tasks on the table.

With 5D Chess wrapping up, I decided I was mostly done with starting new puzzles, and offered to
go more all-in on tech, given my experience with tph-site internals.


Events
-----------------------------------------------------------------------------------

This was less tech work, and more design for how we wanted to represent events in our codebase.

We decided events would act like globally unlocked puzzles, with some event-specific metadata to manually open them
when the event was completed. We considered how to do event accounting, with the goal of minimizing
logistical overhead, and decided to have every event solve to an answer that teams would enter themselves.
Pretty much every recent Mystery Hunt has used this system, it just works.

There was no plan to have an events round like 2019 Mystery Hunt, so what should events do? EICs decided that
events should act as either free answers (manuscrip) or free unlocks (Penny Passes), with free answers
only usable in non-AI rounds and free unlocks usable everywhere. We wanted event rewards to be useful at
all stages of the hunt, but didn't want teams breaking into the AI round gimmicks with free answers.

(The church bells of dramatic irony are ringing. We'll get there, I promise.)

I asked some pointed questions about edge cases. How do free unlocks affect meta unlock? Is all of Museum
one round for unlock purposes or not? The real reason I was asking was because I wanted to figure out
requirements for the real tech blocker:


Unlock System
---------------------------------------------------------------------------------

gph-site and tph-site use a concept called "deep". I don't know why it's called deep. Maybe because of
GPH 2019's starter theme about an excavation? In any case, "deep" is a measure of how far you are in
a Hunt, but how that is measured can be done in different ways.

The 2022 Mystery Hunt chose to have a fixed, single track unlock order. All puzzles, both feeders and
metas, were on that track. The 2021 Mystery Hunt had per-round tracks, where each puzzle gave a lot of
JUICE to its round and some JUICE to smaller rounds.

Our Hunt was looking like it would be more like JUICE, escept weirder. AI rounds were going to have
independent unlocks, since each was its own hunt in-story. Oh except we wanted new AI round unlocks to
be based on Act 3 solve progress. The Museum was slated to have a shared
unlock track between all rounds, but with a min-solves-in-round threshold for metas. Factory rounds were
going to be mostly normal. Oh, and the number of puzzles initially visible in the Factory was going to scale with the number of Museum solves teams had before unlocking it.

I decided that, you know what, we don't know what our unlock system is yet, it could change wildly,
so I am just going to implement the most generic unlock system I can think of. I arrived at what I'd
call the "deep key" system.

* Every puzzle has a deep key (an arbitrary string), and a deep value (an arbitrary integer). A puzzle
unlocks if the team's deep for a deep key is at least the deep value.
* By default, solving a puzzle contributes 100 deep to its round name.
* It would also support two different overrides: a round-level override (changes deep-on-solve for the round)
and a puzzle-level override (changes deep-on-solve for that puzzle).
* Solving a puzzle can contribute any amount of deep to any number of deep keys.

To justify it, I gave examples of how to implement all of the proposals.

* Museum rounds: Every museum puzzle contributes 100 to "museum" and 1 to "X-meta" where X is its round. The meta for each round has deep key "X-meta" and all other puzzles have deep key "museum"
* Unlock an AI round after N solves in Act 3: each AI round puzzle contributes 100 to its own round and 1 to "act3-progress", the round opens at N act3-progress, then solves within the round contribute 100 to itself.
* Breakout: starting Museum puzzles could contribute 1 to "factory". The breakout puzzle would contribute 100 to
factory, then the starting set of Factory puzzles could open at 100 + N depending on how many we wanted open at the start.

This is essentially a generalization of JUICE, or a generalization of Puzzlehunt CMU's "solve K of N"
system. Each puzzle's database entry would only need to store 2 fields, making it easier to scan in Django
admin, with the heavy lifting of the logic done in Python. In the design notes, I mentioned that
arbitrary string keys could potentially be hard to maintain, but I expected us to have at most 20-30 different
keys and for the key names to be mostly self-explanatory, and that max flexibility now would let us adapt
more later.

I didn't get any pushback, and spent a day or so implementing it. Along the way, I implemented a "deep floor" system, basically our version of time unlocks. A deep floor is a Django model that essentially says, "team X should
always have at least this much deep for this deep key". Floors were created in a default-off state, and could
be turned on whenever we wanted to time unlock something, along with an option for a "global deep floor" that
would force a minimum deep for all teams. I also attached a UUID to each deep floor, along with a magic endpoint that a team could hit to opt into a deep floor. This was to support optional time unlocks similar to what
Palindrome did. (I asked Palindrome's team where this was implemented in their codebase, and they told
me it was implemented very rapidly during Hunt and I should just redo it on my own.)


"Wyrm is a Special Child"
--------------------------------------------------------------------------------------------

In the middle of me working on time unlocks, Ivan reached out and asked if I could help on making the
Hunt "feature complete" by December 1st. This was an internal deadline we'd set for creating a minimally
viable Hunt, where a team could go in, solve puzzles, progress through the Hunt story, and reach the end
of Hunt assuming they had all puzzle answers.

When I looked into the TODOs, there was a rather terrifying list of things to implement, including:

* How do we represent repeated Wyrm puzzles across the Museum and Wyrmhole?
* All the logic around team interactions for physical puzzle pickup, and Reactivation, including
the story state changes required for that.
* Connecting the story state to the Factory's display code to support shutting down the Factory and
slowly reactivating it.
* Implementing the teamwide dialogues and connecting them into the unlock and story advancement logic.

I decided to take the Wyrm and story consistency issues, while Ivan set up the dialogue logic ("it's just
Kyoryuful Boyfriend all over again"). The Wyrm logic was especially wild, since each puzzle is actually
displayed in up to 3 contexts:

1. As a puzzle in the Museum.
2. As a puzzle on the conveyor belt in the shutdown Factory.
3. As a puzzle in the Wyrmhole.

Each context needed to display the puzzle differently and uses a different URL and puzzle icon. Additionally, we decided that the errata and hint
state should be identical across a puzzle and its copy, but their guess states needed to differ, since the
Museum puzzle would unlock different things than the Wyrmhole puzzle. We *also* needed to make sure you
couldn't use free unlocks in the Wyrmhole round until after you met Wyrm post-shutdown. Internally solvers
unlock the Wyrmhole round as soon as the teammate interaction is done, because that is easiest for puzzle
management, but externally solvers don't know the Wyrmhole round exists yet, so we had to add an edge case
for that. The List of Puzzles page also needed to redact the
name of the Wyrmhole round until you met Wyrm again, which also needed to propagate to the navbar, and this
all needed to be tied to story progress not solve progress because you are at 2 Wyrmhole solves both before
and after talking to Wyrm...in short, there was an inordinate amount of work done for a section of Hunt
most teams sped through in 3 minutes. :sadwyrm: was definitely the most used Wyrm emote in the server,
used whenever "yet another Wyrm edgecase" was added.

You might ask, was this all *really* necessary? And the answer is pretty much, yes, it was. Remember what
I said earlier: the way you make a Hunt story work is by considering every aspect of Hunt and making
it self-consistent with where solvers are in the story. I don't regret making this happen. (I do want to point out
that this is case where choosing to repeat puzzles 3 months ago led to an extra ~10-20 hours of tech work down the line.)

Aside from Wyrm stuff, I went through the spoilr code and cleaned up a lot of the interaction handling to
fit our use case, and created triggers on interaction completion to fire updates to the story state,
as well as Discord alerting for those story states. We realized that as implemented, story state had
to contribute to puzzle deep, because after Reactivation, teams would either be before or after teammate arriving
to their HQ. If unlocks were only based on puzzle solves, there'd be no way to distinguish before and after.
Our travel time to team HQs would also vary, so a timer wouldn't work. So, the unlock system got modified
to allow arbitrary story interactions to either add deep to a team, or create a minimum deep floor for a team.

Tied to this was live updates of round pages without needing to refresh the site. This was originally
labeled as a "nice to have" issue, but I raised the point that shutting down Mystery Hunt *does not* work
if the site didn't live-update. Imagine that you talk to MATE, and the conversation stops, but
the Factory is still brightly lit and everything is interactable until you refresh the page. Doesn't that
really break the immersion? Ivan went "oh shoot you're right" and we immediately bumped it up to
site blocking.

Very quickly, I was sure that I wanted to make as few changes to existing code as possible. I was already
making wide-reaching changes to support Wyrm edge cases and really did not want to add more complexity.
The frontend gets its state initialized by an API call against the server that populates various
React components, and we decided the easiest solution was to add a websocket-based trigger that resent
the API call.

"Wait, but do we need to worry about self DDOS-ing our site"?

Assuming an entire 100-person team was watching the story, we'd get a quick burst of 100 requests, which
we decided was fine. What would not be fine would be if every tab from that team fired at the same time,
since that could look more like 3000 requests at once instead, so we added some logic to only live-update
the tab when it became active.


December 2022 - The End is Never The End is Never The End is Never
-----------------------------------------------------------------------------------------------

Near the end of Hunt, one of the team leads said Wyrm was intended to be the AI round with the least
tech work involved. Bootes and Eye would have really weird answer checkers, and Conjuri was clearly
going to be a lot of work, but Wyrm would mostly have normal puzzles.

This...did not end up being true. Surprisingly Eye ended up as the most straightforward rounds, since its
round art could mostly be shared with the Museum code. Sure, there was diacritic and language
canonicalization hell, but it wasn't too bad. In comparison, Wyrm had all these subrounds, puzzles that
were either metas or not depending where you were in the round, and five different sets of avatars that
were unlocked in tandem with each round.

IMAGES HERE

Hunt was becoming more real - people outside teammate are talking about it, wow! Boy they sure seem excited.
Meanwhile I was working longer and longer hours on Hunt, and it was accurate to describe it as work. My job
has a cap on vacation hours, and I started taking days off to make sure I wouldn't hit the cap, but most
of those days off were spent working full time on Hunt instead. It was feeling like I was always spending
my time running for something, work or Hunt or this blog, and it had been a while since I stopped.

There was a robot learning conference in New Zealand
that I'd be going to in December. I let people on teammate know I would be travelling for work for a week,
then taking a *real* vacation for another week. But before that:

Walking Tour
------------------------------------------------------------------------

We'd actually started ideating this puzzle at a Bay Area puzzle writing meetup in October. It was a pretty
interesting meetup, since the first thing we did was testsolve Hall of Innovation. We were given most of the
feeders for the Factory Floor meta. After getting nowhere, we were
told "by the way, your team has unlocked a new round". We did all of Hall of Innovation, then were told
"Congrats! The Factory Floor meta isn't ready yet, come back in a few weeks". Really one of the most
trippy testsolving experiences I've ever had. I'm pretty sure they were trying to test whether we would
solve the Factory Floor meta without the Innovation answers or the Blueprint diagram. We didn't, which
was good, but it's still wild that we were told to testsolve a meta, was given an entirely separate
round of puzzles, and then were told we couldn't continue testsolving the meta until they'd finished
doing more art.

But! We are not here to talk about innovation in Museum design. We are here to talk about innovation
in the Tower of Eye. A number of Eye puzzles were written at in-person meetups like this one, since
they were harder to design and needed more manpower to brainstorm. Our group aimed to come up with
a suitable Dutch puzzle. We considered many things: Dutch auctions, Double Dutch, something with
tulips, etc. A teammate mentioned KLM Royal Dutch Airlines gives out Delft Blue house souvenirs that
correspond to real locations in Amsterdam, bringing out the Delft Blue houses they owned. (We were at their
place. They didn't, like, carry this around on a day-to-day basis.) There was a canonical list of KLM
houses, with their number, year of release, and corresponding Amsterdam address on Wikipedia.
For any puzzle constructor, having a canonical dataset is immedaite puzzle bait. That was how we arrived
at a Google Maps runaround in Amsterdam, where the first a-ha was that you were in Amsterdam, and the
second a-ha was figuring out KLM houses existed.


Funnily enough, there was a puzzlehunt at the conference! The puzzlehunt was a student outreach event,
but was open to everyone. It also...had real prizes? Like, $400 NZD (= $244 USD) for
the first team that finished. I caught up with the organizers later. They said that the conference gave them a budget and
most of the budget was turned into prize money. I did the puzzlehunt, and it had all the classic first-time
constructor mistakes, but I had fun and ended up placing 3rd which was enough for a $100 NZD gift card that
I used to subsidize my travel.

The vacation really helped - I needed a few days to get into the headspace of "you have no obligations today",
but once I did it was nice to have no stress about deadlines for a bit. Of course, I wasn't able to
fully disengage from Hunt, and ended up idly working out design for the last puzzle I'd write
