---
layout: post
title:  "Writing MIT Mystery Hunt 2023"
date:   2023-02-06 00:32:00 -0700
---

*This post is XXX words long, and is riddled with spoilers about pretty much every aspect of MIT Mystery Hunt 2023. There are no spoiler bars. You have been warned.*

I feel like every puzzle aficionado goes through at least one conversation where they try
to explain what puzzlehunts are, and why they're fun, and this conversation goes poorly.
It's just a hard hobby to explain.
Usually, I say something about escape rooms, and that works, but in many ways the typical
puzzlehunt is *not* like an escape room? "Competitive collaborative spreadsheeting" is more
accurate but less clear why people would find it entertaining.

Here is how I would explain it to my machine learning colleagues if I had more time.
In a puzzlehunt, each puzzle is a bunch of data. Unlike puzzles you may be familiar with,
these puzzles may not directly tell you want to do. However, if it's a good puzzle, it
will have exactly one good explanation, one which fits better than every alternative.
When done properly, every part of the puzzle will point to some core idea or ideas,
in a way that can't be a coincidence.
In other words, a puzzle is something that *compresses well*.
As a solver, your job is to find out how.

Puzzles can be a list of clues, a small game, a bunch of images, whatever. The explanation
for how a puzzle works is usually
not obvious and fairly indirect, but there is a guiding contract between the puzzle
setter and puzzle solver that the puzzle is solvable and its solution will be
satisfying. At the end of the puzzle, you'll end with an English word or phrase, but that
is more to give a puzzle its conclusion. People do not solve puzzles to declare "The
answer is THE WOLF'S HOUR!". They solve puzzles because figuring out what's going on
is fun, and when you find the explanation (get the a-ha), that's when you're having fun.

As a hobby, puzzlehunting is really biased towards professors and programmers.
Research and debugging share a similar root of trying to explain the behavior
of a confusing system. It stretches the same muscles. It's just that puzzles are about
artificial systems designed to be fun, whereas research is about real systems where progress
may not be fun.

Okay. That gives more of an answer to why people do puzzles. Why do people *write* puzzles?

I have a harder time answering this question.

When working on Mystery Hunt 2023, it started as a thing on the side, then evolved into a part-time job.
Then a full-time job towards the end.
Writing a puzzlehunt is incredibly time consuming. You're usually not getting much money,
and your work will, in the end, only be appreciated by a small group of hobbyists. It all
seems pretty irrational.

In some sense it is. That doesn't mean it's not worth doing.


# Post Structure

Oooh, a section of the post describing the post itself. How *fancy.* How *meta.*

I have tried to present everything chronologically, except when talking about the construction
of specific puzzles I worked on, in which case I've tried to group my comments on the puzzle together.
Usually I was juggling multiple puzzles at once, so strict chronology would be more confusing
than anything else.

This post aims to be complete, and that means it may not be as entertaining.
I'm not sure
of the exact audience for this post, and figure it'd be useful if I just dumped everything I thought
was relevant. If you are the kind of person who reads a Mystery Hunt retrospective that's posted
in April, you probably care about some of the nitty gritty details anyways.

Most of the post is going to be a play-by-play of things I worked on, and the Hunt-wide commentary
is at the end.


- Auto generated table of contents
{:toc}


# December 2021

## Oh Boy, Mystery Hunt is Soon!

Writing and post-hunt tech work for [Teammate Hunt 2021](https://2021.teammatehunt.com/)
has been done for a while. Life is good. Team leadership sends out a survey to see if the
team has enough motivation to write Mystery Hunt.

I use a time tracker app for a few things in my life, and puzzle writing is one of them.
My time spent on Teammate Hunt clocked in at 466 hours. I do some math and find it averaged
to 15 hours/week. This is helpful when trying to decide how to answer the question for
how much time I'd commit to Mystery Hunt if we won.

I already had some misgivings around how much time and headspace Teammate Hunt took up for me.
On the other hand, it is Mystery Hunt. Noting that I felt like I did too much for Teammate Hunt,
I targeted 10 hours/week for Mystery Hunt.

The survey results come in, and there is enough interest to go for the win. I have zero ideas
for a puzzle using Mystery Hunt Bingo, but figure that *maybe* we'll win Hunt, and *maybe*
there will be a puzzle using Mystery Hunt Bingo, so I'd better [remove the "this is not
a puzzle"](https://github.com/alexirpan/mystery-hunt-bingo/commit/77842e1962ac791ce77c56d8a25d00f79227befc)
warning early, just in case.
I didn't want to face any [warrant canary](https://en.wikipedia.org/wiki/Warrant_canary)
accusations if we actually won.


# January 2022

## The Game is Afoot

Holy shit we won Hunt!!!!!

I write a post about [Mystery Hunt 2022]({% post_url 2022-01-22-mh-2022 %}), where I make a few predictions about how writing
Mystery Hunt 2023 will go.

> After writing puzzles fairly continuously for 3 years (MLP: Puzzles are Magic into Teammate
Hunt 2020 into Teammate Hunt 2021), I have a better sense of how easy it is for me to let puzzles
consume all my free time [...]
> Sure, making puzzles is rewarding, but lots of things are rewarding,
and I feel I need to set stricter boundaries on the time I allocate to this way of life - boundaries
that are likely to get pushed the hardest by working on Mystery Hunt of all things.
>
> [...] I'm not expecting to write anything super crazy. Hunt is Hunt, and I am cautiously
optimistic that I have enough experience with the weight of expectations to get through the writing
process okay.

Before officially joining the writing Discord, I set myself some personal guidelines.

**Socializing take priority over working on Mystery Hunt.** I know
I can find time for Mystery Hunt if I really need to. A lot of puzzle writing can be done asynchronously,
and I'm annoyingly productive in the 12 AM - 2 AM time period.

**No more interactive puzzles, or puzzles that require non-trivial amounts of code to construct.**
The goal is to make puzzles with good creation-time to solve-time ratios. Puzzles that require
coding are usually a nightmare on this axis,
since it combines the joys of fixing code with the joys of fixing broken puzzle design.

**No more puzzles where I need to spend a large amount of time studying things before I can even
start construction.** Again, similar reason, this process is very time consuming for the payoff.
I'd estimate I spent 80 hours on
[Marquee Fonts](https://2021.teammatehunt.com/puzzles/marquee-fonts), since I started with knowing
nothing about how fonts worked and ended with knowing much more than I'd ever need to know.

**No more puzzles made of minipuzzles.** Minipuzzles are a scam. "Oh, we don't have any ideas that
are big enough to fill one puzzle. Let's make a bunch of minipuzzles instead because it's easy to
come up with small ideas!" Then you get halfway through, and realize that ideation of small puzzles
is easy, but execution takes way longer since the process of finding suitable clues is somewhat
independent of puzzle difficulty. I also felt it was a crutch I was relying on too often.

**No more puzzles with very tight constraints.** It collectively took 60-100 person hours to
find a good-enough construction for [Mystical Plaza](https://2021.teammatehunt.com/puzzles/the-mystical-plaza),
even with breaking some puzzle rules along the way. Usually, the time spent fitting a tight constraint
does not directly translate into puzzle content.

These guidelines all had a common theme: keep Hunt managable, and make puzzles that needed less time
to go from idea to final puzzle.

I would end up breaking every one of these guidelines.

## Team Goals and Theme Proposals

The very first thing we did for Hunt was run a survey to decide what Hunt teammate wanted to write. What
was the teammate experience that we wanted solvers to have?

We arrived at these goals:

1. Unique and memorable puzzles
2. Innovation in hunt structure
3. High production value
4. Build a great experience for small / less intense teams

**Unique and memorable puzzles:** Mystery Hunt is one of the few venues where you can justifiably write
a puzzle about, say, grad-level complexity theory. That's not the only way to make a unique and memorable
puzzle, but in general the goal was to be creative and have fewer filler puzzles.

**Innovation in hunt structure:** This is something that both previous Teammate Hunts did, and as a team
we have a lot of pride in creating puzzles that stretch the boundaries of what puzzles can be.

**High production value:** teammate has both a lot of software engineers and a lot of art talent, which
let us make prettier websites and introduce innovations like copy-to-clipboard. We wanted to make a Hunt
that lived up to the standards set by our previous Hunts

**Build a great experience for small / less intense teams:** We generally felt that Mystery Hunt had
gotten too big. Before winning Hunt, we had already downsized and were around 60% the size of Palindrome
when they won last year. Correspondingly, we spent a while discussing how to create fewer puzzles while
still creating a Hunt of satisfying length, as well as the importance of mid-Hunt milestones.

We then held team elections for leadership roles. People ran for roles, wrote a short blurb, and
we voted for each. I deliberately did not run for leadership roles, since they implied a baseline
level of commitment that was above my 10 hr/week target.

With elections done, we started on theme proposals. This is always an interesting time in hunt development,
since it sets the agenda of the entire upcoming year. Things can change later, but writing a hunt is an
especially top-down design process. You decide your story, which decides your metametas and metas,
which decides your feeders, and all of this is handing off work to your future selves.

Historically, at the start of theme writing, I say I don't have theme ideas. Then I get an idea right
before the deadline and rush out a theme proposal.
This happened in Teammate Hunt 2021 and it happened for Mystery Hunt. The theme I pitched for Teammate
Hunt 2021 was not revived for Mystery Hunt (I didn't think it scaled up correctly), but the Puzzle
Factory theme is recycled from a Teammate Hunt 2021 proposal.
We talked a bit about whether this was okay, since some organizers for Teammate Hunt 2021 were not
writing Hunt this year. In the end we decided it was fine. At most there would
be plot spoilers, not meta spoilers.

We liked the story structure of Mystery Hunt 2022 a lot, and almost all theme proposals were structured
around a "three Act" framework, where Act I introduced the plot, Act II built up to a midpoint story event,
and Act III resolved that event.

![Theme proposal](/public/mh-2023/theme.png)
{: .centered }

A few members with past Mystery Hunt experience
mentioned that theme ideation could get contentious. People naturally get invested in themes,
and spend time polishing their theme proposal. People working on *other* themes would observe
this, and feel obligated to polish their proposals. This could escalate into a theme arms race,
where lots of time was spent on themes that would ultimately not get picked.

To try to avoid this, a strict 1 page limit was placed on all theme proposals. People
were free to read discussion threads of longer freeform brainstorming, but there would be no expectation
to do so, and all plot and structure proposals needed to fit in 1 page.

Did this work? I would say "maybe". It definitely cut down on theme selection time, and reduced work on
discarded themes, but it also necessarily forced theme proposals to be light on details. Team memes
like "teammate is the villain" seemed to work their way into every serious theme proposal at some level.
Maybe that was genuinely the story we wanted to tell, but it could also have been an artifact of writing
themes while the memes were fresh. There may have been more diversity in theme ideas if they were written
over a longer period of time.

I was not on the story team, but in our post-Hunt retrospective, members of the story team
mentioned they were under a lot of pressure to fill in plot details that weren't in the theme
proposal, because, well, there wasn't space for them in the proposal! Even the details that do exist
differ a lot from where the story ended up. Here is how I would summarize the final version of the Hunt
story.

> teammate announces a Museum themed puzzlehunt written by MATE, a puzzle creating AI. teammate is
> really concerned with making a "perfect" Mystery Hunt that isn't doing anything too crazy. The Museum
> is Act I of the Hunt. Over the
> course of solving, teams discover the Puzzle Factory, the place where Mystery Hunt puzzles are created.
> The Puzzle Factory is not a place that solvers were supposed to discover, and teammate does their best
> to pretend it doesn't exist when interacting with teams. The Puzzle Factory is Act II of the Hunt, and is
> explored simultaneously with Act I. As they explore the factory, teams learn that
> MATE is overworked, and other AIs that could have helped MATE were locked away by teammate due to being
> too weird.
>
> Solvers reconnect the AIs, and this prompts teammate to shut down MATE, Mystery Hunt and the Puzzle Factory.
> They berate teams for trying to turn on the old AIs, then leave.
>
> However, there is some lingering power after Mystery Hunt is shutdown, which solvers can use to slowly
> turn the Puzzle Factory and other AIs back on. This starts Act III of the Hunt.
> Each AI round is gimmicked in some way, ending in a feature request
> that AI wants to add to the Puzzle Factory. When all AI rounds are complete, teammate comes back and
> admits that they were wrong, and the Puzzle Factory makes one more puzzle, which has the coin.

Now, here is the start of the original proposal:

> Act I begins with the announcement of an AI called MATE that can generate an infinite stream of perfect puzzles, as well as provide real-time chat assistance for hints, puzzle-solving tools, etc). During kickoff, teammate gives a business presentation with MATE in the background– but at the end, the video feed glitches briefly and other AIs show up for a split second (“HELP I’M TRAPPED”); teammate doesn’t notice. Stylistically, the first round looks like a futuristic, cyberspace factory. As teams solve the initial round of puzzles, errata unlock (later discovered to be left by AIs locked deeper in the factory), hinting that there’s something “out of bounds”. No meta officially exists for this round (the round is “infinite”), but solving and submitting the answer in an unconventional way leads to breaking out. (To prevent teams from getting stuck forever, we can design the errata/meta clues to get more obvious the more puzzles they solve.) Solving this first meta also causes MATE to doubt their purpose and join you as an ally in act II.

Quite a lot changed from the start to the end. The infinite stream idea was cut because we couldn't
figure out the design. Kickoff did not show the other AIs at all. The surface theme was changed to something
completely different. In a longer ideation process, perhaps more of this design work could be done by the entire team,
rather than just the story team.
Maybe allowing wasted effort is worth it if it gets the details filled out early?

Themes were rated on
a 1-5 scale, where 1 = "This theme would directly decrease my motivation to work on Hunt (only use if serious)" and 5 = "I'll put in the hours to make this theme work".
I don't remember exactly how I voted, but I remember voicing some concerns about the Puzzle Factory.
The plot proposal seemed pretty complicated compared to previous Hunts. I wasn't sure how
well we'd be able to convey the story - [You Get About Five Words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words)
felt accurate for Mystery Hunt, where some people will speedrun the story in favor of focusing on
puzzles. I was also hesitant about whether we'd have enough good ideas for gimmicks to fill out the AI rounds in
Act III. It seemed like a good theme for a hunt with 40 puzzles, but I didn't know if it worked for
a Hunt with 150+ puzzles.

I'm happy I was wrong on both counts. Feedback on the story has been good, and I feel the AI round gimmicks
all justified themselves. I was imagining a Mystery Hunt where Act III was the size of Bookspace (~10 rounds)
and limiting it to 4 rounds did a lot for feasibility.

The Puzzle Factory did not win by a landslide, but it was the only theme with no votes of 1, and had more
votes of 5 than any other theme. Puzzle Factory it is!


## Hunt Tech Infrastructure

I'm going to talk a lot about hunt tech, a very niche topic even within the puzzle niche.
Still, I'm going to do so anyways because

1. It's my blog I get to write what I want.
2. By now I've worked with four different puzzlehunt codebases (Puzzlehunt CMU, gph-site, tph-site,
and spoilr) so I've got some perspective on the different design decisions.

The first choice we had to make was whether we'd use the hunt codebase from Palindrome, or use the tph-site
codebase we'd built over Teammate Hunt 2020 and Teammate Hunt 2021. Our early plan is to mostly build
off tph-site. The assumption we made is that most Mystery Hunt teams do not have an active codebase,
and default to using the code from the previous Mystery Hunt. However, teammate had tph-site, knew how
to use it, and in particular had accumulated a lot of helper code to make crossword grids, implement
copy-to-clipboard, and create interactive puzzles.

The only recent team that seemed like they'd face a similar decision was Galactic, who decided to
build off the spoilr codebase into [silenda](https://github.com/YewLabs/silenda) rather than use gph-site.
After asking some questions, it sounded like the reason this happened was because parts of the tech
team were already familiar with spoilr. So for our situation, it seemed correct to use whatever code
we knew best, which was tph-site.

We start work on open-sourcing tph-site, partly because it's not too much work to do so and partly
because [Huntinality](https://2022.huntinality.com/) is asking if they can see the work we did to
convert our frontend to React.

<div class="shaded" markdown="1">
### A React Tangent

Almost every hunt codebase is written in [Django](https://www.djangoproject.com/). It's
a Python web framework that does a lot of work for you. Python code lets you define your
database schema, user model, what backend code you want to run when users make a request,
and what URLs you want everything to live at. Although it is helpful to know what happens under
the hood, Django makes it possible to build a site without knowing what happens under the hood
using just Python, one of the most friendly beginner languages.
I first learned Django 11 years ago and it's still relevant today.

The default recommended approach in Django is that when a request comes in, you render an
HTML response based on a template file that lives on your backend. The template gets filled out
on the server and then gets sent back as the viewed webpage.

tph-site still uses Django as its backend, but differs in using a React + Next.js based frontend.
React is a Javascript library whose organizing principle is that you describe your page in components.
Each component either has internal state or state passed from whatever creates the components.
A component describes what it ought to look like according to the current state, and whenever the
state is updated, React will determine everything that could depend on that state and rerender it.
The upside: dynamic or interactive web pages become a lot easier to build, since React will handle
a lot of boilerplate Javascript and state management for you. The downside: extra layers of indirection
between your code and the resulting HTML.

Next.js is then a web framework that makes it easier to pass React state from the server, and
support rendering pages server-side. This is especially useful for puzzlehunts, where you want to
do as many things server-side as possible to prevent spoilers from leaking to the frontend.
(As for the merits of [SPAs](https://en.wikipedia.org/wiki/Single-page_application) versus a multi-page setup, I am not qualified enough to discuss the pros and cons.)

The tph-site fork exists because teammate devs wanted to use React to implement the Playmate in
Teammate Hunt 2020. Porting gph-site to React was quite painful, but I don't think Playmate was
getting implemented without it, and we've since used it to support other interactive puzzles.
In general, I believe we made the codebase more powerful, but also increased the complexity by
adding another framework / build system. (To use tph-site, you need to know both Django and React,
instead of just Django.) One of teammate's strengths is that we have a lot of tech literacy and
software engineering skills, so we're able to manage the higher tech complexity that enables the
interactive puzzles and websites we want to make. For new puzzlehunt makers, I would generally
recommend starting with a setup like gph-site, until they know they want to do something that
justifies a more complicated frontend.
</div>



# February 2022

## PuzzUp

[PuzzUp](https://github.com/Palindrome-Puzzles/puzzup) is Palindrome's fork of [Puzzlord](https://github.com/galacticpuzzlehunt/puzzlord), and is a Django app for managing puzzles and testsolves. We considered
giving it a teammate brand name and didn't because there were more important things to do.

The mantra of puzzlehunt tech is that it's all about the processes. The later in the year it gets,
the busier everyone is with puzzle writing, and good luck implementing feature requests during
Hunt. Early in the year is therefore the best time to brainstorm ways to reduce friction in
puzzle writing and hunt HQ management.

The PuzzUp codebase had some initial Discord integrations to auto-create Discord channels when puzzles
were created in PuzzUp. We wanted to extend this integration to auto-create testsolve channels for
each puzzle. However, Discord limits servers to have a max of 500 channels. Based on an extrapolation
from Teammate Hunt, we'd have more than 500 combined puzzle ideas + testsolve by the end of Mystery
Hunt writing. (I just checked out of curiosity, and we hit 338 puzzle ideas and almost 500 testsolves by the
end of Hunt.)

We poked around and found Discord has much looser limits on threads! So anything that lets us permute
channels into threads lets us get around the Discord limits.

Here's what we landed on: all testsolves are threads. Each thread is made in a #testsolve-mute-me
channel. Muting the channel disables all notifications from the channel. Whenever a testsolve session
is created, the PuzzUp server would start a thread, tag everyone who should be in the testsolve,
then immediately delete the message that linked to thread creation. The thread would still exist,
and could be searched for, but no link would appear in the text channel. We also extended the codebase
to have Google Drive integration, to auto-create testsolve spreadsheets for each new testsolve.

I say "we" here but I did none of this work. I believe it was mostly done by Herman. Much later in
the year, I updated the Google integration to auto-create a brainstorming spreadsheet for new puzzles,
because I got annoyed at manually making one and linking it in PuzzUp each meeting. You don't know what will
be tedious until you've done it for the 20th time.


## Puzzle Potluck

A puzzle potluck (no not [that one](https://puzzlepotluck.com/4)) is announced for early March.
The goal is to provide a low-stakes, casual venue for people to start writing puzzle ideas.
There's not much to do in tech yet, so I start working on three ideas. One does not work and does
not make it into Hunt. One is an early form of 5D Barred Diagramless with Multiverse Time Travel.
The last goes through mostly unchanged.

<div class="shaded" markdown="1">
### Quandle

Perhaps you remember that I set a personal guideline for "no more interactive puzzles", and think
it's strange that I was working on an interactive puzzle within a month.
Yeah, uh, I don't know what to tell you.

In my defense, as soon as "Quantum Wordle" entered my brain, I was convinced it would be a good
puzzle and that I had to make it.
I found an [open-source Wordle clone](https://github.com/cwackerfuss/react-wordle) and got to work figuring out how to modify it to support a quantum superposition
of target words. This took a while, since I started with the incorrect assumption that letters
in a guess are independent of each other. This isn't true. Suppose the Wordle is ENEMY, and you
guess the word LEVEE. The Wordle algorithm will color the first two Es yellow, and the last one
gray.
When extended in the quantum direction,
you can't determine the probability distribution of one E without considering the other Es. They're
already dependent on each other. (Grant Sanderson of 3Blue1Brown would [put out a video admitting to
a similar mistake](https://www.youtube.com/watch?v=fRed0Xmc2Wg) shortly after I realized my error, so at least I'm in good company.)

After I got the proof of concept setup, I considered how to do puzzle extraction. I considered trying
to have the extraction be based on finding all observations that forced exactly one reality, but after thinking
about it more, I realized it was incredibly constraining on the wordlist. This certainly wasn't a
mechanic I was going to figure out in time for potluck, so I went with an arbitrary order of words and
an arbitrary letter from each one, to give me flexibility to make whatever cluephrase I wanted. Making
that cluephrase point to specific words feels like the most interesting idea, and after a bit more
brainstorming, the superposition idea came out.

Internally, the way the puzzle works is that the game starts with 50 realities.
On each guess, the game computes the Wordle feedback for every target word, then averages the feedback
across all realities.
When making an observation, it repeats the calculation to find
every target word consistent with that observation, deletes all other realities, and recomputes
the probabilities for all prior guesses. Are there optimizations? Probably. Do you need to optimize
a 50 realities x 6 guesses x 5 letter problem? No, not really. This will become a running theme. For
Hunt, I optimized for speed of implementation over performance unless it became clear performance
was a bottleneck.

The puzzle could have shown 50 blanks, revealing each blank when you solved a word, but I deliberately
did not do that to make it harder to wheel-of-fortune the cluephrase.
During exploration, I generated random sets of 50 words, to get a feel for how the game
played.
My conclusion was
that 1 observation was too little information to reliably constrain to 1 reality, while 2 observations
gave much more information than needed. I considered making the word list more adversarial, but in my
opinion, the lesson
of Wordle is that it's more fun to give people more information than they need to win. People are not
information-maximizing agents [citation needed]. I left it as-is.

As one of the first tests of our Puzzup setup, I did a puzzle exchange with Brian. He tested Quandle and I tested [Parsley Garden](https://puzzlefactory.place/office/parsley-garden). Around 60 minutes into the Quandle test, I
ask how the puzzle is going. Brian says he's stuck, and after asking a bunch of questions, I figure
out that he's never clicked a guess after making one, meaning he's never seen the probability
distributions or used an observation. Oops. I added a prompt to suggest doing that, and the solve was better
from there.

After potluck, I asked for a five letter answer, but none were available. Instead I got an answer that
was two five letter words. Aside from the design changes needed to make that work, the rest of the puzzle
mechanics stayed the same, and the work later in the year was mostly figuring out how to share team state.

I've been told that technically, the quantum interpretation of Quandle is bad. I believe the core
issue is that you're not supposed to be able to observe the probability distribution of a letter before
observing the outcome. The distribution should immediately collapse to a fixed outcome as soon as you look at it,
and you certainly shouldn't be able to make an observation that collapses from one superposition of 50
realities to another superposition of < 50 realities. This is probably all true, and I don't care.
</div>


# March 2022

## Of Metas and MATEs

The internal puzzle potluck runs! It goes well. Editors tell me that there were multiple Wordle-themed
potluck ideas, the Hunt should only have one, and Quandle is the one they're going to go with. Hooray!
I apologize to the other puzzles in the Wordle graveyard.

Chat implementation for MATE and other AIs is coming along smoothly. teammate has multiple people with NLP experience,
including with large language models, and for this reason we immediately know we are not going to touch
those with a ten foot pole. We are going to stick to hardcoded chat responses, that trigger according
to a hardcoded chat interaction graph, where at most we do a bit of NLP to determine chat intents. As
a proof of concept, we use regex-based intents. Everyone involved with chat then gets busy, and we never
move past regexes. I do wish we'd used one of the lightweight ML libraries for chat intents, to reduce
the "sorry, I don't understand" replies, but I do think it was correct to deprioritize this.

Meta writing is also now in earnest. I mean, it was going ever since theme finalization, but now it's
*extra* going. Very approximately, these are the steps of writing Mystery Hunt puzzles.

1. Decide on a theme.
2. Figure out the major story beats that you want in the Hunt.
3. Ideally, your major story beats are tied to metapuzzles, since this connects the solving process
to the narrative. Those metapuzzles block on story development. However, a bunch of
metapuzzles are off that critical path. Think, say, [Lake Eerie](https://puzzles.mit.edu/2022/round/lake-eerie/) in Mystery Hunt 2022. Good round?
Absolutely! Was its answer critical to the
story of that Hunt? No, not in the way that [The Investigation](https://puzzles.mit.edu/2022/puzzle/the-investigation/) was.
4. When story is decided, start writing the metas for story-critical answers.
5. Once the major story beats are decided, meta proposals for story-critical answers can begin.
6. Whenever a meta finishes testsolving, release all its feeder answers.
7. When all your metas and feeders are done, your puzzles are done.

Mystery Hunt writing is very fundamentally an exercise in running out of time, so everything that
can be done in parallel *should* be done in parallel. Interestingly, for the Puzzle Factory, that
means the AI rounds were ideated first, because their answers were not story-critical, whereas
the Museum and Factory metas were. This was a good idea, since AI rounds were gimmicked
for story reasons, and gimmicked rounds take longer to design.

There weren't too many guidelines on AI round proposals. They had to have a gimmick, and their
final meta needed to be a feature request, but besides that anything went.
It turns out asking teammate to come up with crazy round ideas is pretty easy! In teammate parlance,
an "illegal" puzzle is a puzzle that breaks puzzle convention, and we like them a lot.
I think we ended up with around 15 proposals for 4 slots.

The difficult
part was doing the work to decide if an idea that sounded cool on paper would actually
work on closer inspection. One of my hobbies is Magic: the Gathering, and this issue comes up in custom
Magic card design all the time. Very often, someone will create a card that tells a joke, or makes a cute
reference, and it's cool to read. But if it were turned into a real card, the joke wouldn't convert into
fun gameplay.
Similarly, we needed to find the line between round gimmicks that could support interesting puzzles,
and round gimmicks that could not.

For example, one of my round proposals was a round where every puzzle was contained entirely in its title.
It would involve doing some incredibly illegal things, like "the puzzle title is an animated GIF"
or "the puzzle title changes whenever you refresh the page".
There was some interest, but as soon as we sat down to design the thing, we realized the problem was
that it was practically impossible to write the meta without designing the title for every feeder at the
same time. The gimmick forced way too many constraints way too fast. So, the proposal died in a few
hours, and as far as I'm concerned it should stay that way.

There was a time loop proposal, where the round would periodically reset itself, you'd unlock different
puzzles depending on what choices you made (what puzzles you solved), and the meta would be based
on engineering a "perfect run". This idea lost steam, which is really for the best.

In one brainstorming session, I off-handedly mentioned a [Machine of Death](https://en.wikipedia.org/wiki/This_Is_How_You_Die) short story I read long ago. In it, the brain scan
of a Chinese woman named 愛 is confused with the backup of an AI, since both files were named "ai".
I didn't think much of it at the time, but many people in that session went on to lead the Eye
round, and I'd like to think I had some tiny contribution to that round.

The main round I got involved with was "Inset", which you know as Wyrm. But we'll get to that later.

In one of our weekly general meetings, the plan for the Factory metas are announced. There will be
three metas that need to deliver these story beats.

1. There are multiple AIs.
2. teammate discarded all AIs except for MATE.
3. Remnants of the AIs are still causing strange things in the Mystery Hunt.

We split into groups to brainstorm the three Factory metas, which is where we came up with...

<div class="shaded" markdown="1">
## The Filing Cabinet

I'm not sure how people normally come up with meta puns. What I do is use [RhymeZone](https://www.rhymezone.com/)
to look up rhymes and near-rhymes, then bounce back and forth until something good comes out.
The brainstorm group I was in was focused on the "multiple AIs" story point. Looking for rhymes
on "multiple" and "mate", we found "penultimate".

At which point Patrick proclaimed, "Oh, this puzzle writes itself! We'll find a bunch of lists, give a thing
in each list, and extract using the penultimate letter of the penultimate thing each list."

And, in fact, the puzzle idea did write itself! Well, the idea did. The execution took a while to
hammer out. A rule of thumb
is that [there's a 10:1 ratio](https://www.ybrikman.com/writing/2018/08/12/the-10-to-1-rule-of-writing-and-programming/)
for raw materials to final product in creative endeavors, and that held true here too. The final puzzle
uses 16 feeders, and this was sourced from around 140 different lists.
Our aim was to balance out the categories used, which specifically meant not all music, not all
literature, not all TV, and not all things you'd consider a well-known list (like the eight planets).
Lists were further filtered down to interesting phrases that
ideally wouldn't need to be spelled letter by letter, while still uniquely identifying their list from
a single entry. The last point was the real killer of most lists. I liked [Ben Franklin's list
of 13 virtues](https://fs.blog/the-thirteen-virtues/), but the words ended up being too generic.

Despite having the entire world as reference
material, some letters (especially the Ps) were really difficult to find. I remember arguing
against SOLID YELLOW for a while, saying it was ambiguous between "green stripe" and "striped green"
no matter what Wikipedia said, but didn't find a good enough replacement in the 20 minutes I spent
looking for an alternative, and decided I didn't care enough to argue more.

I feel every puzzle author tries to shoehorn their personal interests into a puzzle, and this
was a fun exercise in trying to do so.
[FISH WHISPERER](https://vyletpony.bandcamp.com/album/can-openers-notebook-fish-whisperer) did not
make the cut, but I knew it had zero chance of clearing the notability bar.
[MY VERY BEST FRIEND](https://www.google.com/search?q=madoka+magica+episodes) was a funny answer
line that got bulldozed in the quest to fit at least one train station into the puzzle, which was
harder than you'd think. Not a great showing for stealth inserts, but I'm happy
[WAR STORIES](https://en.wikipedia.org/wiki/Firefly_(TV_series)) stuck around until the end.

Also, have a link to some [Santa's reindeer fanart and fanfiction](https://holidappy.com/holidays/The-Personalities-of-Santas-Reindeer). Testsolvers cited it as a source for "Olive is Santa's 10th reindeer",
a mondegreen from people who misheard the song as
"Olive the other reindeer used to laugh and call him names".

![A conversation about reindeer](/public/mh-2023/reindeer.png)
{: .centered }
</div>


# April 2022

## Round and Round and Round and Round!

![Wyrm](/public/mh-2023/wyrm_avatar5.png)
{: .centered }

It's Wyrm time!

Wyrm took *quite a while* to come together, but was started in earnest around April.
From the start, the round proposal was "really cool fractal art", and the design around it
was figuring out what an infinitely zooming fractal round could look like. This started with
the metameta.

<div class="shaded" markdown="1">
### Period of Wyrm

Really, I did not do much on this puzzle. The mechanics stayed the same throughout all
testsolves. My main contributions were helping on feeder search during round writing,
and writing a script to auto-search for equations that would give a desired period. I limited
the search to linear functions, which was good enough most of the time.

I learned a lot about how Mandelbrot periods work during this puzzle. Although we suspected that
writing code would be the way most teams solved this puzzle, we wanted the puzzle to support
non-coding solutions, given it was required to finish the Hunt. The way that Mandelbrot
set periods work is based on the "bulbs" along the outer border of the Mandelbrot set. The central
cardioid has period 1, and you can find any period by finding the right bulb connected to
the cardioid. Each bulb is self-similar to the original set, so instead of only going around
the central heart, you can use the bulb of a bulb. For example, to get a period of 6, you either
use a 6-bulb, or a 3-bulb branching off a 2-bulb, or a 2-bulb branching off a 3-bulb. Long story
short, composite periods are easier than prime ones.

Qualitatively, the period converges faster if the point is towards the middle of the bulb, so
we tried to do that when possible.
We also aimed to use the largest bulb per period to reduce precision needed to the solve the puzzle,
and spread the points across the border of the Mandelbrot set (a holdover from
an earlier version of the puzzle that hinted the Mandelbrot set less strongly).

The puzzle was very deliberately designed to be flexible enough for any answer, as long as we
had a good enough set of feeders.
</div>

The round structure went through multiple iterations, done over Jamboard. Here's a version where
every puzzle would be 1/4th of a future puzzle:

[Wyrm brainstorm](/public/mh-2023/Inset_Jam_6.png)
{: .centered }

Here's one where every puzzle's answer would depend on answers from the previous layer,
such that one puzzle could be backsolved per layer.

[Wyrm brainstorm 2](/public/mh-2023/Inset_Jam_3.png)
{: .centered }

And here's one where the entire round would be serial, each puzzle would rely on the
previous puzzle's answer, and you'd need to figure out how to bootstrap from nothing to solve
the entire round.

[Wyrm brainstorm 3](/public/mh-2023/inset_jam_line.png)
{: .centered }

Most of these ideas would have been quite tricky to pull off, especially given we needed to
fit it within the Period of Wyrm constraints. This led to the cyclic round structure proposal.
Each layer would be normal puzzles, building to a meta, which would then be 1 pre-solved feeder
for the next layer's meta. The rounds would then form a cycle, where the last meta would be a feeder
in the first layer. This restricted the "weirdness"
to just the metas of each layer, and all regular feeders in each layer could be written without
constraints besides the answer. The zoom direction of moving outwards rather than inwards was
done to make it more distinct from [⊥IW.giga](https://puzzles.mit.edu/2021/round/giga/).

[Wyrm last brainstrom](/public/mh-2023/Inset_Jam_2.png)
{: .centered }

Our first plan was to have one unsolvable puzzle in the first layer of puzzles, that would
become solvable once you got to the last layer of puzzles. This idea got discarded pretty
early because it didn't feel very impactful, it seemed hard to guarantee that a puzzle couldn't
be backsolved, and giving teams an unsolvable puzzle would be pretty rude. That led to the
road of creating a metapuzzle disguised as a feeder puzzle, solvable from 0 feeders but still
allowing for backsolving of feeders. Figuring out exactly what that meant would be a future problem.

Since I didn't have any leadership responsibilities, and tech was still on the slow side,
I ended up self-assigning myself a lot of work in brainstorming metas that fit the answer
constraints.
I noted that our Hunt had a lot of similarities to Mystery Hunt 2018:
a goal to reduce raw puzzle counts, but including complex meta structures in their stead.
As homework, I spent a lot of time reading through the solutions for
both the Sci-Fi round and Pokemon rounds from Mystery Hunt 2018, since they also had
overlapping constraints between metas and metametas. Going through each solution several times,
I started to appreciate some 2018 metas that I found dull at the time, but which made the round
construction possible when viewed through a constructor's lens.

You can read more about the Wyrm answer design process in an AMA reply I wrote [here](https://www.reddit.com/r/mysteryhunt/comments/10iq756/comment/j5gehcg/). The short version is that all metas
range between using answers semantically and using answers syntactically. The Period of Wyrm
metameta forced semantic constraints, and the first Wyrm meta written (Lost at Sea) also used
semantic constraints. This forced the remaining metas to be syntax based.

Over a few months, all the Wyrm metas were drafted and tested in parallel, using one central
coordination spreadsheet to track the metameta categories used. Around 60 different categories
were considered for the metameta, of which 13 were used, so more like a 5:1 ratio instead of
a 10:1 ratio. Feeders were constantly shuffled between metas as we found better answers that
satisfied the constraints, or changed meta designs to loosen their constraints enough
to make feeders work.

We knew early on that some categories would be fixed. The category that led to INCEPTION
was just too good as a "teaser" answer for the rest of the round, and got quickly locked in
as the answer to Wyrm's first layer. The FELLOWSHIP and EYE OF PROVIDENCE categories were
locked in early as well, to fit the meta they went towards. My favorite category that didn't
make it was "Socialist", for Social Security Numbers, using MONTGOMERY BURNS and TODD DAVIS.
It got cut because we decided TODD DAVIS was a bit ambiguous with an "Athlete" category we
were considering, and larger numbers would have forced awkward equations in the meta. Too bad,
the juxtaposition of two capitalists getting labeled "Socialists" would have been great.

The other category we wanted to force was Hausdorff, since at the time Period of Wyrm did not
have as strong of a hint towards Mandelbrot Set. There were no names, just occupations, and
the Mandelbrot Set needed to be inferred from the flavor and round structure. Thus, we wanted
as many context clues pointing to fractals as possible.

I factchecked that category, which was a fun time. I'll quote my despair directly.

> aw man why have so many recreational math people tried to estimate the dimension of brocolli and cauliflower
>
> [their] values are like +/- 0.2 the value from wikipedia
>
> but that value is based on some paper someone put on arxiv in 2008 with 4 citations

> put some notes in the sheet but in summary, of the real-world fractals, the most canonical ones are
>
> 1) the coastline based ones, because they were so lengthy that only 1 group of people really bothered estimating them.
>
> 2) "balls of crumpled paper", which is usually estimated at dimension 2.5 and I found a few different sites that repeat the same number (along with 1 site that didn't but the one that didn't was purely experimental whereas the wikipedia argument is a bit more principled)

When I checked deeper, I found that actually the coastline paradox is well-known enough that multiple
groups have checked the dimension of coastlines, getting different results, so those aren't canonical
either. The only one that was consistent was Great Britain, whose dimension of 1.25 is repeated in both
the original paper by Benoit Mandelbrot and all other online sources I could find.

In my experience, factchecking is the most underappreciated part of the puzzle writing process.
The aim of factchecking is to make sure that every clue in the puzzle is both true, and only
has one unique solution. And even with the solution, this can take a long time to verify, on par with
solving the puzzle forward.
Although Wikipedia is the most likely source for puzzle information, Wikipedia
isn't always correct, and it's important to verify all reasonable sources share consensus. You never
know what wild source a puzzler will use during Hunt.

(Sometimes, that consensus can be wrong and you still have to go with
it for the sake of solvability! See Author's Notes for
[Hibernating and Flying South](https://2021.galacticpuzzlehunt.com/puzzle/hibernating-and-flying-south)
from GPH 2022 for an example, or the Author's Notes for Museum Rules from Mystery Hunt 2023. It's
unfortunate to propagate falsehoods, but sometimes that's how it goes.)


## The First Bay Area Retreat

This year teammate was spread out, with rough hubs around the Bay Area,
Seattle, and New England. We held a meetup for Bay Area people, for people who felt the COVID exposure
potential was within their risk tolerance.

We started brainstorming Weaver at this retreat. Much of the work would be done later, but April was when
Brian first mentioned wanting to make an underwater basket weaving puzzle, using
special hydrochromic ink that dried white and became transparent when wet. The idea sounded super cool,
so we did some brainstorming around what the mechanics should be (different weaving patterns, presumably), as
well as some exploration into the costs. I then found an [Amazon review](https://www.amazon.com/gp/customer-reviews/R2HQQ7DWE56RY1/ref=cm_cr_dp_d_rvw_ttl?ie=UTF8&ASIN=B086Q344PQ).

> I've tried a bunch of hydrochromic paints and they're all kinda like this one. It's a fun idea in theory -- a paint that goes on white when dry and turns clear when wet, so you can reveal something fun on your shower tile or umbrella or sidewalk.
>
> But... it doesn't work great. It takes a pretty thick set of coats to actually hide (when dry) what's underneath, and that makes it prone to cracking, and also not entirely transparent (more like translucent) when wet. It's hard to get the thickness just right. Mixing some pigment into the hydrochromic helps a bit but adds a tint when wet. And even aside from all that it's not very durable paint, it's kind of powdery and scratches off. And you can't add a top-coat, otherwise the water won't get to it.
>
> You *can* make it work, we *did* make it work for a puzzle application (invitation cards that reveal a secret design when wet) but I'd prefer not to use it again.

The Amazon review was written by Daniel Egnor. For those who don't know, Dan Egnor runs [Puzzle Hunt Calendar](http://puzzlehuntcalendar.com/).
This was easily the most helpful Amazon review I've ever seen.

Unfortunately, it suggested our idea was dead in the water (pun intended).
This was super sad, but underwater basket weaving was too compelling to discard entirely, so Brian
ordered some to experiment with later.

We then shifted gears to writing puzzles for the newly released Factory meta answers, finishing
a draft of:

<div class="shaded" markdown="1">
### Broken Wheel

This is one of those puzzles generated entirely from the puzzle answer. There were a few
half-serious proposals about treating the answer as PSY CLONE and doing a Gangnam Style
shitpost, but they died after I said ["It's been done"](https://puzzles.mit.edu/2013/coinheist.com/feynman/sages_style/index.html).

Alright, what is a Psyclone? There are two amusement park rides named the Psyclone, one of which is a spinning ring.
How about a circular crossword that spins? That naturally led to the rotation mechanic.
There were some
concerns about constraints, but I cited [Remy](https://puzzles.mit.edu/2022/puzzle/remy/) from
Mystery Hunt 2022 to argue that it'd be okay to not check every square of the crossword.
The entire first draft was written in a few hours, since we had a lot of people and it was
very easy to construct in parallel. I guess that shouldn't be
surprising, since crosswords are easy to solve in parallel too.

Enumerations were added in the middle of the first testsolve because it was too hard to get
started without them. As for the final rotation, we went through many iterations of
flavortext and clue highlighting, before settling on placing the important clue first and
mentioning "Perhaps they can be rotated" directly in the flavortext. "Rotated"
in particular (over "spin" or "realigned") seemed to be the magic word that got testers thinking
about the right idea. It was a good reminder of how much subconscious processing people do
in puzzlehunts.
</div>

## "I Have a Conspiracy"

It is late April, and the round structure of Hunt is solidified.

* Five Museum rounds, that will combine into one metameta where both the metas and feeders are important.
* Three Factory rounds, one of which will be about solvers "creating their own round" (this would
later evolve into the Hall of Innovation)
* Four AI rounds, where the four AIs are locked in as Wyrm, Boötes, Eye, and Conjuri. These are the
four AI ideas that have the most partial progress. There wasn't a formal selection process for this,
it was more that team effort needed to be directed elsewhere.

The aim is to end up with around 150-160 puzzles. At planning time, teammate was around 50 people and
we thought we'd literally die if we tried to write a 190+ puzzle hunt. We still ended up doing a lot
of external recruiting partway through the year to get over the finish line - I'd guess the writing team
net grew by 25% between the start and the end, and this is accounting for people who dropped out due to
life reasons.

There were two lingering problems. One, the Wyrm round was significantly larger than all the other AI
rounds. Two, the story team was figuring out details of the midpoint capstone interaction. At the midpoint
of the Hunt,

1. Solvers should reactivate the old AIs.
2. This causes teammate to shutdown the Puzzle Factory and Mystery Hunt
3. Solvers should then start powering up the Factory by solving puzzles that were in the process of getting
created for Mystery Hunt.
4. Which should then be enough to cause the old AIs to wake up and start writing their own puzzles, letting
teams continue powering up the Factory until endgame.

The question is, what are the puzzles in step 3?

At one general meeting, the Wyrm round
authors and Museum metameta authors were gathered into a meeting with the editors-in-chief and
creative leads for a conspiracy: what if Act I feeders from the Museum repeated in Wyrm's round?

This proposal filled a lot of holes.

* Wyrm's round would be 6 puzzles shorter, bringing its size in line with other AI rounds. Between the
repeated puzzles and backsolved puzzles, there would be around 13 "real" puzzles total.
* The reused feeders could become the puzzles solved after shutdown. It would be reasonable for copies
of those feeders to be leftover in the Factory.
* The gimmick for Wyrm was in the structure, not the feeder answers. Out of the four AI rounds it was
most suited to repeated feeders.
* The overall hunt would require 6 fewer puzzles to write. Puzzle production was starting to fall
under the target trendline of all puzzles written by December, and reducing feeders was one way to
catch up.
* If we could make that set of feeders fit 4 meta constraints (Museum meta, Museum metameta, Wyrm
meta, Wyrm metameta), it'd be really cool.

Making this happen would be quite hard. The first step was a testsolve of the Museum metameta, so that we knew
what we'd be signing up for. After getting spoiled on MATE's META. we discussed whether this was
ambitious-but-doable, or too ambitious. It seemed very close to too ambitious, but we decided to go for it
with a backup option of reversing the decision if it ended up being impossible.

It made it to Hunt, so we *did* pull it off. I'm happy about that, but given a do-over chance I would
have argued against this more strongly.
First of all, I don't think many solvers really noticed the overlapping constraints. It leaned too
hard towards "showing you can solve an interesting design problem" without a big enough "fun" or
"wow" payoff. (In contrast, the gimmicks of the AI rounds are much more obvious and easy to appreciate.)

The more problematic issue that was not clear until later was the way it delayed feeder release.
Here is the rough state of Hunt by late April.

* The Office meta is done and its feeders are released.
* The Basement meta will go through more testsolving when the final art assets are in, but is essentially
  finalized and its feeders are released.
* All AI rounds are in the middle of design and are not ready to release feeders.
* Innovation and Factory Floor is doing its own crazy thing, and won't be ready for some time.

In short, there were 2 rounds of feeders open for writing, and every other round
was not. It is already known that Bootes and Eye will have answer gimmicks that make their puzzles harder to
write, and Conjuri feeders will likely be released quite late since the meta relies on developing and
designing the game.

The status quo is that all the Museum feeders can't be released until five Museum metas pass testsolving,
and all the Wyrm feeders can't be released until four Wyrm metas pass testsolving. Including retests of
the metameta, this is 6 metas blocking Museum and 5 metas blocking Wyrm. Repeating feeders between Musem
and Wyrm literally turned it up to 11 metas blocking both sets of feeders. There were 53 feeders in that
pool, about 40% of the feeders in the whole Hunt.

Most of the non-gimmicked feeders were in that
pool as well, leaving fewer slots for people who just wanted to write a normal puzzle. I don't have
any numbers on whether teammate writers were more interested in writing regular puzzles or gimmicked
puzzles, but I suspect most of the newer writers wanted to write puzzles with regular feeders, and did not have
as much to do while the metas were getting worked out.

I'd estimate that the extra design constraints delayed the release of that pool of 53 feeders by 2-4 weeks.
It forced more work on Museum meta designers who already needed to fit their meta pun and
feeders into the metameta mechanic.
Perhaps in a more typical Hunt, this would have been fine, but the AI rounds had already spent
a lot of complexity budget and this probably put us in complexity debt.

But, this is all said with hindsight. At the time, I did not realize the consequences and I'm
not sure anyone else did either. It did genuinely fix problems in the Hunt and story structure,
it's just there were other ways to fix them that would have made things easier.

<div class="shaded" markdown-"1">
### The Legend

Part of the deal for accepting the repeated feeders constraint was editors-in-chief signing up to
help design and push the relevant metas. The Legend was the meta brainstormed to take all the
repeated feeders, and needed to be designed to take pretty much any answer.

People say "restrictions breed creativity". That's true, but what they don't say is that meeting those
restrictions is not necessarily *fun*. It's work. Rewarding and interesting work, but still work.

Before
the decision to repeat feeders, I had sketched some ideas around using the Sierpinski triangle, after
noticing INCEPTION was $$9 = 3^2$$ letters long. The shape is most commonly associated with
Zelda in pop culture, so ideas naturally flowed that way.

![Prototype Legend triangle](/public/mh-2023/triangle.svg)
{: .centered }

The early prototype associated one feeder per triangle, extracting letters via Zelda lore. It
was reference heavy and not too satisfying.
After talking with Patrick a bit, he proposed turning it into a logic puzzle, by scaling up to
27 triangles, giving letters directly, and having feeders appear as paths in the fractal.

This was especially
appealing because it meant we could take almost any feeders, as long as their total length
was around 60-70 letters. Brian was spoiled on some of the Museum metas
mentioned that TRIFORCE was a plausible answer for both the Museum and Wyrm metameta, so
if we could make the Sierpinski idea work, we could do a "triangle shitpost" by making TRIFORCE
the looping answer for the round.

Cool! One small problem: I've never written a logic puzzle in my life.

There are, broadly, two approaches to writing a logic puzzle.

1. Start with an empty grid and an idea for the key logical steps you want the puzzle to use.
Place a small number of given clues, then solve the logic puzzle forward
until you can't make any more deductions. Add the given clue you wish you had make progress,
then solve forward again. Repeat until you've filled the entire grid. Then remove everything
except the givens you placed along the way, and check it solves correctly.
2. Implement the rules of the logic puzzle in code, and computer generate a solution.

Option 1 tends to be favored by logic puzzle fans. By starting from an empty grid, you
essentially create the solve path as you go, and this makes it easier to design cool a-has.

Option 2 makes it way easier to mass produce puzzles if, like, you're running a newspaper and want
to have one Sudoku in every issue. Puzzle snobs may call this "computer generated crap" because
after you've done a few computer generated puzzles, you are usually going through the motions.

I knew I was going to eventually want a solver to verify uniqueness. So I went with option 2.

I have some familiarity with writing logic puzzle solvers in [Z3](https://github.com/Z3Prover/z3),
since I like starting logic puzzles but am quite bad at finishing them. My plan was to use
[grilops](https://github.com/obijywk/grilops), but I found it didn't support the custom grid shapes
I wanted. Instead, I referred to the grilops implementation for how to encode path constraints, then
wrote it myself.

Figuring out how to represent a Sierpinski triangle grid in code was a bit of a trip. The solution
I ended up at was pretty cool.

* A Sierpinski triangle of size 1 is a single triangle with points $$0, 1, 2$$.
* A Sierpinski triangle of size 2 is three triangles with points $$(0,0), (0,1), (0,2), (1,0), (1,1)$$, and so
on up to $$(2,2)$$.
* A Sierpinski triangle of size N is three Sierpinski triangles of size N-1. Points are elements of
$$\{0,1,2\}^N$$. The first entry decides which N-1 triangle you recurse into and the rest
describe your position in the smaller triangle.

![Coordinate diagram](/public/mh-2023/sierpinski_coord.png)
{: .centered }

Checking if two points are neighbors can then also be checked recursively.

* If their first entries are the same, chop off the first entry and recursively check if the points
are neighbors in the triangle one level smaller.
* If their first entries differ, they are in different top-level triangles, and the only three cases are $$0111-1110$$, $$0222-2000$$, and
$$1222-2111$$.

![Adjacency diagram](/public/mh-2023/triangle_neighbor.svg)
{: .centered }

The first draft of my code took 3 hours to generate a puzzle, and the uniqueness check failed
to finish when left overnight. Still, when I sent it to editors, they were able to find the same
solution my code did by hand,
so we sent it to testsolving to get early feedback while I worked on improving my solver.

Testsolving went well. The first testsolve took pretty much exactly as long as we wanted it to
(2 hours with 5/6 feeders), and solvers were able to use the Sierpinski structure to logic out
deductions that combined into the final grid. Not too bad for a computer generated
puzzle! This was very much a case of "getting lucky", where we discovered a logic puzzle format
constrained enough that the solve path naturally felt a bit like a designed one.

I tried alternate means of encoding the constraint that every small triangle had to come from
the given set of 27, and every triangle needed to be used exactly once. Swapping some ugly
and-or clauses into if-else clauses got Z3 to generate fills 10x faster, but when switched to solve
mode, it still failed to verify uniqueness.

At this point I decided to step in and mess around with different fills by hand. The goal was
to minimize the number of triangles where 2+ letters were contributing to extraction. There were around
5 different fills with different feeder lists, and I suspect all of them were unique,
but my verifier was only able to halt on one of them. I didn't have much intuition for how to speed
up the solver any more, so we stuck with that fill.

After that fill was found, there were only two revisions. The first was deciding how much hinting to give
towards the Sierpinski triangle. This was the step with the largest leap of faith. In the
end we decided to hint that all triangles should point upwards, and the final shape should
be triangular, but no more than that.

The second isn't really a revision, it was more logistics. In Mystery Hunt 2020, teammate got stuck on
the final penny puzzle for 7 hours. This was quite painful, given that we had literally no other relevant
puzzles to do, but at the end of Hunt we liked that we ended up with a bunch of small souvenirs that people could
take home. We had already observed some struggles with spreadsheeting the triangle grid, and since
The Legend was so close to the story midpoint, it seemed cool if we could give out physical triangles.
They could serve double duty as puzzle aid and puzzle souvenir.

I'm not sure if teams used the wooden triangles as a souvenir in the way we imagined, but I hope
shuffling wooden triangles was more fun than manipulating spreadsheets!
</div>


# May 2022

## The Triangles Will Continue Until Morale Improves

The co-development of Museum and Wyrm metas is fully underway. The editors-in-chief have created a
"bigram marketplace" spreadsheet, listing every bigram that MATE's META needs, along with a guess
of expected bigram extraction mechanisms. Based on Museum meta drafts so far, editors ranked how
tight their constraints were, and gave more constrained metas higher priority. The repeated feeders
for Wyrm that we wanted in The Legend then got higher priority on top of that. Wyrm authors were
directed towards testsolves of the Museum metas, so that we could help brainstorm alternate
answers that were compatible with Wyrm constraints. Meanwhile, testsolves of The Legend
were often biased towards Museum meta authors, so that they could know a bit about what was going on.

I helped field questions from Museum meta authors. Yes, someone needs to take the answer GEOMETRIC SNOW, we
know it's not a great answer but haven't found an alternative that fits the double O constraint. Yes,
TRIFORCE can't change.
To help aid in feeder search, I write a script that attempts all Museum metameta bigram mechanics, then feed
as many Wyrm feeder ideas I can think of into the script to see what sticks. Eventually this converges
to a set of repeat answers that is a bit greedy at taking good bigrams, but not too greedy.

As the bigram marketplace settle down and The Legend feeders get more locked in, we start pivoting to
dealing with our own constraints.

<div class="shaded" markdown="1">
### The Scheme

From the start of writing, we knew the meta answer was locked to EYE OF PROVIDENCE and it had to
use the feeder INCEPTION in some way. There was a bit of brainstorming on whether we could
exploit that teams would 100% have the INCEPTION answer, but we did not come up with anything good.

The first serious idea I had for The Scheme was one I liked a lot. We weren't able to make the design
work, but I'm not willing to give up on the idea, so I won't reveal it.

After that idea fell apart, we noticed that the Eye of Providence was a triangle, and we had triangles
in The Legend, so why don't we try to extend the triangle theming into this meta? It was a bit of a
meme, but it would be cool...

When researching constrained metas, I found
[Voltaik Bio-Electric Cell](http://puzzles.mit.edu/2018/full/puzzle/voltaik_bio_electric_cell.html),
a triangular meta with a hefty shell that used the lengths of its feeders as a constraints.
Well, we already
had a 1 letter word in one of our feeders (at the time, it was V FOR VENDETTA). There was no length
constraint on our feeders yet (meaning it would be easier to fit), and
It seemed plausible we could make a full triangle out of words in our feeders.

We did a search for missing feeder lengths, tossed together a version that picked letters out of the triangle with
indices, and sent it for editor review. I was fully expecting it to get rejected, but to my surprise,
editors liked the elegance of the word triangle, and thought it was neat enough to try testsolving.
(Perhaps the more accurate statement is that the editors knew the difficulty of the Wyrm constraints
were, most Wyrm metas we'd proposed needed shell, and this was closest we'd gotten to a pure meta for Wyrm
so far.)

The spiral index order was originally added because I was concerned a team could cheese the puzzle by
taking the indices of all 6! orderings of the feeders. Doing so wouldn't give the answer, but it seemed possible
you'd get out some readable partials that could be cobbled together.
With hindsight, I don't think it was possible to get anything out of bruteforcing,
but one of our testsolve groups did attempt the brute force, so we were right to think about it.
We kept the spiral in the final version because it let the arrow diagram serve two purposes: the ordering of
the numbers, and a hint towards the shape to create.

![Diagram from The Scheme](/public/mh-2023/scheme.svg)
{: .centered }

In Puzzup, there were a list of tags we could assign to puzzles. This was to help editors gauge the puzzle
balance across the Hunt. This puzzle got tagged as "Australian". When I asked what it meant, I was told
it's shorthand for "a minimalistic puzzle,
where before you have the key idea there's little to do, and after that idea you're essentially done."
Puzzles like this tended to appear in the Australian puzzlehunts that used to
show up every year (CiSRA / SUMS / MUMS), and are a bit hit-of-miss. Usually, they hit if you get the idea
and miss if you don't.

One of the tricky parts of such puzzles is that you get exceptionally
few knobs to tweak difficulty. This is a challenge of pure metas in general.
The other tricky part of Australian puzzles is that solve time can have incredible variance. The first
test got the key idea in 20 minutes. The second test got horribly stuck and was given four
different flavortext + diagram combinations before getting the idea 3.5 hours later.
The second group did mention the meta answer 2.5 hours into their solve, when despairing about their
increasingly bad triangle conspiracies. It was the best dramatic irony I've ever seen.

![Conspiracies](/public/mh-2023/eyeofprovidence.png)
{: .centered }

Even in batch testing, where teams solved The Scheme right after The Legend, none of our testsolves considered
using the arrangement from The Legend when solving this puzzle. A few teams got caught on this during the Hunt -
sorry about that! The fact that The Legend triangle had an outer perimeter of 45 was a complete coincidence,
and if we'd discovered that early enough it would have been easy to swap BRITAIN / SEA OF DECAY back to GREAT BRITAIN / SEA OF CORRUPTION to make the Scheme triangle 55 letters instead.

Maybe having an Australian puzzle as a bottlenecking meta was a bad idea. A high variance puzzle naturally means
some teams *will* get it immediately and some teams *will* get walled, and getting walled on a bottleneck is a sad time. This is
on my shortlist of "puzzles I'd redo from scratch" with hindsight, but at the time we decided to ship it so we could move on to other work. ["You get one AREPO per puzzle"](https://docs.google.com/presentation/d/166MkkDmij_4_XcA8JP8JcSztVPjHJeibDJxr9EUMEv4/edit#slide=id.p) - I'd say this is the one AREPO of the Wyrm metas.
</div>


<div class="shaded" markdown="1">
### Lost at Sea

Nominally, I'm an author on this puzzle. In practice I did not do very much. The first version provided the
cycle of ships directly, and tested okay (albeit with some grumbling about indexing with digits).
By the time I joined, the work left to do was finding a way to fit
in triangles, and finding answers suitable for the metameta. We were basically required to have triangles
*somewhere* after The Scheme evolved the way it did.

I looked a lot into the Bermuda triangle, as did others, but none of us found reasonable puzzle fodder.
So instead, we looked
into triangular grids. Over a few rounds of iteration, the puzzle evolved from a given cycle into a
triangular [Yajilin](https://en.wikipedia.org/wiki/Yajilin) puzzle where feeders were written into the grid and
the Yajilin solution would give a cluephrase towards the rest of the puzzle and extraction.

It all sorta worked, but the steps were a bit disconnected, the design was getting unwieldly,
and we had a really hard time finding a way to
clue all the mechanics properly in the flavortext. There were lots of facts about each ship that could
be relevant, so the longer we made our flavortext, the more rabbit holes testsolvers considered.
(It's the natural response: if you get more data, then maybe you need to research more data to solve.)

My main contribution was suggesting we try removing the Yajilin entirely and brainstorm a different
way to use triangular grids.
The final version of the puzzle is the result of that brainstorm. I'm pretty happy with the way the puzzle
guides towards the hull classification, self-confirms it with the ARB classification code being weird,
and then leaves the classification number suspiciously unused if you haven't figured out it's
important yet.

The other main contribution was helping find feeders. The first draft used MRS UNDERWOOD as a feeder, for
indexing reasons. The answer worked, but was *really* not a good answer.
The S in MRS was needed for extract, but it looked sooooooo much like
a cluephrase for Claire Underwood. Brian told me that it could be changed to just UNDERWOOD if we added
a feeder that clued San Francisco, which fit the metameta, and had an S as its 5th letter. We collectively spent 5-10 hours
doing a search for one, before landing on the "stories" idea with SALESFORCE TOWER or
TRANSAMERICA PYRAMID. The puzzle was rewritten with SALESFORCE TOWER, went through an entire testsolve
with zero issues,
and then we learned that [actually there are multiple Salesforce Towers](https://www.salesforce.com/company/ohana-floors/salesforce-tower-atlanta/) in literally the final runthrough of all metas and the metameta. Oops.
Thank goodness we had a backup answer!
</div>


# June 2022

## My Favorite Part of Collage Was When Teams Said "It's Collaging Time" and Collaged All Over The Place

Wyrm feeders are *almost* ready for release. The Wyrm metas work. The shared feeders with Museum works. It
all works, except for two action items:

1. Write the 4th Wyrm meta.
2. Do a batch solve of the entire round to get data on what it's like to solve each meta sequentially.

We had meetings about the 4th Wyrm meta starting all the way back in March.
The design requirements were pretty tight:

* The puzzle had to look like an regular Act 1 puzzle.
* It also needed to be interpretable as a metapuzzle.
* Feeders had to be used in enough of way to allow for backsolving.
* At the same time, the puzzle had to be solvable without knowing any of those feeders.

The very first example for what this could look like was a [printer's devilry](https://en.wikipedia.org/wiki/Printer%27s_Devilry) style puzzle. Instead of each clue solving to the inserted word, inserting a word would complete a crossword clue with its own answer that you'd index from. The inserted words would then be unused
information that would be revealed as the backsolved feeders once you got to the end of the round.

None of the authors were very excited by this, but it was important to prove to
ourselves and editors that the design problem was solvable.
Once we knew the meta answer was TRIFORCE, the idea died more officially, since TRIFORCE was 8 letters
and we only had 6 feeders to work with.

Another idea proposed for this puzzle may have worked, but was basically impossible to combine with the
Wyrm metameta constraints. That idea eventually turned into
[Word Press](https://puzzlefactory.place/factory-floor/word-press).

The key design challenge was that the puzzle needed to embed some process for backsolving. But, that backsolving
process would look like unused information during the initial forward solve. If a team got stuck, they
could rabbit hole on that unused information.
The round structure of Wyrm only really *worked* if almost all teams solved the 4th Wyrm meta at the start
of Wyrm, so add two more constraints:

* The puzzle should be very easy.
* The puzzle should be attractive enough that teams won't skip it.

And so, the ideas bounced back forth for another few weeks, until we landed on the word web idea in late
April. I immediately started advocating for it. Word webs
are the closest thing to guaranteed fun for puzzlehunts, and were a highlight of the now-defunct Google
Games puzzlehunts.
It solved our "solve from 0 feeders" problem, because
spamming guesses and solving around hard nodes is just what you're supposed to do in a word web.
Teams would also be unlikely to get stuck in the word web, as long as we gave it enough redundancy, so
it would avoid the risk of teams paying too much attention to the content seeded for Wyrm.

My main worry was that it'd take a while to construct.
It was clear we *could* construct it though, so we tentatively locked it in. We punted on doing the construction
itself until the other Wyrm metas were written, so that we'd know exactly what backsolve feeders we needed to
seed in the web. Well, now those Wyrm metas were written.
Time to reap what we'd sown.


<div class="shaded" markdown="1">
### Collage

The first thing I did was reach out to David and Ivan, the authors of [Word Wide Web](https://2020.teammatehunt.com/puzzles/word-wide-web), to see if they had any word web tools I could borrow.
David sent me a D3.js based HTML page that
automatically layed out a given graph, with some drag-drop functionality to adjust node positions,
but told me there was no existing interactive code due to time reasons.

I repurposed that code to create a proof-of-concept interactive version that ran locally. This quickly
exposed some important UX things to support, like showing past guesses and allowing for some
alternate spellings of each answer..

![Prototype of filled out web](/public/mh-2023/webprototype1.png)
{: .centered }

![Prototype of solvable web](/public/mh-2023/webprototype2.png)
{: .centered }

For unlocks, I decided to recompute the entire web state whenever the list of solved words changed.
This wasn't the most efficient, it could have been done incrementally, but in general I believe
people underestimate how fast computers can be. Programmers will see something that looks like
a software interview question, and get nerd sniped into solving that algorithms problem, while neglecting
to fix their page loading a 7 MB image file it doesn't need to load. (If anything, the easier
it is to recognize something is an algorithms question, the less likely it is to matter. The hard
parts are usually in system design.)

In short, I figured recomputing the entire graph would be more robust,
and didn't want to deal with errors caused by messing up incremental updates.

![Advice on building the most robust thing](/public/mh-2023/braid.png)
{: .centered }

(From ["The Implementation of Rewind in Braid"](https://www.youtube.com/watch?v=8dinUbg2h70))
{: .centered }

From here, I worked on creating a pipeline that could convert Google Sheets into the web layout code. The
goal was to make tech literacy not be a blocker for making edits to the word web, and to make it easier
to collaborate on the web creation.
I added a bunch of deduplication and data filtering in my code to allow the source-of-truth spreadsheet
to be as messy as it wanted, which paid off. Pretty sure around 10% of the edges appear twice in the
raw data.

![Web spreadsheet](/public/mh-2023/webspreadsheet.png)
{: .centered }

The process of coming up with the words themselves took a while. Collage is the first time I've
ever made a word web, but my assumption going in was that word webs are best when there's a
high density of edges and vertices have large average degree. To get a sense for how big the web needed
to be, I took the [Black Widow](https://puzzlepotluck.com/3/14) web from Puzzle Potluck 3 and collected
stats on connected that web was.
I then referred to various metrics to check how the density of the current Collage draft compared.
I gave very serious though to looking for my ancient [spectral graph
theory](https://en.wikipedia.org/wiki/Spectral_graph_theory) notes from college to estimate if the web was an
[expander graph](https://en.wikipedia.org/wiki/Expander_graph), but decided that was overkill.

There was an initial "expansion" phase, where starting from the backsolve feeders, I put down literally
anything I could think. Other authors did the same. Then, after the web grew enough to
have some collisions occur naturally, there was a "contraction" phase where I trimmed nodes that were
hard to connect to the rest of the web, and future brainstorming was directed towards reducing leaf nodes
in the graph. I'd estimate about 90% of the web is from me and 10% is from other people.
This is why there's so much My Little Pony and MLP-adjacent material in the web. I apologize for nothing.

After the prototype was tested a bit, it was time to get it into the site for real. I wrote some awful code
to save and load node positions from the D3.js prototype, then set to work figuring out how to do
teamwide live updates. This was my first time implementing websockets for a puzzle.
(Websockets are a common approach for creating a persistent connection between user and server, where either
the user or server can send messages to trigger events. We used them for solve notifications, the AI
chats, and anything else where an update needed to trigger even if the solver isn't doing anything.)

This was also my first time using D3.js, which made this puzzle have a lot of "firsts" for me. That contributed
to how long this puzzle took to construct...but I knew I wanted to use websockets whenever I got to
converting Quandle, and I wanted to use zoom from D3.js for the Wyrmhole round art, so I treated it
as an investment.

![Surprise tool meme](/public/mh-2023/surprise.png)
{: .centered }

Thanks to us reusing tph-site, it was easy for me to find websocket code examples from Teammate Hunt 2021.
We had gone through a lot of pain setting it up initially, so the end API I had to deal with was not
as bad as I feared.

There was some danger in relying on such an interactive puzzle for a key part of Hunt. Websockets add
complexity, and widen the set of things-that-can-go-wrong in the Hunt site. But, given that we were going
to rely on websockets for chats with MATE, making Collage rely on them as well wasn't adding any additional risk.
MATE and the Wyrm metas were going to live together or die together, and we were going to bet on "live".

Polishing and factchecking this puzzle was a nightmare.
It turns out graph layout is a really hard unsolved problem. Even after turning the D3.js force graph parameters a bunch, I needed to do several adjustments
by hand to clean up collinear points and reduce overlaps. The manual cleanup pass still missed some collinearity
The click-to-highlight feature was added when I recognized that getting to 0 overlaps would basically
be impossible, to provide a stopgap for reading the web.
As for factchecking, the graph has over 300 words, and I knew there was no way we were going to
exhaustively verify all O(N^2) pairs of those words.
I think we got most of them, but I know a few slipped through (somehow no one noticed that
"the princess bride" wasn't connected to "bride").

Across both testsolves, no one suspected the puzzle was seeding puzzle content, which I was happy about.
Thanks to Patrick for suggesting a hardcoded threshold of 90% to solve the puzzle, rather than allowing
the puzzle to be solved whenever all 3 neighbors of the goal node were revealed, since the latter usually
happened by the 20% mark.
</div>

With Collage done, every Wyrm meta had gone through at least one clean testsolve, albeit with different
feeders. That left doing one last batch test of all the Wyrm metas and metametas, with the finalized feeders
for each.
We debated back and forth on whether the batch test should testsolve the backsolving step of
Collage during this test. Given that the testsolve was advertised as a "test of the Wyrm metas", it would
be basically impossible to fit Collage into the test without arousing suspicion. The only test of Collage
that seemed
like it'd
accurately model the real Hunt would be having the round page with prototype art, and having testsolvers
test many random feeder puzzles, including Collage, before they tested the Wyrm metas. Those random puzzles didn't
exist yet, because they were waiting on feeder release. Which was waiting on testing the metas. Which really
wanted random feeder puzzles to exist. Which didn't because the metas weren't done. Which...you get the idea.

Who knew making an ouroboros-style round would make it hard to find a starting point?

There were strong arguments both ways, and the decision was to not test the gimmick. We would describe the
backsolve step at the time the testsolve group unlocked the metameta, and we would push the backsolve
testsolve to a much later hypothetical full hunt testsolve.

The batch test group tested the Wyrm metas in sequence, starting with 4 feeders each and occasionally getting more
if needed, and when they got to the metameta, we gave them this diagram:

![Wyrm structure](/public/mh-2023/wyrmstructure.png)
{: .centered }

along with the backsolve feeders they would have had.
We did redact the title of Collage to something else, because there was no reason to reveal which puzzle
had the seeded content.

One last bit of trivia: the four Wyrm metas used to be named "The Legend", "The Scheme",
"The Sea", and "The Collage". We ended up dropping the "The".

<iframe width="560" height="315" src="https://www.youtube.com/embed/PEgk2v6KntY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

The concern was that the pattern in the titles was *too* strong. With all the other context clues, if a
team figured out that the round would loop, they could potentially solve the 3rd Wyrm meta imemdiately
on unlock. We kept the pattern in mind as a potential nerf to make down the line.

(Two teams told me after Hunt that they suspected the Wyrmhole round would loop the whole time, but neither
believed in it enough to attempt the cheese we were worried about. It's always weird to talk about
puzzle skips that people didn't do. Sometimes I wonder if I shouldn't mention them so that future
puzzle design is easier.)

The batch test went well, and we were able to release all the Wyrm feeder answers.
The Museum metas finished shortly after, and this officially opened the floodgates for regular writing.

With this resolved, the other major announcement was the plan for a teamwide in-person writing retreat.
The goal was to get together, test drafts of physical
puzzles and events, and start brainstorming AI round puzzles with very constrained feeders (like Bootes
and Eye). There would also be a fullhunt testsolve of the start of the Museum. The retreat was scheduled
for August.

However, the true goal of the in-person retreat was a more closely-held secret. And I was in the thick
of that conspiracy.


# July 2022

## It's Breakout O'Clock

"Breakout" was our internal name for what would eventually become the Loading Puzzle. It was the
moment when teams would discover the Puzzle Factory hiding behind the Museum of Interesting Things.
It was the goal we wanted all teams to see, and the key introduction to the entire story that would
unfold over MLK weekend.

And 80% of the team was spoiled on its existence by February.

This is always the risk you take with structure-level gimmicks. The transition to discovering the
Puzzle Factory was a key part of the theme proposal, which was heavily discussed early in the year.
How do you testsolve the discovery of something that you already know exists?

In general, it is okay for people to testsolve puzzles even if they know the answer to it already.
It's pretty easy to compartmentalize that knowledge. But for a gimmick this out there, we wanted
as accurate a test as possible.

The traditional answer to this problem is to recruit external testsolves. We tested the Teammate Hunt
2021 gimmick on a testsolve
team from Galactic, and Galactic tested the gimmick of Galactic Puzzle Hunt 2022 on some people from
teammate. This is a lot harder to do for Mystery Hunt though.

For Hunt, teammate did a lot of external recruiting.
To get a sense of scale, we won hunt with about 60 people, not all of them decided to write, and we ended
the year with 70 people on the credits page.
My estimate is that around 30% of people joined after theme selection.

Any recruited person that joined after theme discussion was not spoiled on breakout. Although the Museum
fullhunt testsolve would use real puzzles, the true goal was to testsolve breakout on that group of 30%.
After that testsolve, we'd treat the existence of breakout as not-a-spoiler and discuss it freely.

However, to make this plan work, the breakout would need to exist first.


<div class="shaded" markdown="1">
### Loading Puzzle...

During one of the weekly general meetings, I was asked if I could join a brainstorm group for "a small puzzle".
Said brainstorm group had Jacqui (creative lead) and Ivan (tech lead).

Having been in the trenches with Jacqui and Ivan on many an occasion for tech-heavy puzzlehunt
story integrations,
I was 80% sure this "small puzzle" was breakout, and 100% sure it would be small to solve
but take a stupidly long time to make. I didn't mention my suspicions to anyone, I just said "sure", but I'd
like it on the record that I was right on both counts (and agreed to help anyways because it sounded
like it needed help). I joined the breakout group in June and we didn't finish until basically the
day before the testsolve at retreat.

Over the course of those three months, we brainstormed how to create a puzzle that looked innocent at
first glance, but became more suspicious the more you looked at it.
A common theme in brainstorming was "peeling back the facade". Those of you who have played a JRPG video
game should know the feeling of talking to NPCs until you see all the dialogue, or trying to run over a
waist-high fence and running into an invisible wall. We wanted something in that vein, that rewarded
exploring the contours of the hunt site, and surprising you when the contours broke instead of holding
firm.

Many of the early ideas from this were unused, but got repurposed for the final clickaround of the hunt,
MATE's TEAM. For breakout, we converged towards something like a loading animation. The loading
animation would represent MATE getting increasingly overworked trying to create puzzles, diegetically
getting longer as teams solved puzzles and forced MATE to write new ones, and non-diegetically
scaling up with solve progress to increase its obviousness and encourage teams to look more closely over
time. The loading animation would appear on every puzzle.

As for the details? Boy was there a lot to figure out. Here's a screenshot of the brainstorm spreadsheet.

![Breakout brainstorm](/public/mh-2023/breakoutplanning.png)
{: .centered }

There was a fairly serious proposal that the "hole" between the Museum and Factory would only exist on the puzzle page where
you first solved the loading animation. The hole that appeared on the puzzle page would stay there,
and although you could visit the Puzzle Factory URL directly, in-site navgiation would rely on
visiting the puzzle where you first solved the animation. I liked this a lot, but we dropped it due
to complexity concerns.

Early prototypes were based on an infinite conveyor belt of pieces, except there would only be a few
different ones. Screenshotting and assembling the pieces would spell out the answer, and entering it
would cause the conveyor belt to breakdown. At the time, the answer was TERMINATE, due to its similarity
to TEAMMATE.

![Early breakout version](/public/mh-2023/breakout_old.png)
{: .centered }

Some early tests of this rated it as "okay but not exciting, a bit too obvious when letters exist
on the pieces".
This was later changed to the version that went in the Hunt, which avoided showing letters directly
by making the letters drawn in paths. Here is the original version we shipped to the August retreat.

<div class="centered">
<video width="640" controls loop autoplay muted>
    <source src="/public/mh-2023/breakout_old_video.mov">
    Your browser does not support .mov files.
</video>
</div>

Teammate Hunts have always had a rule that solvers can look at source code if they want, and Mystery Hunt
usually has a similar one. We have a lot of trust in our ability to hide things from the client.
For the loading animation, it meant the animation was drawn using CSS rather than a video file, with
animation info coming from the server. This way solvers could not right-click -> "Download Video" and solve the animation offline.

Our testing process for this was pretty goofy. We would take groups of people spoiled on the
existence of breakout, and tell them that we wanted to test the unlock structure and backend of the
hunt website. The hunt site had many fake puzzles that would ask you to wait for 60 seconds, then
give you the answer.

![Fake puzzles](/public/mh-2023/fakepuzzles.png)
{: .centered }

This testsolved correctly in isolation, but we knew the real test would be at retreat.

Meanwhile, we were working on the second part of the puzzle that would appear after the loading
screen. There are a few principles that guide teammate's approach to storytelling. I don't want to
put words in the mouth of the story team, but in my opinion the key points are:

* Decide on the story you want to tell, then make sure as much of the Hunt as possible echos and
is consistent with that story. This will take a long time to polish - do so anyways.
* Tie changes in narrative or story state to actions the solvers take. Usually this means attaching
it to puzzle solves.
* Use bottlenecks to direct team attention towards the same point, then put the most
important story revelations at those bottlenecks.

The breakout puzzle acts the bottleneck into the Puzzle Factory, and we wanted to
avoid one rogue team member solving the puzzle, finding the Factory, and leaving the rest of
the team in confusion about what was going on.

Our solution was to have the second part of the
puzzle be a "teamwork time" puzzle (borrowing a term from MIT Mystery Hunt 2020), where collaborating
with others would make it go faster. This evolved into the collaborative jigsaw seen during
Hunt. Each user was required to enter STOP on the loading
animation individually before they could access the jigsaw puzzle's URL, to force a flow where
early solvers would need to tell others how to break out of the Museum if they wanted to have
collaborators or share what was going on.

The collaborative jigsaw married three of the worst parts of frontend development:
handling different screen sizes, live updates of other people's actions, and non-rectangular
click regions. Many thanks to Ivan for figuring out most of those details.
Internally, every cursor fires a location update in Websockets every 150ms,
and the team-wide cursor state is broadcasted back to all viewers, using [a spring animation](https://liveblocks.io/blog/how-to-animate-multiplayer-cursors)
to make cursor movement smoother by respecting momentum. The puzzle pieces are squares that have
an SVG applied on top of them to cut them into the shape of the puzzle piece, with the SVG defining
the click region for each piece. Custom hooks on mouse-enter and mouse-leave then track which
puzzle piece the user is moving, which is also sent to the server and broadcasted back.

![Cursors](/public/mh-2023/cursors.png)
{: .centered }

I played QA testing on checking it worked properly on Firefox and Safari,
and debugging bugs like phantom cursors and puzzle pieces that claimed to be held when they weren't.
(Fun fact:
did you know that when you enter an element, browsers can sometimes fire a 2nd enter event in the middle
of the first enter event? It's true! It happens entirely randomly, causing you to see 2 enters then 1 leave
all at once. I'm sure this won't cause a bug that take 5 hours to root-cause.)

At one point, we planned to implement an adversarial UI, where pieces would wiggle out
of their position, try to run away, and more. The story justification would be that MATE was trying to
stop you from getting
into the Factory. We ended up cutting this because it was more work. Also, in internal testing,
a teammate trolled us by dragging a puzzle piece 5000 pixels down the page, and we decided
we didn't need to make solving the jigsaw any harder.

![Conversation during Discord testing](/public/mh-2023/breakout_discord.png)
{: .centered }

We did end up adding a max draggable range. Somehow this caused a problem in the live Hunt, where some teams
dragged pieces outside of the draggable range and couldn't move them again. I'm still not sure what
happened there, because we tested it pretty thoroughly. I'm guessing something in the screen resizing
logic caused problems? In any case, we had to figure out live on-the-fly fixes for this, which was
"exciting".
</div>

Around this time, hunt leadership mentioned we were behind expected hunt progress, and organized a
teamwide "hack weekend" where each group would try to draft and testsolve a puzzle within the weekend.
In said weekend, this was written:

<div class="shaded" markdown="1">
### Museum Rules

There isn't really too much of a story with this one. We looked at the pool of open answers, picked
one of them, then brainstormed ideas based on that. That landed on doing
something with a list of rules, which we'd justify in story as rules for attending the Museum.
After the laws idea, the idea of extracting from Supreme Court seals felt like a more suitable way
of using the puzzle information than just indexing. The puzzle presentation was stolen directly from
[Storytime](https://2020.teammatehunt.com/puzzles/storytime) in Teammate Hunt 2020 (as in, I found
the code on my laptop and copy-pasted it into our Mystery Hunt repo).

The puzzle was written in about 2 days. I worked on creating the seal overlay extraction,
Catherine did the drawings, and Harrison did the factchecking + research of weird laws. Harrison
found that most weird laws did not have a solid primary source, but laws like "no killing a fly next
to a church" were too funny to exclude. The urban legends were wrong, but they were at least
*consistenly* wrong, so we decided to perpetuate a bit of inaccuracy for the sake of solvability,
then confess our sins in the puzzle solution.
Other laws were deliberately misinterpreted for humor, like "no marathon dancing".

My personal favorite weird law is that
[New York has a specific child labor law exception for working as a bridge caddie in a bridge tournament](https://law.onecle.com/new-york/labor/LAB0130_130.html). We did not use it because it was just, not going to
be searchable. (I learned about it through someone at [BEAM](https://www.beammath.org/) telling stories
about verifying legal compliance.)

I do regret that we didn't figure out a way to cleanly push to Supreme Court seals
rather than state seals. The ambiguity came up in testing, and we specifcally called out things that only existed in the Supreme Court seal when possible, but it was still a sticking point. We didn't get around
to finding a way to make the relevant bit of flavortext more obvious.
</div>

The hack weekend brought us back to the on-target trajectory, although it didn't bring it back to
surplus. I ended up working on two other puzzles in July.

<div class="shaded" markdown="1">
### Interpretive Art

As mentioned in this solution, this puzzle started from a shitpost someone posted on Facebook.

> "First I was afraid, I was petrified."
>
> - person losing a fight with Medusa

Me: "haha. Wait this isn't a bad puzzle idea."

I mentioned it in our #puzzle-ideas channel, there was enough buzz to get a brainstorm going,
and we were off.

One of the common tools a puzzle constructor reaches for is "does this have a small, canonical
dataset"? Such things are easier to search and usually give a pre-built Eureka moment when
solvers discover the canonical dataset. Accordingly, the first idea on this puzzle was to
only use songs from the [Guardians of the Galaxy Awesome Mix](https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_(soundtrack)). It was a good intersection of pop music and alien interpretations of that music.

After I realized the GotG Awesome Mix did not have Never Gonna Give You Up or All Star, I decided
using it was a bad idea, and we switched to "literally any popular song". If you cannot meme in
a puzzle, what are you even doing? An early version did not index words, and instead built up
an eight word cluephrase by highlighting 8 squares of the fill. In testsolving, we didn't like that
only the clues crossing those squares "mattered", so it was changed to indexing.

My proudest moment of the puzzle was in the first testsolve,
where testsolvers solved NA NA NA NA NA NA NA NA NA NA NA HEY JUDE, and placed it while saying
"what an absolute shitpost". It was! It was also intentionally a potential
break-in point for starting the grid fill. A shitpost must be a bit serious to be good.

For a puzzle written so quickly, this puzzle got insanely high fun ratings in testing and seems
popular with Hunt participants as well. Between this and [Young Artists](https://2021.galacticpuzzlehunt.com/puzzle/young-artists) from GPH 2022, I suspect that any puzzle that riffs on pop music is
going to be a fan favorite.
</div>


<div class="shaded" markdown="1">
### Conglomerate

Remember how I said I didn't want to write minipuzzles anymore?

I still don't, but editors asked if I could help contribute to a puzzle skeleton
chuttiekang came up with. The ideas were there, it just needed more hands. I suppose bending
the rule to work on someone else's minipuzzles is okay.

I ended up writing Birdhouse Kit, Camera, Microphone,
Quilt, and Telescope, as well as a revision of Fishing Rod after the original mini failed.
The sheer quantity of minipuzzles did make construction a bit difficult, since we wanted
to avoid repeating encodings or extraction mechanisms. That left fewer options after we exhausted
binary, semaphore, Braille - you know, the old standbys. The Quilt puzzle was computer generated,
I wanted the puzzle done fast and figured people would forgive a computer generated Sudoku if
it was just part of a puzzle.

This puzzle
had more errata than I'd like. I think this is expected for minipuzzles. You have to make
more content,
so your exposure to missing something in factchecking is larger as well.
</div>


# August 2022

## Archaeology

As the date for retreat approached, most of my work was on fixing parts of the Loading
Puzzle. Still, there was time to work on other puzzles along the way.


<div class="shaded" markdown="1">
### Lost to Time

Whenever I wrote a puzzle for Hunt, I did a search in the /dev/joe index to double check it's not
too close to a prior Mystery Hunt puzzle. At one point, I opened a link to the 1995 Mystery
Hunt, and got really confused why the solution page redirected to devjoe's domain. I posted a link
in #puzzle-ideas and kept going on other work.

About a week later, Bryan said it was turning into a real puzzle, and I could join if I wanted because
I'd be too spoiled to testsolve it anyways. The plan was to pretend we found all the missing parts of the
1995 Mystery Hunt in the Puzzle Factory basement.
The core idea was to write puzzles with two answers, one for the 1995 Hunt and one for the 2023 Hunt,
where knowing the 1995 answer would help to solve the 2023 answer.

Given how niche the idea was, it seemed like a
shame to leave any stone unturned, so we aimed to use
as much of the missing content as possible. That meant writing 8 minipuzzles, based on the
constraints given by devjoe's writeup and the 1995 Hunt document.
This definitely contributed to the length of the final puzzle. Even at planning time, we knew it was reallllly pushing
it for a puzzle in Act II of the Hunt, but decided it could be okay if it was a one-off exception.
In hindsight, I believe we were correct to use all the minipuzzles, but each minipuzzle should have been
made much easier. At the same time, both testsolves were clean and gave it high fun ratings, so I don't
think there was any way our systems were going to point us to revising the puzzle.
I'm a bit disappointed this wasn't solved more, but I understand given its length and placement in Hunt.

The minipuzzles I directly made were the conundrum and the screenshot of Rick Astley, but in general
I helped out on the presentation of other minipuzzles and the meta.
In the vein of [Cruelty Squad](https://store.steampowered.com/app/1388770/Cruelty_Squad/), we
spent a lot of time trying to make everything look as bad as possible. Well, "dated" would
be the more accurate term. Every image in the puzzle was saved as a JPEG with lower resolution
and maximal image compression. For the conundrum, I printed it out, cut it by hand, scanned
it, then JPEG-ified the output because the scanner I used was too high quality.
Last, we planned to run the video through a filter to add VHS lines, but this
got dropped because none of the premade filters we found matched our desired aesthetic, and
making a custom one too much work.

For the newspaper puzzle,
I briefly considered buying a [newspapers.com](https://www.newspapers.com/)
subscription to track down the original newspaper used for the 1995 Hunt, and then realized that
it would be much better if the puzzle did not encourage solvers to find the original. Hence the
name "The Boston Paper". That
being said, I bet you could find what the original puzzle looked like if you dug hard enough.

It's also interesting to write a puzzle that will potentially be broken if
anyone finds more material about the 1995 Mystery Hunt. I'm usually rooting for better archiving.
This is a rare example where I hope it stays in a steady state!
I know an old Dan Katz post speculated whether an entire early Mystery Hunt equaled a single
puzzle in a modern Hunt. I think this puzzle shows that no, it can't, but it's kind of close.
</div>

As more retreat prep, I did an internal test of a paper version of Weaver.
I failed to understand the final cluephrase even with the correct extraction, so it got revised.
I also remember commenting
that it might be one weave too long, but we didn't plan any changes before retreat. Brian had done
some experiments with the hydrochromic paint he'd ordered before, and was making a copy by hand
for retreat.

## Working on the Hunt Site

Hunt tech was also starting to ramp up. This was taking up more time. We had done many planning meetings
about Hunt tech starting from the beginning of the year, but now we had a deadline: the hunt site needed
to be in a good enough state to testsolve breakout.

Most of the hunt tech meetings were done async, although it became more synchronous as we had more work.
Earlier, I mentioned that we planned to stay close to tph-site, but after seeing Palindrome's
release of [spoilr](https://github.com/Palindrome-Puzzles/2022-hunt), we revisited this plan. On a read
of the code, Palindrome's copy of spoilr had more team management that was relevant to Mystery Hunt.
We still wanted to keep our frontend, so we decided to merge tph-site and spoilr into one codebase,
then separate out the updates posthunt. Many thanks to Alex Gotsis for signing up to go through
integration hell.
Like most software integrations, the end result is a bit more complicated than both in isolation.

One interesting thing about tech for Mystery Hunt in particular is that there's greater emphasis on
creating internal tooling. In a smaller Hunt, it's possible for a small number of tech-inclined
people to get all the puzzles into the Hunt website. In Mystery Hunt, this is less true, just because
of the scale. We knew the end of Hunt writing would be busy with art and puzzle postproduction, so
any tools we wanted were best planned and coded early.

Top of our list was improving our postproduction flow, given Palindrome's comments that they
had a postproduction crunch. It had been a bit of a grind for us too in Teammate Hunt.
Ivan proposed an "auto-postprod" system. Given a Google Doc link, the tool
would load the page, retrieve the HTML within the Google Doc, create a puzzle file based on that
HTML, and auto-create a commit plus pull request for that puzzle.
I said "sure", not really expecting it to ever exist. Imagine my
surprise when it actually got built!

It wasn't perfect. It didn't handle Google Sheets, it
would sometimes timeout if a Google Doc had an especially large image, there was an issue with
URL escaping at one point, and if it errored on a puzzle it would require someone
to clean up git history before it would work again.
Despite all of this, it was still quite helpful.
You don't realize how long it takes to mechanically copy-paste paragraphs from Google Docs
and wrap them in `<p>` tags until you've done it literally thousands of times.


## Retreat!!!

Retreat was held in late August, in a big AirBnB in the Bay Area. I took Friday off work
to attend in person, and people from East Coast would arrive over time each day.

### Friday of Retreat

![Retreat schedule, Friday + Saturday](/public/mh-2023/retreat_friday.png)
{: .centered }

It was great to see people from teammate in person, given how many people I had only met
over Discord voice calls. It's really not the same as meeting in person.

As an icebreaker, we went around saying what our favorite puzzle was. I forgot what I
said - I think it was [A Puzzle Consisting Entirely of Random Anagrams](https://puzzles.mit.edu/2015/puzzle/a_puzzle_consisting_entirely_of_random_anagrams/)?
Vinjai said, "134". Later I'd learn this was the puzzle ID for the Hall of Innovation meta.
He was one of the authors of that meta and it sounded like it was in the middle of taking over
the lives of everyone involved.

Retreat started with a group testsolve of all the Act I puzzles and physical
puzzles. Most physical puzzles were in Act I anyways, based on 2022 feedback that puzzles
like [Diced Turkey Hash](https://puzzles.mit.edu/2022/puzzle/diced-turkey-hash/) were placed
pretty late last year.

Almost immediately, we started hitting technical problems. A few were real software bugs,
but most of them were tied to trying to have ~20 laptops and ~20 phones all connect to
the same WiFi. It...did not work that reliably. I switched to using a WiFi hotspot from my phone.
The other issue was that our testing server was fine for 3-5 people, and less fine for 20+. The
setup for tph-site assumes it runs on a single VM, so we were not connected to an autoscaling
service and needed to migrate on our own.

These tech problems made people assume that the loading animation was real, and that the site was
just struggling. Which was useful data to have, it implied we *really* needed to land the prod
version of the site. We also observed that when there were real puzzles instead of fake puzzles,
teammates from teammate would get in the habit of opening every puzzle at once in a
separate tab, then going back to a puzzle spreadsheet while waiting for the puzzle to load.
That was less great. The loading puzzle was not getting solved and no one was suspicious.

In an attempt to get people to look at the loading animation more, the editors-in-chief
nudged testsolvers towards puzzles that unlocked later (had longer load times). Meanwhile,
on the tech side we did a number of deploys to "fix site bugs", that was *actually* just
increasing the time of the loading animation and encouraging solvers to refresh the site
to get the "bug fix".

An added part of the difficulty was that testsolvers were progressing through Act I slower
than expected, so our load time curve needed to be adjusted. I can see some of you yelling,
and yes, in hindsight, this was an early sign our difficulty estimates were off. But the
main goal of retreat was getting one good testsolve of breakout, and it was *not working*.
If that didn't work, the entire Hunt design was in jeopardy.
In comparison, many of the Act I puzzles were on their 1st draft and would likely
get easier as they got polished or revised.

I think both of these arguments were correct! We were correct to focus on breakout and
also correct to assume puzzles would get easier over time as they got cleaned up. The
error was in estimating how *much* the solve time would drop in future revisions. The magnitude
of the effect was probably smaller than we were hoping for.

In between trying to ramp up loading times, I testsolved many puzzles, including
[Exhibit of Colors](https://interestingthings.museum/puzzles/exhibit-of-colors) (solved the
flower puzzle and got the animal a-ha), [Dropypasta](https://interestingthings.museum/puzzles/dropypasta)
(complained heavily about the old Pokemon Stadium mechanic - it got changed to what our testsolve
group thought it should be), and [Brain Freeze](https://interestingthings.museum/puzzles/brain-freeze). At
the end of Brain Freeze, someone asked why it had such a garbage answer, and I had to stay silent
knowing I was responsible for pushing that garbage answer into the Hunt.

![Shifty](/public/mh-2023/shifty.png)
{: .centered }

After that, Collage got unlocked! Except, it was entirely broken. Oh no.

One meme in teammate
during writing was "bamboozle insurance". You offer bamboozle insurance when you want to assure
people something will be true, and you pay bamboozle bucks when you are wrong.
Example uses include:

"This meta testsolve needs 2+ hours but it will be worth your time, you can buy bamboozle insurance from me"

"Can I get bamboozle insurance the loading animation time won't change, I'll have to redo math if it does"

The nature of writing
puzzlehunts is that you'll be wrong quite often and I think everyone who offered
insurance is in bamboozle buck bankruptcy. Here are some emojis we generated
using DALL-E 2 when it was first announced.

![Bamboozle emoji generations](/public/mh-2023/bamboozle.png)
{: .centered }

I bring this all up to say that I offered bamboozle insurance that Collage would be fine,
and then it wasn't. In my defense, it was fine when we testsolved it a few weeks
ago.
It turned out there was an issue where the loading animation was conflicting with the startup of
the interactive puzzle content, and this
caused the async initialization requests for Collage to never fire. We spun up
a hotfix to get Collage working, and made a note to avoid interactive puzzles
at the start of the final puzzle order.
Unfortuately, this fed into the narrative of "the site is struggling, so the loading animation isn't special".
It is the sort of thing where if you think about the loading animation for a
bit, it does not make sense for a list of text to take longer to load than the
loading animation itself, but that is not the mindset most people use unless
prompted to.

The original plan was to get to breakout by around the evening (8 PM to 10 PM), then leave time for
board games and socializing. But it was still going. My original plan was to pick up my parents
from the airport at midnight, then come back to the AirBnB. With how the testsolve was going,
I called my parents to let them know I wouldn't be free, and they should make other plans.
They did, and my parents informed me they tested positive for COVID the next day.
Which left me with a *very weird*
set of emotions around "I feel bad for cancelling on familial responsibility, but
it's good that doing so let me dodge COVID exposure, and it's extra good I did not go
the airport and come back to bring COVID exposure to retreat".

At 1:40 AM, the Act I testsolve was still not done, but it was officially put on pause
to encourage people to go to sleep.
Hunt exec + breakout authors then convened for a secret meeting in the basement.
It felt a little like the Project Elrond scene from The Martian.

We concluded that all our intuitions about the loading puzzle were off, and it
needed to be *way* easier, easy enough that we would not need to worry about
the bottleneck it could have on Hunt. The additions we landed on were:

* Be much more aggressive on hinting with the messages MATE sent during loading.
Instead of just giving a generic error, MATE would give messages that suggested
the solution and say things like "It's puzzling why this is taking so much time."
* The original version shuffled pieces of each color to disguise the letter forms.
This was partly done because we did not want the puzzle to be solvable from
a screenshot of the animation, we wanted to require teams look at the site to
solve it. This was cut and the pieces were all shown in order.

![New puzzle piece order](/public/mh-2023/neworder.png)
{: .centered }

By the time this was resolved and implemented, it was 4 AM. I went to go collapse on an air
mattress so that I could be sort-of awake tomorrow.

### Saturday of Retreat

The Act I testsolve was still running, but we did want to do the originally
scheduled events. People could continue the testsolves in downtime, but were
told to prioritize event testsolves and AI round puzzle ideation.

At this time, I was starting to get questions from friends about whether Mystery Hunt would be
on-campus this year, and I still could not tell them anything besides "we're working on it".
The tentative signs were pointing to "yes", so we decided to plan as if we'd run on-campus,
and events were correspondingly designed around in-person interaction.
This was done knowing that the events wouldn't be doable without on-campus
presence, which could hurt fully remote teams, but, well...in my opinion,
it's called the MIT Mystery Hunt for a reason. I understand that not everyone can come in
person and that this hurts accessibility, but after two years of remote-only events, it seemed
appropriate to swing the pendulum back to in-person events. The events were made optional to
finishing Hunt to make sure it still be possible to finish Hunt if a team was fully remote.
Perhaps future Hunts will do more hybrid events - I'll leave that choice up to future organizers.


<div class="shaded" markdown="1">
### We Made a Quiz Bowl Packet but Somewhere Things Went Horribly Wrong

Okay. I know this puzzle isn't that popular, but all puzzles will have their stories told.

We were given the feeder answer for this puzzle, and after investigating various pyramid options,
we laneded on pyramidal quiz bowl questions. Some authors were fans of quiz bowl
and it's a puzzle-adjacent hobby. That got pyramid into the theming.
The idea of extracting based on diagonalizing words based on a Hamiltonian path of
the United States seemed like a good way to tie America into the theming as well.
We spent much of the morning working out mechanics in more details (how to
connect answers to states, how to extract), and made a suitable quiz bowl question
for the desired puzzle answer.
Unlike regular quiz bowl, solvers would have access to the Internet, so we
treated the puzzle as a "how well can you Google" puzzle. Since it was in an
AI round, we decided to allow loose searchability bars as well (which was one
of the big causes of the puzzle's length).

Originally, the plan was that every question would semantically clue a state in the Union.
This was thrown out pretty quickly when we tried to tie DADDY LONGLEGS to South Carolina, and
the best we could come up with was a South Carolinian high school's rendition of the musical,
which was a bridge too far.

A sketch of 4 sentences per clue x 48 clues was, in hindsight, a lot. Part of the justification
was that this was not a puzzle that required a lot of context to make progress.
It was something that parallelized well and was easy to jump in and out of.
With other puzzles trending in the direction of serial deductions, it seemed
okay to have a big puzzle that could run in the background.

Over the course of a few weeks, we started filling out the draft. It was clear that the puzzle
skeleton would work mechanically, but we were quite unsure if the puzzle would be fun.
For those unfamiliar with quiz bowl questions, the way they work is that they
start obscure and become more obvious over time. Teams buzz in to interrupt the
reader whenever they know the answer, which rewards teams with more obscure
knowledge. But in this puzzle, you'd have search engines, and you'd have all
text of all questions (albeit not in order).

It seemed like the solve process would be that you Googled every last sentence,
which would immediately solve to the list of words. Then for every word, you'd
find every sentence that looked like it could describe that word, and searched
it to quickly verify yes vs no for each. That would work and everything would
be reordered and then you'd be done. It felt pretty mechanical, where you didn't
have an a-ha anywhere and just shuffled words together until it solved. Like
solving a jigsaw puzzle - not *necessarily* a problem, but probably not
interesting.

Given the structure, the easiest place to add an a-ha was in the 1st step of
Googling everything. That motivated the decision to
obscure questions by pretending they were always talking about a location. In this way,
although the bulk of the puzzle would still be Googling, you'd start with a global
a-ha that the location theming was misidrection. Then every sentence would
have a local a-ha in deciding how to reinterpret the clue. "Cryptic-like" is the right analogy
here. Cryptics work because you need to decide what is wordplay and what is definition,
and similarly you'd need to decide what parts to read straight and what parts were location
wordplay.

This made the puzzle...a *lot* harder. which got corroborated by testsolving. To paraphrase, the
feedback was "this was really fun at the start, then unfun after we got really stuck".
That is how most puzzles feel like when solving, but *relative* to those puzzles this puzzle
was less fun to be stuck on because you cannot be saved by one good idea.

I've read a decent amount of negative feedback about the "all locations" aspect of the puzzle
(some direct, some passive-aggressive). I am willing to die on the hill that it does improve
the puzzle and shouldn't be removed. The better fix is to keep the obfuscation and delete half the clues.
This way the a-has are still there, but pairing is much easier and less grindy, since there
are 1/4th the number of possible pairs.
</div>

AFter lunch, priority shifted from writing to continuing the Act I testsolve. The 4 AM fixes were
now on the testing site, but embarassingly we had to do another hotfix because the loading
animation had an invisible `<div>` that made the answer box unclickable.

![Invisible box](/public/mh-2023/answerboxcovered.png)
{: .centered }

Solving of the last few Act I puzzles continued, and the pool of unspoiled testsolvers *still*
weren't solving the loading puzzle.
From our #puzzle-ideas channel during retreat:

![Someone saying the loading animation should be a puzzle](/public/mh-2023/itisapuzzle.png)
{: .centered }

This was triggering some very strong *"what the f&ast;&ast;&ast; do we do"* reactions. What
nerfs were even left to apply?

I don't think the relevant pool of solvers ever solved the puzzle organically. Instead,
hunt exec, breakout authors, and a few unspoiled solvers were all shuffled into the same
basement room we were in last night. The unspoiled authors were told to look at the loading
screen, and solved it in a few minutes. Their initial feedback was "yeah, this seems
obvious, we should have noticed this earlier", to which we said, "You say this, but we have
12 hours of empirical feedback saying the opposite."

After some more Q&A on what made them believe it wasn't a puzzle, reasons why they didn't pay
much attention to the animation, etc, we planned a few more changes:

* The four green dots (used to give an ordering on the letters) would be moved out of the
answer submission box. Appearing inside the submission box made it look like the answer submission
field was "loading" and could not be interacted with.
* Submitting an incorrect answer during loading would give a custom message ("Whatever you do, definitely don't submit anything until it's finished!"). Hopefully this would inspire some reverse psychology.
* MATE would become increasingly depressed as loading continued, to entice people to keep
watching the page to see what would come next. We planned to add this yesterday night,
but the art assets didn't exist for it.
(Much later, this led to the quote of "why does the animation look better when MATE is perpetually sad?")

![sad MATE](/public/mh-2023/mate_sad.png)
{: .centered }

The most important consequence was the creation of the megahammer. Internally, "the hammer"
was the time when teams would see infinitely loading puzzles. We envisioned this as the point
that would force teams to solve the loading puzzle. "The hammer" would be time-unlocked
to reveal the loading puzzle to all teams. Given that the hammer was too weak, we needed something
bigger. Hence, the megahammer. The megahammer was an email that would contain a video tutorial
of how to solve the loading animation. The story team would figure out how to justify it, and
we would send the email an hour after the hammer, to give teams some chance to solve the puzzle
on their own before being sent the solution.

With all these changes planned, we assumed there was enough redundancy to go forward with the
original plan of revealing the existence of breakout. After solving the loading puzzle, all
"load times" were brought back to normal, and remaining testsolves could be continued without
knowing the drama of what unfolded.

With breakout resolved, we could go back to puzzle brainstorming and wrapping up testsolves. The
[Teammating Dances event](https://interestingthings.museum/solutions/teammating-dances) was
written and tested entirely at retreat. Dinner arrived (I think it was Thai food?). After dinner,
I joined a very-stuck testsolve of [Tissues](https://puzzlefactory.place/factory-floor/tissues). At the time, it did not have the TetraSYS hint. We were given various hints towards "four" during the testsolve,
and I mentioned there were four Greek elements (no idea why). I then said something along the lines
of, "it's a physical puzzle. We should do something that exploits the physicality", and someone
mentioned setting the puzzle on fire or dunking the tissues in water. "We should go do it,
even though it's likely not correct, because if it's wrong the authors watching us right now
will stop us from destroying the puzzle." This gave the authors a bit of an aneurysm. "PLEASE
DO NOT PUT THE TISSUES IN WATER why did you force us to clarify this??"

(Meanwhile the testsolver group for Weaver was putting the puzzle in water.)

After a number of other revisions, we were finally able to make it to the cluephrase of Tissues,
but I decided to turn in for the night to catch up on sleep.


### Sunday of Retreat

I woke up and continued the Tissues testsolve while waiting for other people to wake up. Some
authors of Tissues were awake as well, and I did a few searches of "Nikoli + black cells" in front
of them to prove it was not unique. They decided to try the cluephrase "DOKODESU". I did searches
of that and showed that no, whatever you're going for is not enough. Eventually some combination
of "Nikoli + [dokodesu's English transition](https://www.google.com/search?q=dokodesu+in+english) + black cells"
got there. I assume our testsolve was why this step got nerfed.

With more people awake, we proceeeded with the scheduled State of Story meeting.
The creative team presented the broad strokes of the Hunt story, the existence of
breakout, our plans to make teams believe they shut down Mystery Hunt, and the broad art direction
for each AI round. AI gimmicks were not revealed, since some still needed to be testsolved.
Wyrm would be a precocious child with
a construction paper aesthetic (since every explanation of a wormhole in fiction involved sticking
a pencil through two pieces of paper). Bootes would be a meme-y ASCII art cat in space.
Eye would be a noble, biblically accurate angel, with nods to the Tower of Babel. Conjuri would be a pixel
art owl eager for new challengers. Only Eye's gimmick was spoiled widely at this time, since their
puzzle ideas were most constrained and needed the most help for writing. The rest of the gimmicks
were not spoiled to make testsolves of their puzzle act as if the team had not broken into the
answer gimmick yet.

There was then time for a Q&A at the end. An audience member who did not notice breakout asked
"what's our plan if a team doesn't notice breakout?", which had *incredible* dramatic irony.
We said we had a plan and moved on.

We did a final round of puzzle brainstorming and testsolving. I remember getting pulled aside to
a puzzle brainstorm because we thought it'd be funny to write a puzzle with "the full house
of Alexs over Brians" (Alex Gotsis, Alex Irpan, Alex Pei, Bryan Lee, Brian Shimanuki,) Don't let
your memes be dreams! We got in a room, tried to write a puzzle for [l(a](https://en.wikipedia.org/wiki/L(a),
spun in circles for an hour, and failed. The puzzle idea was officially killed a few weeks later.
Some memes should stay dreams.

Retreat concluded with a bunch of board games. We played some rounds of Just Two. It's [Just One](https://en.wikipedia.org/wiki/Just_One_(board_game)) except a clue is only provided if exactly two people
put it done. It's good for large groups and is a *lot* harder. I'm told that when the word "pony"
showed up, the only clue that made it through was "Irpan".

East Coasters flew back home, Europeans flew *really* back home, and we ramped up the fall push
for finishing Hunt.


# September 2022

## Ah Yes, Websites are a Thing

It's a bit disingenous to say that I started working on the site in September, becaues I had been
working on parts of it starting even back in January. But I'd say my main ramp-up started around
here.

There were a number of tasks to do, and the main one I took was handling the Wyrm round page.
This ended up being a *lot* more complicated than I thought it would be. During Wyrm ideation,
we discussed the [Zoomquilt](https://zoomquilt.org/) and its [sequel](https://zoomquilt2.com/)
as both art inspiration and proof-of-concept that we could make a infinitely zooming round page.
{I remember Huntinality 2022's [registration form](https://2022.huntinality.com/) also came up.)

Each of the Wyrm layers was done by a different artist. Turns out that if you want every puzzle
to have its own icon, it doesn't matter that 13 puzzles in the round are "fake" or repeats. You
still need to draw 13 art assets. Creatively, the art team did not want to reuse art assets
between the Museum and Wyrm versions of the Act I puzzles, since even though they were the same
puzzle, one would be in "MATE's Hunt" and one would be in "Wyrm's Hunt", with all the visual
changes that implied. Much of the ideation was on fractals, self-symmetry, repeating patterns,
and so on.

![DeepDream inspo](/public/mh-2023/deepdream.png)
{: .centered }

With some discussion turning to fish tessallation patterns, tech was asked how hard it would be
to support something where all fish of a certain type would link to the same puzzle, even if
there were multiple fish in the round art. This would cause the click region to be both
nonconvex and disconnected, but tentatively we believed it would be doable.

I looked into creating a proof of concept for the round art and backend, using placholder
art assets. My first step was to copy the D3.js zoom code that I'd made for Collage.

![Surprise tool](/public/mh-2023/surprise.png)

Investing in learning D3.js was starting to pay off. To make it work for the round page, I
ended up stripping out the ability to drag the page, and the event listeners that responded
to scroll wheel inputs. The user experience of
scrolling down a page and then getting your scroll wheel eaten by the round art was not great,
so I restricted the page to only work via hardcoded buttons.
I also needed to figure out how to make
the zoom level "roll over" once you passed the infinite threshold. I expected that to be a nightmare,
and was pleasantly surprised when it really wasn't.

Based on the barebones version, I asked if art could pick a fixed aspect ratio for all layers, along
with always zooming in or out of the center of the art. They said yes.
I hardcoded some placeholder puzzles to prove my server-side management worked at all stages of
puzzle unlock. Then it was optimization time.

> Based on the profiler call, current D3 setup is 11 ms to re-render all 4 layers when I do the 1 -> 2 zoom. (All 4 divs get re-rendered since they all look at scale variable). I set up the CSS transition for just layer 1 -> 2 and that was 8ms. The gap should get smaller when I make the CSS animation apply to all 4 layers.
>
> So, in short, yes it'll be faster, I'm not sure it'll be appreciably faster.
> I'll probably leave this for later based on the timing, in favor of some other things I need to fix in the prototype. But can keep it in mind as a performance win that may be more important when we have more assets in the round.

Performance optimization is not really in my wheelhouse, but it's something we thought about
a lot for Wyrm.
To make the zoom work, we *really* needed the zoom implementation to look seamless.
The initial prototype was not. There was very noticeable pop-in as you swapped between layers,
on both the background image and puzzle icons.

It 100% seemed liked a solvable problem, given that Zoomquilt was seamless and I remember
it looking seamless on 10 year old hardware. When I looked more closely, their code was based
on using an HTML canvas. I was less excited about this, because HTML canvases don't use vector
graphics, and the default usage created weird rasterization artifacts when I tried using it
in Collage.

I asked the tech people with more frontend experience for advice, and they helped explain
React's rendering logic and where I could run a profiler. It turned out the problem was that
my prototype was taking fixed `<img>` tags and changing the `src` field based on what zoom level
I was on. This effectively forced the image to clear and re-render every time I crossed layers.
No browsers are built to do this seamlessly. We redesigned the layers to create all the
`<img>` tags for all four layers at once, then use CSS and JS to adjust the zoom and
[z-index](https://developer.mozilla.org/en-US/docs/Web/CSS/z-index) ordering of the static images.
This pushed the loading and drawing upfront, and the rest would just be image transforms, which
were relatively cheap.
This fixed the bulk of the performance issues.

Next was fixing the blurriness. That was easy. When I set up the prototype, I defined the zooms
as $$1, 8, 8^2, 8^3$$. I found that easier to think about, but it meant you would sometimes see parts of a 64x magnified image during transitions. It looked awful. I inverted the scale to $$1, 1/8, 1/8^2, 1/8^3$$
and things were fine.

The proof-of-concept now looked good enough that we were convinced the art concept was implementable,
and I moved towards other work.

## Feature Requests

From the start of the year, we'd been dumping feature requests and bug reports into our Github issues
list. With less pressing puzzle work, I ran through all the features I knew I could do.

The first was improving our puzzle icon placement system.
This also came out of Teammate Hunt 2021.
In tph-site, icon placement is a field on the Puzzle model, meaning it's saved in the database and
can be adjusted live via admin pages. This made it possible to change positioning without
a commit, but in Teammate Hunt 2021 it was kind of ass to alternate back and forth between the admin
page and round page, refreshing to check every adjustment since the page did not live-refresh.
To fix this, I coded a what-you-see-is-what-you-get drag/drop positioning tool. When logged in as the admin team,
it drew this horrible, ugly red box that
could be moved around. Whenever it was moved it would auto-save the new position and size info to server.
We'd then do one reconcile step at the end to export position data from the server and commit it into
code to save a record of where puzzles should be.

![Drag Drop UI](/public/mh-2023/dragdropui.png)
{: .centered }

The drag-drop tool had a bunch of bugs I never figured out, but I'd like to think it helped anyways.

I also did some work on postproduction of Quandle (to move the wordlist and game logic to
server-side), and handling some of the TODOs tied to breakout's testsolve at retreat.

Galactic Puzzle Hunt 2022 ran around this time, notably with more tech issues than last time.
In a spoilery discussion about GPH with some puzzlehunt devs, a dev from Huntinality shared
a k6 loadtesting script they used for Huntinality, along with some optimizations
and bug fixes they found in tph-site's websockets code. I immediately make a note to test this
later. The loadtest script in particular looks nicer than the hodge-podged Locust code I had
written to loadtest the Playmate in Teammate Hunt 2020.

I also joined the brainstorm group for what we called the midpoint capstone puzzle, which you know as:

<div class="shaded" markdown="1">
### Reactivation

I'm on the author list for this puzzle but I really didn't do much. I was in the initial brainstorm
meeting, where the creative team said they wanted each puzzle to be thematic to the AI. I shared some
links to [turtle graphics](https://en.wikipedia.org/wiki/Turtle_graphics) for Wyrm, which matched
the repeating patterns idea, then stopped paying attention to the puzzle. That's all I did.
</div>



# October 2022

## How About a Nice Game of Chess?

I was running out of burning puzzle ideas to work on. That was fine with me, I only wanted to work on
puzzles that I was excited about.
My plan was to work on token tech tasks the rest of the year,
except for one puzzle, which would become much bigger than I expected.

<div class="shaded" markdown="1">
### 5D Barred Diagramless with Multiverse Time Travel

I consider this puzzle my magnum opus.

If you want to solve a fun puzzle, go solve Interpretive Art. If you want a novel
solve experience, go solve Puzzle Not Found. But, if you want a puzzle with
spectacle that will strike fear into your heart, I can only recommend
5D Barred Diagramless with Multiverse Time Travel. It is very upfront about
what you are getting into, and, to quote Bennett Foddy, "I made this puzzle
for a particular kind of person - to hurt them."

Okay I am exaggerating a bit. I think it's long, but actually not as difficult
as the title suggests. Let's start from the beginning.

The first draft of this puzzle was written in February for our internal puzzle potluck.
I really liked the joke [made by a team in DP Puzzle Hunt](https://dp.puzzlehunt.net/team/5D+Crosswords+With+Multiverse+Time+Travel.html),
and figured it could be a good puzzle if made for real. The first draft went
all-out on extending the [Clinton/Bob Dole](http://www.alaricstephen.com/main-featured/2017/7/3/the-clintonbobdole-crossword)
crossword into another dimension. If I extended Joe Biden to Joseph Biden, then
I could make JOSEPHBIDEN/DONALDTRUMP be the equivalent of CLINTON/BOBDOLE.

At this time, I had no plans to use 5D Chess with Multiverse Time Travel,
since I knew nothing about how the game work and had a limited amount of time
before potluck. Most of my design time was spent on determining a placeholder
extraction of any kind.

In this puzzle, some clues solve to two unordered answers. In any puzzle where
this is true, you either need to find a way to order the pair, or use an extraction
method that gives the same letter no matter how the pair is ordered.
I first realized this when solving [Split the Reference](https://2018.galacticpuzzlehunt.com/puzzle/split-the-reference.html)
in GPH 2018, and have since noticed that pattern all over the place.

I read more presidential election Wikipedia articles (would not recommend), and
noticed JOJORGENSEN was *also* exactly 11 letters.
I mean, come on, you have to try doing something with that. This arrived at
a "3 timeline" idea. Two timelines would be directly solvable from the crossword
clues. Each two-timeline entry would then have a valid answer from a 3rd timeline,
that would form valid words along its crossings if placed into the grid.
(i.e. CRAG / CRAW could be solved from the crossword clue, but CRAB would fit as
well.)

This seemed like a *horribly* tight set of constraints, but I had no better ideas
and went forward with construction, using a cryptic crossword shaped grid to cut
required work in half. After reading even more presidential election Wikipedia
articles (again, would not recommended), I found two other years where I could
finagle three candidates that all had eleven letter names. It required dipping into
the endless sea of 3rd party candidates and was awful, but it was good enough
for a proof of concept.

The testsolve at potluck thought the grid fill was pretty fun. They got stonewalled
on extraction, but aside from that it seemed promising. I dropped the puzzle to
work on other puzzles (i.e. everything I talked about earlier in the post).

Fastforward to October, 8 months later. In a team meeting, the EICs (editors-in-chief)
mentioned we could use more crosswords in the Hunt for puzzle variety. Hey, I
have an unfinished crossword idea! Let's see if we can fix it.

I reflected on the design, and came away with these goals.

* As much as possible, I wanted
the puzzle to look like a single grid. A 5D crossword can have many different
projections into 2D space, but it should be viewable as a single crossword if you
adopted a 5D perspective.
More specifically, the given borders and black squares should
match across all of the 2D grids.
* The potluck version based extraction on just the alternate timeline entries.
Once you figured this out about the puzzle, it's the only part of grid fill
that matters, and that's a bit disappointing if you were working on other parts of
the fill.
(This was one of the bits of feedback I remember from writing
[Cross Eyed](https://www.puzzlesaremagic.com/puzzle/cross-eyed.html).)
* The other special entries in the grid were the time-dependent entries, that
were only true in 1980 or 2004 or whatever. One option was to index into those
entries to extract. To do so, I'd need to provide indices in some way, and
I didn't find a way to do so without giving away the a-ha that some entries
*were* time-dependent.
* Really, it's a shame the puzzle doesn't use 5D Chess in any way, given how directly it's called
out in the title.

Remembering [✅ ](https://2020.galacticpuzzlehunt.com/puzzle/check.html)
from GPH 2020, I considered the 5D Chess point more seriously. Suppose the puzzle
*did* culminate in a 5D chess problem. You'd want to solve the entire grid before
doing the chess, because you'd want to make sure you found every piece. That
made every clue matter. The puzzle would not need to label any of the special
entries because it wouldn't do any indexing for extraction. It would live up
to the meme puzzle title.
Thematically
and mechanically, it just made sense to make it a chess puzzle. Since chess
pieces could end up on any square, I'd need to make the crossword barred, and
making the crossword diagramless would let me satisfy the first goal of keeping
all given grids identical (since all 13x13 diagramless crosswords start off
with a blank square).

Great, problem solved! Time to learn how 5D Chess works. How hard could it be?

Neither of the editors had played 5D Chess before, but they said this all sounded
good and assigned me an answer from Bootes, with the reasoning that a 5D puzzle could accomodate
a 2D answer. The answer I got had some punctuation in it, which I agreed to try
fitting into the puzzle. I played around with having some rebus entries,
like cluing the actor GAI**L PAREN**T and filling it in the grid as GAI(T. The
answer options weren't great, but it wasn't impossible.

The much bigger problem was the chess.
Over the span of a month, I put increasingly deranged comments into the puzzle brainstorm channel,
talking to myself about how 5D Chess worked and the constraints it placed on the
grid fill. I'd say the default state of the editors was "confusion", as I monologued my struggle learning 5D chess
to people who only sort of understood what I was struggling with.

I initially
tried to learn the rules by watching gameplay videos. Within an hour I decided that it would
be more efficient to buy the game, so I did. I mostly paid for Mystery Hunt with time. This
was the rare puzzle that cost me money to make.
Even now,
I have yet to play a full game of 5D Chess with Multiverse Time Travel. However,
I have played a bunch of the in-game checkmating puzzles, which are excellent
and quite fun and are specifically constructed to teach you the rules.

After getting a basic understanding,
I decided to have the puzzle only use bishops, knights and kings. Rooks were too boring in 5D, pawns could introduce
questions about whether they had been moved yet or not, and queens were terrifying. A single queen in
5D chess has, like, 60 possible moves. There are arrangements where if a queen and king are arranged
properly, the queen can mate the king in 7 different ways. Trust me, using queens would have been a bad time.

To get early feedback on just the chess step, I ran some tests with a grid filled wtih random letters.
What was immediately clear from those tests was that it was very easy for someone to skim a 5D chess guide,
believe they knew 100% of the rules when they only knew 50% of the rules, and then attempt to solve the
chess puzzle with incomplete information.
As a solver, there is really no way to distinguish "I do not understand the rules" from "I understand
the rules and am just bad". Like, they would be 50 minutes in, asking for hints, and we'd have a conversation
like this:

![A conversation about 5D chess rules](/public/mh-2023/thefuture.png)
{: .centered }

"The future doesn't exist yet" is such a great line. I recommend using it in daily life without context.

In response to the pre-test, I created some 5D chess examples that could not be solved without understanding
the basic rules of timeline creation and branching. This was part inspired by [Time Conundrum](http://puzzles.mit.edu/2013/coinheist.com/get_smart/time_conundrum/index.html),
and part inspired by [Cryptoornithology](https://ecph.site/puzzle/cryptoornithology.html) from EC Puzzle Hunt.

The pre-test also revealed that I had missed some checkmates that worked. At that point I decided that okay,
I need to write code to verify my 5D chess solutions, because I'm not going to be able to trust my ability
to verify 5D chess by hand.

Using the game as a reference, I reimplemented the basics of the rules engine in Python, just enough to
check mate-in-ones. I went back and forth between my code and the in-game puzzles to check my work. For
timeline ownership, I skipped implementing it since I assumed it wouldn't matter to the puzzle.

With this done, I worked on reconstructing the chess position. I sent a question upwards:
if I clue the answer directly, would my letters need to be aligned exactly like the answer?
The editors sent this to Bootes meta authors, and the reply was:
yes, if you aren't using a cluephrase the spacing needs to line up exactly.

Well okay then I'm not going to be able to get the extraction to work that way, it's too constrained, so
I dropped the rebus ideas and switched to a cluephrase. I was worried it would be too hard to search, but editors tried it and didn't have too much trouble.

During reconstruction of the chess position, I realized an important constraint
that I hadn't before. In chess, almost all pieces attack symmetrically. A knight
can attack by going up or down, a bishop can attack along any diagonal. The
only exception was thw pawn, which I had already decided to exclude. This is true
in 5D Chess as well. Suppose you have pieces in the same location in both timelines.
Then, the checkmated squares across both timelines will also be identical.

DIAGRAM

This is a problem for the puzzle, because as designed, almost all letters are
the same across both timelines, but the cluephrase had very few repeated letters.
Any instance of this would be a problem.

DIAGRAM

The only way to avoid this was to add symmetry-breaking by changing piece locations
between timelines. This can only occur via the two-timeline entries.
I was only planning to have 3-4 of those entries since each
entry took a lot of real estate to support. (It's about 25% of a grid per entry,
once you account for the down crossings.)

Are you following me? If not, don't worry about it. The short version is that
The short version is that every alternative timeline pair needed to satisfy
one of the following:

* One has kings (K), the other does not.
* One has bishops (B), the other does not.
* One has knights (?), the other does not.

This meant the crossword fill and chess problem could not be constructed
independently. The puzzle needed to start from the two-timeline entries and their
placement, then the chess problem could be made, and only then could the fill
be done.

This is also the only reason the 2008 entry for "Olympics host city" was
BEIJING / BANGKOK. Bangkok failed in the very first round of IOC evaluation,
and was never a realistic host city, but
I really, desperately
needed the Ks that BANGKOK would provide.
Similarly the Kentucky Derby was picked for 1980 just because it was easy to
find a historical winner with a K in its name.

For knights, this constraint also influenced choosing J for knights instead of N.
Using N would have made grid fill way too hard, and J in particular appeared in
both JOSEPHBIDEN / DONALDTRUMP and KENJENNINGS / LEVARBURTON to break symmetry.
(I wanted
to use MAYIMBIALIK / LEVARBURTON originally, since it seemed fun
to use "the Jeopardy host that isn't Ken", but it caused problems due to sharing
a B in the same position.)

With a better understanding of the requirements, I placed my seed entries in the grid,
then wrote some code to
greedily place kings until it had achieved 22 checkmates. It failed! The checkmate
count went 17, 21, then got stuck. For reasons I don't want to get into, there's
a parity issue in most 5D chess grids that makes it hard to add an odd number
of checkmates.

Hang on, 17 checkmates from one king? That had to be a bug.
I added tons of debug print statements, and learned it wasn't a bug.
My brute forcer was running the rules correctly and had found a discovered mate.

I verified it in game, and went "holy crap that's so cool." But should it be in the puzzle? I tried
banning my code from using discovered mates, and found it made construction
high-impossible without placing a ton more chess pieces, which would make the
chess step even harder due to how many different moves you'd have available.

Essentially, I had two choices: use discovered mates and keep the search space
as 30-40 possible moves, or ban discovered mates and make the search space
100-200 moves. I went with the former.

Funnily enough, I started with making the chess position by hand, switched
to making it by code in the middle, and ended with doing a bit of both.
I first placed the two-timeline entries to engineer a discovered mate, then
had my code augment it to a 23 checkmate position, then added some fudging
to push it back down to 22 checkmates. With this done it was finally possible
to send it to another chess-only testsolve.

This testsolve went a bit better, but still had some trouble due to confusion
on how to set up the chess problem and what "one move" meant.
By this point the puzzle had evolved from "solve a crossword, get an answer"
to "solve a crossword, congrats you're 50% done", and it felt like it was on the edge of unreasonably
long. I expressed these feelings of uncertainty to testsolvers, at which point
they said that even though they had been stuck for 2 hours, 5D Chess was
"exactly the bullshit I expect to see in Mystery Hunt" and I should keep it.
I'd say a stronger statement is true: I don't think this puzzle could exist
outside of Mystery Hunt.
The question was whether I wanted it to exist.

I think people believe that, like, I was super
gung-ho about getting 5D Chess into Mystery Hunt, and I really wasn't!
All I wanted
was a 5D time travelling crossword with a suitable extraction, and all roads for that led to chess.
There may be a world where the puzzle is just a 5D Chess puzzle, no frills, but I don't think there's
a world where the opposite is true.

I decided to keep the chess step, but added even more entries to the crossword
fill to point towards the important 5D chess rules. The intention was that no
one should need to learn all of 5D chess to solve the puzzle, just enough to
get a readable extraction out. I was confident enough in the changes that
I decided to go forward with completing the entire grid fill and testing the
full puzzle on the next group.

Around the middle of the fill, I realized my placement of two-timeline entries
had created some 1-wide columns, which I wasn't too happy about. It wasn't fully
kosher to diagramless rules, but fixing it would have required regenerating
the entire chess position that had already been testsolved...I decided I didn't have time to fix it.
Getting to this point had taken me a month, I'm guessing around 60 hours of work
so far, and we had yet to do one full testsolve.

In the first full testsolve, testsolvers' initial reaction was "oh no",
which is pretty much what I was going for. The grid fill went great! The
chess less so. We discovering ever more exciting ways to play 5D Chess
incorrectly, this time on how to arrange the timelines of the puzzle. Luckily
TWO was already in the crossword, and I gave them a revision that was extra
explicit about rules of 5D Chess they'd missed.

Once hinted towards the correct initial state, they were able to solve the
chess problem without intervention.
I'd say that was the first moment I started to believe the puzzle could pass
testsolving and be done.

They did need to be given the total number of checkmates,
which I initially did not give out of concerns you could use it to cheese
the puzzle. If you find all squares that pieces *could* move to,
and construct a regex to pull out exactly 22 letters, the extraction phrase is mostly readable
from nutrimatic. Testsolvers told me that knowing they were missing so many
checkmates was important for them to conclude that they did not have an important
idea. Only 5 mates were findable without discovered mates, and you can assume
that you're missing 1-3 if you've made some mistakes, but missing 3/4ths of the
checkmates forces you to eventually confront the possibility you're missing
an entire class of solutions.

The 2nd testsolve of the full puzzle went well. At this point the grid fill had
been cleanly tested twice, and the chess step had been messily tested twice and
cleanly tested once.
I was looking to do a 3rd testsolve, but was told that our testsolve bandwidth
was running low, and this puzzle did not need testsolves as badly as other puzzles
did.

By this point, the puzzle had become a bit of a meme. About 1/3rd of teammate had testsolved it and another
1/3rd had heard a rumor that there was a "five-dimensional puzzle". We had another
puzzle called 4D Geo, and it was funny to realize that our hunt had a 3D puzzle, a 4D puzzle,
and a 5D puzzle.
"We have too many dimensional puzzles, non-2D puzzles are banned."

The chess factchecking was easily the most terrifying factchecking experience of my life. When I'd
set up my Python rules engine, I had ignored rules around timeline ownership in
5D Chess, but they were actually criticially important to the correctness
of the chess position. Now that I didn't even trust my code to be 100% correct,
I decided to install a 5D Chess mod that would let me load arbitrary game
positions from JSON.
Unfortunately, the game is hardcoded to only work for boards up to 8x8, so I could
only verify parts of the puzzle in-game.

None of the testsolvers had bothered reading the rules around active and
inactive timelines, nor had they checked any of these edge cases:

* Leaving a timeline unplayable for the other player by not moving in that timeline.
* Moving two pieces to coordinate a checkmate that was not doable by either
piece individually.
* Forking two kings on an active board.

I did not realize any of these edge cases existed until after the puzzle had
finished testsolving, and any of them could have threatened the correctness
of the puzzle in a way that would have forced a complete rewrite of the
chess position.

It took me another month to analyze the puzzle enough to conclude that none of the edge cases were an issue. For the first, I had won a 50-50
coinflip with having black own the 2nd timeline instead of white. For the second,
there were no issues, but I wasn't expecting that to be an issue. The last edge
case was 1 square away from being a problem, but wasn't. In retrospect I think there
was a 60% chance the puzzle was wrong, and I got *very* lucky I ended up in the
40% outcome instead.

In our PuzzUp setup, we can label puzzles by how difficult their postproductions
will be. This puzzle was flagged right to the top. The puzzle was short, but
the solution was an 8000 word monster filled with diagrams that our
auto-postprod tool failed to handle.
Many thanks to Holly for factchecking all the crossword clues, and Evan for
cleaning up all the chess diagrams with hundreds of lines of [Asymptote](https://asymptote.sourceforge.io/) code.

I'd say this level of excess is why I consider
this my most important puzzle. It is this perfect encapsulation of what it's like to fall down a rabbit hole by taking a joke way too far, and come out the other side with a coherent, self-consistent puzzle.
Only two teams solved it forward, but given how many free answers there were
and how scary the title is, I'm really not too torn up about it.
I'm really not too torn up about it. I got most of my enjoyment from making it,
not watching it get solved. (It helps knowing that one of the forward solvers said it made their top 5 puzzles ever.)

The one regret I have is the difficulty of finding the discovered mate.
Testsolvers told me to keep it without hints, but I think there was a bit of
positive bias, since this came from people who finished the puzzle.
As a standalone puzzle, I believe it is fine without changes. In the broader
context of Hunt, including an example that was only solvable with a discovered
mate would have made the ending a lot smoother and still given most of the a-ha.
</div>


# November 2022

## AH YES, WEBSITES ARE A THING

There were a...concerning number of things to figure out for the Hunt website.

A lot of it was classic stuff. Figuring out how to make the Factory round page meet the design
spec, supporting different site themes per round, etc. The problems were straightforwardly
solvable. There were just a lot of them and concerningly few people to work on them. My understanding is
that a lot of people who planned to do tech were either busy with real life, or sucked
into the Conjuri rabbit hole. This left a lot of infra tasks on the table.

I offered to go more all-in on tech and starting picking up more tasks.

## Events

We decided events would act like globally unlocked puzzles, with some event-specific metadata to
manually open them when the event was completed. Although events would not contribute to a meta of
any kind, having them act like a puzzle with a submitted was the easiest way to handle accounting
of which teams had solved the event. We would not need to track which teams showed up to each event.
Instead we could just announce the answer and watch the submissions come in later.
Pretty much every recent Mystery Hunt has used this system. It just works.

If there is no events meta, what should events do? EICs decided that
events should act as either free answers (manuscrip) or free unlocks (Penny Passes). Free answers
would only usable in non-AI rounds, and free unlocks would be usable everywhere. We wanted event rewards
to be useful at all stages of the hunt, but didn't want teams breaking into the AI round answer gimmicks
from free answers. (I think most teams did end up breaking into AI round gimmicks using free answers,
which was unfortunate.)

I asked some pointed questions about edge cases. How do free unlocks affect meta unlock? Is all of Museum
one round for unlock purposes or not? The real reason I was asking was because I wanted to figure out
the implicit requirements list for the next major task.


## Unlock System

gph-site and tph-site use a concept called "deep". I don't know why it's called deep. It might be from
GPH 2019 and the name stuck? In any case, "deep" is a measure of how far you are in a Hunt, and is used
to decide when puzzles should unlock. How that works can be done in different ways.

Mystery Hunt 2022 chose to have a fixed, single track unlock order. All puzzles, both feeders and
metas, were on that track. Mystery Hunt 2021 had per-round tracks, where each puzzle gave a lot of
deep to its round some deep to other rounds. In that hunt, deep was called JUICE.

Our Hunt was looking like it would be more like 2021. AI rounds were going to have
independent unlocks, since in-story each hunt was written by a different AI. But, we did want to track
total AI round solves, to decide unlocks of new rounds and change the speed of animations in the Puzzle
Factory. The Museum was slated to have a shared unlock track between all rounds, but with a min-solves-in-round threshold for metas. Factory rounds were
going to be mostly normal, but would be slightly entangled with Museum solves to decide the initial round
size (a team that solved the loading puzzle later would have fewer Museum puzzles left and should get to
unlock more Factory puzzles).

The desired properties were changing quickly, and
I decided that, you know what, we don't know what our unlock system is yet.
So, I am just going to implement the most generic unlock system I can think of. I arrived at what I'd
call the "deep key" system.

* Every puzzle has a deep key (an arbitrary string), and a deep value (an arbitrary integer). A puzzle
unlocks if the team's deep for a deep key is at least their deep value for that key.
* By default, solving a puzzle contributes 100 deep to the deep key for its round name.
* This default can be overwritten at either the round-level, or the puzzle-level if certain puzzles
need to do special things for unlocks.
* Puzzle solves can contribute any amount of deep to any number of deep keys, but puzzle unlocks
are required to rely on just one deep key.

To justify this system, I gave examples of how to implement all of the proposals.

* Museum rounds: Every museum puzzle contributes 100 to "museum" and 1 to "X-meta" where X is its round. The meta for each round has deep key "X-meta" and all other puzzles have deep key "museum"
* AI rounds: each AI round puzzle contributes 100 to its own round and 1 to "act3-progress". New AI rounds unlock at N act3-progress.
* Breakout / Loading Puzzle: all starting Museum puzzles contribute 1 to "factory", and the starting set of Factory Floor puzzles could open at 1/2/3/etc. depending on how we wanted to balance the start of the round.

This is essentially a generalization of deep.
Each puzzle's database entry only needs to store 2 fields (deep key + deep value for that puzzle),
making it easier to scan in Django admin. The heavy lifting of the unlock logic would then be in Python.
In the design notes, I mentioned that
arbitrary string keys could potentially be hard to maintain, but I expected us to have at most 20-30 different
keys and for the key names to be mostly self-explanatory.

I didn't get any pushback on this proposal, and spent a day or so implementing it. Along the way, I implemented a "deep floor" system, to use for time unlocks. A deep floor is a databae entry that says, "team X (or all teams) should
always have at least Y deep for deep key Z". Floors were created in a default-off state, then turned
on manually whenever we wanted to unlock something. I also set up a magic endpoint that a team could
hit to opt into a deep floor, to recreate the optional time unlock system that Palindrome had in 2022.
(I asked Palindrome's team where this was implemented in their codebase, and they told
me it was implemented very rapidly during Hunt and I should just redo it on my own.)

One important point of clarification: in our Hunt, there are actually two tracks of progress. One
is deep, and describes puzzle solve progress. The other is story state, and tracks progress in the story.
The two tracks were handled independently, and this would be the source of many fun bugs throughout
the year.


## Reactivation

Before I finished setting up the flow for automatically creating deep floors for all teams that could use
them, Ivan reached out and asked if I could help on making the
Hunt "feature complete" by December 1st. This was an internal deadline we'd set for creating a minimally
viable Hunt, where a team could go in, solve puzzles, progress through the Hunt story, and reach the end
of Hunt (assuming they solved all puzzles immediately).

When I looked into the TODOs for Wyrm and the midpoint of Hunt, there was a rather terrifying list of things to implement.

* How do we represent repeated Wyrm puzzles across the Museum and Wyrmhole?
* How do we handle physical puzzle pickup and other team interactions?
* Reactivation includes an unscheduled interaction with the team, we need to handle that properly.
* Shutting down the Factory, and connecting story state to Factory display logic in general.
* Implementing teamwide dialogues and connecting their resolution into updates to unlock and story
state.

I decided to take the Reactivation and Wyrm issues, while Ivan set up the dialogue and Factory
logic ("it's just Kyoryuful Boyfriend all over again").

The Reactivation logic was *wild*. The plan for the midpoint story flow was:

* Teams talk to MATE as they reconnect the old AIs
* Mystery Hunt gets cancelled.
* teammate stops by the team HQ for the in person interaction.
* Only after this point should teams unlock anything tied to Act III.

Note there are 0 puzzle solves between any of these events. It all needs to be handled via story state
triggers.

Tied to this was supporting live updates of round pages without requiring page refresh. This was
originally labeled as a "optional but nice to have" issue, but I raised the point that shutting down Mystery Hunt *would not* work if the site did't live-update. Imagine that you talk to MATE, and the conversation stops, but
the Factory is still brightly lit and everything is interactable. Only when you refresh the page does
everything go dark. Doesn't that really break the immersion?

Ivan went "oh shoot you're right" and we immediately bumped it up to site blocking.
To support it, I wanted to make as few changes to our site code as possible.
The frontend gets its state initialized by an API call against the server that populates various
React components. The solution that would change frontend code the least was adding a websocket-based trigger
that said "resend your initial API call and update your component state with the reply."

"Wait, do we need to worry about self DDOS-ing our site"?

Assuming an entire 100-person team was watching the story, we'd get a quick burst of 100 requests. That
seemed fine. What would not be fine would be if every tab from that team fired at the same time, since
that looked more like 3000 requests at once. To handle this, the site only resends the API call if
the tab is active. We then added a listener that delayed any other API requests until the tab was made
active.


## "Wyrm is a Special Child"

Near the end of Hunt, one of the team leads told me a secret: Wyrm was intended to be the AI round
with the least tech work required. Bootes and Eye needed weird answer checkers, and Conjuri was clearly
going to be a lot of work, but Wyrm would mostly have normal puzzles.

This...did not end up being true. Eye ended up being the most straightforward round, since its
round implementation was essentially the same as Museum, and diacrtiic / language canonicalization
hell was less hellish than expected. In comparison, Wyrm had an endless maze of edge cases. Puzzles
needed to be metas or not depending on round placement, which meant adding Wyrm specific API call.
Art wanted Wyrm's avatar to change with round progress, which meant a Wyrm-specific avatar path.
The list of puzzles for Wyrm needed to convey its subround nature and looping nature, which meant
a Wyrm-specific list of puzzles page.

Then there were the shared puzzles between Museum and Wyrmhole. After teams had their Reactivation
interaction, the repeated puzzles from Museum would appear on the Factory Floor. Solving enough
of them would unlock an interaction with Wyrm, and doing that Wyrm interaction would move the puzzles
from Factory Floor into the Wyrmhole.

This meant the same puzzle would be displayed in up to three different contexts.

1. As a puzzle in the Museum.
2. As a puzzle on the conveyor belt in the shutdown Factory.
3. As a puzzle in the Wyrmhole.

![Three views of Natural Transformation](/public/mh-2023/natural_transformation.png)
{: .centered }

Each context needed to serve the puzzle from a different URL, have a different puzzle icon, and
be styled differently. Additionally, we decided that the errata and hint
state should be identical across a puzzle and its copy, but their guess states should differ.

We ended up representing each puzzle in the database twice, once as its Museum version and once as its
Wyrmhole version. The Wyrmhole version would have a special "canonical puzzle" pointer to the Museum
version. To handle the conveyor belt, the Factory puzzle API call was hardcoded to shove Wyrm
puzzle data into its puzzle list if teams were in that ephemeral moment between shutdown
and talking to Wyrm post-shutdown.

This was still not all of the Wyrm edge cases. It was easiest to unlock the Wyrmhole immediately
after shutdown, but we needed to make sure free unlocks couldn't be used until after the Wyrm
interaction. The list of puzzles also needed to redact the Wyrmhole title, which propagated to
redacting it in the list of rounds in the navbar. This also all needed to be tied to story triggers,
not puzzle triggers, since the state change needed to occur during the Wyrm interaction, where
no puzzle was getting solved.

In short, there was an inordinate amount of work done for a section of Hunt
most teams sped through in 3 minutes. Of the Wyrm emotes, :sadwyrm: was definitely used the most,
whenever "yet another Wyrm edgecase" appeared.

![Wyrm sad](/public/mh-2023/wyrm_sad.png)
{: .centered }

You might ask, was this all *really* necessary? And the answer is pretty much, yes, it was. Remember what
I said earlier: the way you make a Hunt story tick is by considering every aspect of Hunt and making
it self-consistent with where solvers are in the story. I don't regret the work needed to make all of
this happen, but it was a lot of work.

## Interactions

After integrating spoilr code into tph-site, a lot of the flows around interactions had leftover bugs
that needed to be fixed for our Reactivation use case. The Reactivation interaction needed to get
connected to the Reactivation solve, and resolving the interaction needed to auto-advance story
state. Discord alerts for all these events were also set up at this time.

![Interaction](/public/mh-2023/interaction.png)
{: .centered }

For other parts of the Hunt logic, it was easier if the cutscenes with MATE contributed to deep,
since this made it easier to force the cutscenes to be required for hunt completion. ("Create bottlenecks, and direct
team attention to those bottlenecks.") That got implemented along the way.



# December 2022

## The End is Never The End is Never The End is Never

Hunt was becoming more real - people outside teammate are talking about it, wow! Boy they sure seem excited.
Meanwhile I was working longer and longer hours on Hunt. It was decidedly crunch time and I was starting to
take days off my regular job to work on Hunt instead. (My job caps vacation hours and I was near that cap,
so I was going to take the day off regardless.)

I was feeling like I was always spending my time rushing for something - for work, for Hunt, for this blog,
and I had not had the time to stop.

There was a [robot learning conference](https://corl2022.org/) in New Zealand this year, which I was
attending for work. I let people on teammate know that I'd be away from Hunt for a week attending the
conference, then away for another week when taking a *real* vacation from everything.

But before that:

<div class="shaded" markdown="1">
### Bedtime Stories, Castle Grounds, Easy as Pie, Fire Starter, Follow Me, Robber Baron

![Easieest +6 of my life](/public/mh-2023/blank_wyrm.png)
{: .centered }
</div>

<div class="shaded" markdown="1">
### Walking Tour

We'd actually started ideating this puzzle at a Bay Area meetup in October. It was a pretty
interesting meetup, since the first thing we did was testsolve Hall of Innovation. We were given most of the
feeders for the Factory Floor meta, and told to testsolve it. After getting nowhere, the authors
said "by the way, your team has unlocked a new round", and we looked at the Innovation puzzles. We solved
all of Hall of Innovation, then were told "Congrats! The Factory Floor meta you started with isn't ready
yet, come back in a few weeks". Really one of the most
trippy testsolving experiences I've ever had. They were testing whether a team could solve Blueprint
without the Innovation answers. We didn't, which
was good, but it's still wild that we were told to testsolve a meta, was given an entirely separate
round of puzzles, and then were told we couldn't continue testsolving the meta we started with.

But! We are not here to talk about innovation in the Museum of Interesting Things. We are here to talk about innovation in the Tower of Eye. A number of Eye puzzles were written at in-person meetups like this one, since
they needed more manpower to brainstorm. One of the major design requirements was that every Eye
puzzle should be very strongly tied to its language. It was not okay to use Dutch in a token way. Dutch
language or Dutch culture needed to be tightly interwoven with the puzzle. The same was true for
every other puzzle in the round.

We considered many things: Dutch auctions, Double Dutch, something with
tulips, etc. A teammate mentioned KLM Royal Dutch Airlines gives out Delft Blue house souvenirs that
correspond to real locations in Amsterdam, showing the Delft Blue houses they owned. (We were at their
place. They didn't, like, carry Delft Blue houses with them everywhere.) There was a canonical list of KLM
houses, with lots of data: number, year of release, address in Amsterdam, and more.

For any puzzle constructor, having a canonical dataset is immedaite puzzle bait. That was how we arrived
at a Google Maps runaround in Amsterdam, where the first a-ha was that you were in Amsterdam, and the
second a-ha was figuring out KLM houses existed. During ideation, we found the airline published its own [KLM HOuses](https://apps.apple.com/us/app/klm-houses/id371664245) collecting app, which I still have installed
on my phone despite owning zero KLM houses.

The idea seemed straightforward, but we sat on constructing it for two months. It was then written at
another meetup in December. Much of the work was on defining what features to consider for each house,
arriving at "blue shapes" to avoid having to define what counted as a window. Data for each house was
put into a spreadsheet, then a bit of code was used to randomly shuffle different yes/no conditions
until we found a set of five conditions that gave out the correct binary. The directions between houses
were then written in parallel by each author, followed by a combination step to reduce overlaps in locations
and location descriptions.

Since the conditions were generated by code, it took us a while to notice the 3rd bit of every letter in the
answer was 1, an incredible coincedence. The question that was there was switched out to a different one
that was obviously true. The puzzle testsolved cleanly without changes.
</div>

With this done, I was officially flying away to New Zealand. On the flight over, I spent most of my time
writing up the Appendix of the 5D Barred Diagramless solution.

Funnily enough, there was a puzzlehunt at the conference! The puzzlehunt was a student outreach event,
but was open to all attendees. It also...had real prizes? The first place team got $400 NZD (= $244 USD),
second place got $250 NZD, and third place got $100 NZD. I knew there was going to be a puzzlehunt of
some kind, because the student organizers worked with a professor that collaborated with my team
at work. They'd reached out for puzzle advice, and I replied with both the standard
spiel ("testsolve everything") and a non-standard spiel ("decide how tightly you want to tie
your puzzles to the conference, and if you want your puzzles to be solvable if teams skip a poster session").
I had not seen any of the hunt prior to showing up, so I was free to do the puzzlehunt. It had all the classic
first-time constructor mistakes, but I had fun and ended up placing 3rd. I caught up with the organizers
later, asking why there were prizees, and they told me that there was a student outreach budget that they
were well under - so all the spare budget was turned into prize money. The $100 NZD I got was mostly
spent on food. The last few pesky dollars were sent to a pony convention crowdfunding campaign to help
them fill a COVID-shaped hole in their finances.

The vacation really helped - I needed a few days to get into the headspace of "you have no obligations today",
but once I did, it was nice to have no stress about deadlines for a bit. I also learned that if a trail
sign says "do not pass at high tide", you should check when high tide is instead of assuming things will
be okay.

IMAGE

Of course, I wasn't able to
fully disengage from Hunt. One night, I ended up idly working out the design for the last puzzle I'd
write for Hunt.

<div class="shaded" markdown="1">
### Win a Game of Bingo

I run [Mystery Hunt Bingo](https://www.alexirpan.com/mystery-hunt-bingo/). It's a tiny joke I
threw together in an afternoon. Other people on teammate had asked me if I was writing a bingo puzzle this
year, and I kept telling them I would if I had a good idea. I had not had any good ideas and stopped
thinking about it to work on other puzzles.

Well, it was December, and we were in our final puzzle writing push. The official answer claim deadline
was December 1st, but it had gotten pushed to January 1st. The list of available feeders was dropping
fast, so if I wanted to make a bingo puzzle, it was now or never.
It felt like an enormous missed opportunity if I didn't do something, so I figured I'd give it another
shot.

There was a previous [Bingo](https://puzzles.mit.edu/2021/puzzle/bingo/) puzzle in Mystery Hunt 2021,
based on scoring bingo boards based on that Hunt. I didn't want to do the same thing, but the
author's notes for the puzzle mention a different aspect of bingo: rigging the board by refreshing until you get a board that you
like.
Independently of this, I had considered making
my site pseudorandomly generate boards based on a given seed, to make it easier to share bingo boards with
other people. A puzzle about reverse engineering a randomization process had been in my mind for a while.
One of my hobbies is watching "video game science" videos, for lack of a better word, and there's been
some really cool work on manipulating RNG that relies on pretty non-trivial number theory.

<iframe width="560" height="315" src="https://www.youtube.com/embed/-9YLCoK5K6o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

This inspired the core idea of creating a seeded bingo generator whose generator could be reverse engineered,
then using the same seed on my website to do the last step of the puzzle.
I wrote up the sketch while on vacation, saying I could take any answer, but wouldn't be
free to work on it until later. It got approved, and I was given "a meme answer".
(The puzzle answer has appeared many times in the past few years, and was pushed
into our Hunt just to continue the pattern.)

My past
experience with blackbox puzzles is that you don't need to be very tight on designing the solve path.
As long as the solution is unique, solvers can usually find a way. That's not to say you don't think
about the solve path, but it is pretty hard for the puzzle to be fundamentally unsolvable once you
have a proof that the solution exists and is unique. I was pretty confident that I could bash out
the tech implementation in a day or two, since it was basically Collage but much easier.

The issue was that I had no idea how to design a unique solution, and I wanted to get it done ASAP so that
I could spend more time on tech. The puzzle needed help. Justin (author of
[Functional Analysis](https://2020.teammatehunt.com/puzzles/functional-analysis)) agreed to help out.
I explained the rough idea, as well as the cluephrase that I'd pretested on a few people. So we knew the
target seed length was 9 letters long.

This puzzle was in Conjuri's Quest, which was targeted to be an easier round. There were a few reasons for
this. It was the last round, and we wanted it to act more like a victory lap than a gauntlet. Since the puzzles
would be opened last, hard puzzles were much more likely to get completely skipped. The focus of the
round was meant to be the game, and it'd be better if more teams got to see the whole thing. Most
importantly, feeder release
of Conjuri was last due to delays in game development, so puzzle development time was shorter and
puzzles needed to bias towards ones that could be written quickly.
This doesn't inherently mean "easy", but it's usually faster to design a puzzle with one a-ha instead
of two.

Justin suggested that in any game of Bingo, there were going to be three parties: you, your opponents, and
the person calling out numbers. Nine splits into three nicely, so each letter could as a function on one of the three parties. That decreased the "depth" of the puzzle, since you'd only need to reason up to 3 functions deep
once you understood what was going on. He also suggested the core idea used in the puzzle: numbers in our
grid should satisfy some property X (like "even"), numbers in our opponent's grid should satisfy some disjoint
property Y (like "odd"), and the numbers called should be very biased towards Y. The functions should then generally keep property Y true, except for a small number that pushed numbers towards X, and we'd use that
to engineer a unique solution.

That's all very abstract, but based on that I decided it'd be easiest if our numbers started low and the opponent's numbers + numbers called started high, with some functions adding and other functions subtracting.
The functions needed to be noncommutative to give a unique order in the solution, which led me towards
permutations. I figured they were a class of functions easier to derive from the data.

As I messed around with permutations, I found I didn't have too much trouble designing the functions to
give a unique solution for our grid + numbers called, but I had a lot of trouble designing the functions
to make the opponent's grid unique. To reduce work at solve time, I tried
to have all functions apply identically to each grid,
but it was very hard to prove the opponent's grid would always win before our grid
in every combination except one. So, I relented, and allowed letters to act differently between the opponent's grid and the other ones. This increased the number of functions
from 26 to 52, which was scary (combinatorial explosion grows fast).
To combat this, the opponent functions were made much simpler to make them
easier to derive.

During design, we were planning to require all seeds to be 9 letters. Editors pointed out it
could be really hard to break in without a lot of data, since it'd be hard to
examine functions in isolation. We adjusted to 1-9 unique letters to make
it easier to get started.
The nature of most blackbox puzzles is that they start very hard and end
quite easy, and our thinking was that if you could enter 1 letter seeds, you'd be able to run any experiment
you wanted without having to solve past unknown intermediate transformations.
That ended up working exactly the way we wanted in testing.

When you win the game,
the puzzle directs you to reverse the winning seed, to point you to Mystery
Hunt Bingo, to solve the puzzle. This reversing step exists
only to fight wheel-of-fortune attempts. All the testsolvers tried to nutrimatic
from 3/9 letters, and the one [Hunt writeup](https://blog.vero.site/post/2023-hunt)
that mentioned Bingo did the same thing. I mean this in the best way possible:
y'all are too predictable.

If you check the Mystery Hunt Bingo source code, the incoming seed is passed through a hash
function to generate the seed for a PRNG, which is used to sample from the list of phrases.
This makes it practically impossible to reverse engineer a desired board.
However, as the site maintainer, I have
control over the order of the phrase list! The PRNG outputs a list of indices, so after locking
in the seed, I changed the order of the phrase list such that the generated indices
lined up with the right phrases. It's been a long long time since I've done any
cryptography, but I'm pretty sure this protocol isn't breakable even if you
checked source before Hunt.

The side effect is that right now, I can't adjust the phrase list without breaking my Mystery Hunt
puzzle. Sometime I'll get around to hardcoding the edge case or making a
standalone copy to be hosted in the Mystery Hunt archive. One of the two.

In the actual Hunt, teams got more stuck on the last step than I expected. I have the stats on time
between winnning seed and solve - they are all longer than the 2-4 minutes I wanted it to be.
We just never observed teams getting stuck on this step in testsolving, probably
because a lot of teammate knows about Mystery Hunt Bingo, and I do wish the
game win message was more direct about what to do.
</div>

I landed back in the US a bit after Christmas. With no plans for holiday travel,
I was free to help on physical production of a puzzle that was a long time
coming.

<div class="shaded" markdown="1">
### Weaver

They say all good stories end where they began. Weaver was one of the first
feeder puzzles I helped brainstorm, and now it was one of the last I'd help make.

"Make" is a very literal word here. I did very little of the puzzle *construction*.
That was mostly done by Brian, and my role was more about internal testing
of weaving patterns on paper.

Meanwhile, Brian had done some experimentation with hydrochromic ink on different materials.
Much earlier, we had read an Amazon review saying the ink was a bit chalky,
and you needed to spread a lot of it to cover up the text it was hiding. However,
since the paint dried white, you needed much less paint to hide white text.

The final version was wooden reeds of a tan color, covered with a coat of hydrochromic
ink, then regular white paint on top, and regular black paint on top of that. The result
very clearly felt painted, but since the entire strip was painted it wasn't seem
too suspicious. The background layer of white would disappear when wet, leaving
the regular white letters visible.
We assumed teams would not wet the reeds wet until instructed to, because if the paint
washed away, they would be in trouble.
(A few teams *did* start the puzzle by putting the reeds in water to make them bendier.
When they sent hints asking if they made a mistake, figuring out what to tell them
was pretty tricky.)

The October retreat was tested with wooden reeds painted by hand. This left figuring out
how to mass produce 80 copies for the live Hunt. Logistics people were looped in very
early, and they were not able to find a company who understood and could handle all
our constraints. That wasn't too surprising, but meant we'd need to do it ourselves.

Over multiple Bay Area meetups, different production prototypes were created and sanity
checked. The core idea in all prototypes was to build a stencil to send spray paint
through. There would be four different stencils, for front-white, front-black,
back-white, and back-black respectively. To maintain a consistent alignment, the reeds
would be held inside a wooden frame. This frame was iterated several times,
with the last one being two flat pieces of wood, each laser-etched with grooves where
the reeds should go. The frames would be held together with binder clips for easier
assembly and disassembly.
A teammate with access to a makerspace in Berkeley ordered a bunch of wood, to laser cut the stencils,
frames, and triangles for The Legend.

The wood arrived! It was then stolen from their mailroom. So they had to ship another order, which
pushed production back a few weeks. I'd ask "who steals a bunch of wood from a mailroom", but, well,
we were evidence that wood was in high demand. Maybe it's easier to resell wood on the black market.
(This is why people read my blog, for the riveting package stealing commentary.)

Once the first stencils were made, we tried making a proof-of-concept.
This trial showed that spray paint would dry and gum up the stencil holes really quickly.
We'd either need to wash the stencil regularly to keep the holes clear, or do something else.
If we had to wash the stencil repeatedly, it could not be made out of wood. Acrylic stencils
were considered, but another teammate offered trying their airbrush instead. This did not
have the same problem that spray paint did.

Most of us were off for holidays after Christmas, and those of us still in the Bay Area met up
over two days to do the final mass production push.

![Weaver production](/public/mh-2023/weaver0.jpg)
{: .centered }

The Weaver assembly line (or puzzle factory) was set up in a garage to
allow paint fumes to escape, and had many steps.

1\. The reeds were delivered in large rolls. One person would cut the reeds to a reference length using a
tree cutter.

![Weaver reed cutting](/public/mh-2023/weaver1.jpg)
{: .centered }

The reference length was the size of the frame, plus a bit extra to give leniency.

2\. The cut reeds would be painted with a coat of hydrochromic ink by hand. We did this by hand because it
did not need to be precise, but we did need to save ink.
The hydrochromic ink was manufactured in Europe, and we would not be able to ship another container of hydrochromic ink before Mystery Hunt. The ink we had was the only ink we'd get, and it needed to stretch until
80 copies were made.

![Reed painting](/public/mh-2023/weaver2_0.jpg)
{: .centered }
![Reed painting](/public/mh-2023/weaver2_1.jpg)
{: .centered }
![Reed painting](/public/mh-2023/weaver2_2.jpg)
{: .centered }

3\. The loose painted reeds were placed inside an "oven". This was a portable heater directed towards a cardboard tunnel, to dry the paint faster.

![Oven](/public/mh-2023/weaver3.jpg)
{: .centered }

4\. Once dry, the reeds were sent to a framer. Reeds that were too bent were thrown out. The remaining reeds
were placed into a frame, then sent back to the painting station for touch-ups. Hydrochromic ink applies
clear, so it's hard to tell if you covered every spot until it dries.

![Frame](/public/mh-2023/weaver4.jpg)
{: .centered }

A completed frame going through touch-ups.
{: .centered }

Every frame went through two painting passes. The funny part of hydrochromic ink is that it if you paint
it on top of itself, it makes the undercoat turn clear too, so even on the 2nd pass we
had to remember where we had painted before.

5\. After touch-ups, the frames were placed back in the oven for another round of drying.

6\. The airbrushing station would take dried frames, and airbrush the remaining letters.

![Airbrush](/public/mh-2023/weaver5.png)
{: .centered }

One person manned
the airbrush, and a second person kept track of which stencil to use, did a final factcheck the letters
were correct, and bagged the reeds into individual puzzles.

The airbrush was the bottlenecking step, and it took 8 people manning the pipeline to saturate the
airbrush. 4 painters, 1 framer, 1 airbrusher, 1 airbrush support / quality controller, and
1 cutter / painter. We ended up working 12-hour shifts both days to finish the puzzle, creating
83 sets. This gives a very rough estimate of 2.5 man-hours per puzzle.

We decided to livestream Weaver creation into our writing Discord, saying people could come
"watch paint dry". I don't think they actually saw much paint dry, since our camera angle didn't
leave the oven in-frame.

![Watching paint dry](/public/mh-2023/paintdry.png)
{: .centered }

I don't think many teams made it all the way through the puzzle without relying on the virtual version, which
was unfortunate. The mass produced version was less readable than the paint-by-hand version. My guess is
that some paint was lost by friction in the flight to Boston, and more was lost during weaving, so a
bunch of teams lost too much paint by the end to do the last step.
Hopefully the intermediate a-ha was exciting enough
to make up for the deficits of material science. If a team asked for the answer after putting the weave
underwater and putting effort into the last step, we just gave it them.
</div>

## Email is Awful

We were trying to anticipate the tools that we would want
to have on Hunt weekend, since we wouldn't have time to make them during Hunt itself. One of these
tools was email. The tph-site codebase had an email flow, spoilr had another email flow, and
they had been *sort of* but not really merged. I did some plumbing work to connect them together,
and fixed some longstanding bugs in our interactions flow along the way.
The main piece added was predefined email templates for puzzles that ended on teams submitting
something to HQ, and standard hint templates for questions like "why is the loading animation
not finishing?"

The implementation for this was awful. I hope to never write code that writes Javascript via
Django's template language ever again.

## Wyrm (Continues) To Be a Special Child

![Wyrmhole, v0](/public/mh-2023/wyrmholev0.png)
{: .centered }


Now that more art assets were in, we could
finish setting up the puzzle icons. Puzzle icons in the round were non-convex and could be
disconnected (this is easiest to see on the layer of fish), and our plan was to make each puzzle
icon the same size as the round art. Each icon would have an SVG mask that would cut out the icon
and describe its click region.

One of the rules we'd placed on round art was that you should be able to read puzzle state
without interacting with the page. This rule was put in place to improve legibility of the hunt
state. In the Museum round, this was done by showing the answer below a puzzle's title on solve.
In the Puzzle Factory rounds, the highlight color of the puzzle icon would shift. A subtle change,
but better than doing nothing.

Neither of these fit for Wyrm. Any change would need to carry over when zooming between layers,
and it felt like it would distract from the work put into making the round art transition
nicely.

![Old Wyrm](/public/mh-2023/oldwyrm.png)
{: .centered }

Early iteration of Wyrm art
{: .centered }

We decided to break our rule, just this once, and make puzzle info only available on hover.
There would be a list of puzzles at the bottom of the page as a fallback.

This had all worked back in October, but when trying it again in December with real art assets,
the round was quite laggy. The problem was that in October, we had tested with 100 x 100
icons of differnt shapes, but now we were using full 1024 x 768 images, one per puzzle. Pop-in
was a lot more obvious.

To solve this, we changed out code to send a single image for the entire background. Then,
instead of brightening icons on hover, there would be six 100% transparent white rectangles,
cut to the shape of each puzzle icon. Hovering over each dive would toggle the transparency
to 50%. A white rectangle could be done entirely in CSS, reducing the network + render cost
from six 1024 x 768 images down to one.

This is leaving out many tedious details about z-index ordering, manual surgery of SVGs, code
that treated "fish.png" differently from "fish.PNG", and a long series of visual bugs. Suffice
it to say that I learned much more about CSS than I
ever wanted to, but somewhere along the way Wyrm had turned into my baby and I was going
to be damned if we didn't pull it off.


### Full Hunt Speedrun

I think pretty much every Mystery Hunt does this at some point. The idea of a full hunt speedrun is not to
testsolve the puzzles. It's to test the overall logic of the Hunt structure and site. Teams get all
the answers, go through the site, and report any bugs they find in the site behavior.
Our site had a ton of intermediate states, especially in how much of the Factory was accessible and
what the Factory monitors displayed, so this was extra important for us to check.

Much like the in-person retreat, the full site speedrun had its own set of secret testsolve objectives.

* **Breakout:** Our fully unspoiled test at retreat was "used up", but a bunch of the team
still didn't know how the puzzle actually worked. Now that we'd implemented all the post-retreat
nerfs, we could do partially-spoiled tests during the speedrun.
* **Hall of Innovation:** Testsolves of this round had always been done from a copy of the site
where only the Factory Floor and Hall of Innovation rounds existed. With more puzzles and art
assets complete, it would be harder to realize that the gizmos on the Factory Floor were what
mattered to Innovation. Parts of the meta also relied on art, and it would be good to get
a testsolve with the final art assets.
* **Wyrmhole:** Due to limited testsolve bandwidth, EICs had cancelled full-hunt testsolves.
Even if testsolvers only testsolved puzzles that hadn't been testsolved yet, it was looking
dicey if every puzzle would get two clean testsolves before Hunt...This is not a situation
any team wants to be in, but there were real TODOs more important than testsolving, like "fix
game-breaking crashes in Conjuri's Quest".
This was a major problem for Wyrm, since we'd planned to test the backsolve step in the
recently cancelled full Hunt testsolve. There had been some miscommunication between Wyrm authors
and EICs, where EICs though the last layer of puzzles would show Collage as the meta immediately
on unlock and that the last step was trivial enough to not test. After clarifying this was not
the case, it got added into the speedrun.

The more specific consequence of this was that a bunch of Wyrm art and tech deadlines were moved
up 2 weeks. I'm very grateful to the artists of the Wyrm layers for getting art done so quickly
after the new deadlines were conveyed to them.
All my work outside of Wyrm was spent on quashing bugs we'd found on our own before the speedrun, and
making sure our dryrun before the speedrun worked.

The speedrun itself went fine. We ended up splitting into three groups due to how many people showed
up.
Breakout got one clean test, one mostly clean test, and one group that
needed to be given the megahammer email (but which solved it immediately after).
My group got to Innovation, I recused myself to go eat dinner, and came back to my group
figuring out which gizmos went to which puzzle. This was considered "good enough"
"good enough" to fastforward them to the end.

This is an aspect of writing hunts that can be a bit disappointing. Your teammates write these really cool
puzzles that you get to testsolve, but you won't get to testsolve all of them.
Everyone testsolving all puzzles is super inefficient. But everyone *does* need to be spoiled on most
puzzles when writing hint responses and answering emails, so you are eventually forced to spoil yourself
on things you don't get to experience normally.

Once all three groups made it to Wyrm's last layer, they were merged into one big group. Between all
three teams, there were
only 7-8 people who
were unspoiled on the Wyrm gimmick.
Two people in the spoiled group set privste messages about noticing the looping puzzle in the art.
The unspoiled group eventually broke in by noticing the triangle pattern in all previous metas, and
looking for something triangular elsewhere, only noticing the looping art after backsolving
("oh god, we're dumb").

I was mostly relieved that all the parts of breakout and Wyrmhole had worked the way we wanted,
and that we could move them into the "done" column.


# January 1-8, 2023

## Oh No, Mystery Hunt is Soon

> There are decades where nothing happens; and there are weeks where decades happen.

As we get closer to the start of Hunt, I'm going to need to segment things more.


The full hunt speedrun had found many more tech issues than I thought we'd find going in.
"Wyrm should save its zoom level", "I hit a 404 on this link that showed up",
"Notifications are going off-screen", and so on. Each issue was prioritized and we went down them in order.

Meanwhile puzzle postprods and factchecks are getting merged in at an absurd rate.
Thank goodness for the auto-postproduction script, because boy were there just a lot of words words words to copy into code.
I caught up on writing a bunch
of solutions that I'd been putting off for a while, but left their postproduction to other people
since I was needed elsewhere.
I did end up doing code reviews for most of the postproduction
pull requests. It was a simple menial task to do, in between all the harder ones.

The plan was to do a second full hunt speedrun, the last weekend before Mystery Hunt.
Not everyone was free for the first one (given it was in the middle of the holidays), and it wouldn't
hurt to have another round of testing after we'd found more bugs.

However, by now I had shifted my focus to something complete different.

## Load Testing

This would be the third time I'd done load testing for a puzzlehunt. First I did it on my own for
[Puzzles are Magic](https://www.puzzlesaremagic.com/). Then I was asked to do it for Teammate Hunt
thanks to prior experience. And now I was still the person with most experience on how to set it up.

Puzzlehunts are not *that* big. If
anything they're quite small. My blog is fairly niche and got more page hits in a year than
Teammate Hunt did.
The difference is that puzzlehunts are very bursty in their load, where during the event you need to
be firing on all cylinders.
In my experience, I've seen two classes of errors.

* Your HTTP handlers are overloaded. In this scenario, requests to the server will be replied to very slowly,
but the server and database will mostly not go down. They'll just be very laggy. This is usually only a problem
at the start of Hunt. You can either ignore it, or make the server CPUs more beefy.
* Your websocket handlers are overloaded. The websocket connection layer in tph-site uses Django Channels
and is mediated by the database.
Having too many websockets open at once causes the number of active database connections to rise. If the
database connections go past a max limit, all new database queries will fail. This is a much worse problem,
because websockets are designed to be persistent connections, and the most likely outcome is that everyone
past the first N people will never be able to load the site. This is a configuration problem, not a
server-size problem, so upgrading the server is not enough to fix it.

I was not at all worried about the first, and quite worried about the second. All my previous load test
adventures were done using [Locust](https://locust.io/), since I learned how to load test from reading
example code in the [Puzzlehunt CMU codebase](https://github.com/dlareau/puzzlehunt_server).
This time, I decided to try [k6](https://k6.io/), since I had example load testing code from
Huntinality, and the documentation for testing websockets in k6 was
a lot more complete.

Broadly, the way all these load testing libraries work is that you define the access
patterns of a hypothetical user. "This user will login, wait a few seconds, open the list of puzzles,
open a random puzzle, and submit a guess." Something like that. They will then provide utility functions
that let you spin up users for some period of time, and provide statistics at the end. The main work for
loadtesting is in defining user behavior and making them send the same requests real users would.

I created a few different user profiles:

* Chatter: opens MATE, sends some chat messages, leaves.
* Collager: opens Collage, sends 1 guess into collage per second for a few seconds, leaves.
* Browser: opens each round page a few times, leaves.
* Jigsawer: opens the collaborative jigsaw from the loading puzzle, drags pieces to a new location
every 0.3 seconds, leaves.

The last was the one we most wanted to test. In the collaborative jigsaw, cursor locations are broadcasted to
every team member on the page, which requires O(N^2) messages per update, and they'd be sent frequently.
The update frequency was rate-limited to 0.3 seconds per update, but this would still be a lot.

<div class="centered">
<video width="640" controls loop autoplay muted>
    <source src="/public/mh-2023/breakout_loadtest.mp4">
    Your browser does not support .mov files.
</video>
</div>

One mock user doing the jigsaw.
{: .centered }

I got the loadtest script done just before the second full hunt speedrun.
For this speedrun, a small group of people acted as a mock HQ, and some puzzle answers would say
"send a hint to get the answer", "request a physical puzzle pickup to get the answer", "oops this puzzle
is impossible, redeem a free answer". This was to get all those aspects of site behavior tested as well.

Once again, there was a secret objective: tech would make a deploy to the site during the speedrun, to
measure the length of the deploy during active usage, and check how noticable the interruption was.
There was also an extra secret objective: I was going to spam the site with fake users during the
speedrun.
I'm pretty sure the only person who knew I was planning to do this was Ivan.

![The conspiracy](/public/mh-2023/50test.png)
{: .centered }

There was some slowdown on entering guesses on the site, but nothing too bad. There was much worse lag
in the collaborative jigsaw.
It was to the point where one person would
drag a piece, and it wouldn't change on anyone else's screen for several seconds later.
I confirmed I saw the same lag, then killed the load test. It was quite a sight to see 20 jigsaw pieces
all snap into place at once after the other load went away.

![Load test stats](/public/mh-2023/loadteststats.png)
{: .centered }

Brian was of the opinion that the stats would look much better if CPU load weren't at 100%, but he'd also take
a look at breakout and Collage to see if there were easy performance wins. He found some places to batch
websocket updates and add more caching. This made the next run of 50 users a lot better, but we
made a note to try again with the full-size machine.


# Monday Before Hunt

## Hello, Boston

I had taken (another) week off work, but this time it was more justified. Even in a year where I'm
not writing Hunt, I have trouble being productive the week before Hunt starts.

A bunch of teammate was flying in for Hunt. I left the Bay Area Monday morning. We'd been warned
our Tim Tickets wouldn't work until closer to the event (and also that the Tim Ticket app was a mess).

I was still in the mode of "optimizing how I use my time", so I prepared a bunch of work that I could
do without Internet to do on the plane.
It turned out my plane had in-flight WiFi (praise JetBlue), but I had trouble connecting to it from my laptop
for whatever reason. My phone worked just fine, so I ended up spending my flight doing code
reviews from my phone. As I tapped out a comment on my 8th code review of the flight, a part of me
acknowledged that trying to squeeze Mystery Hunt writing into this many aspects of my life was really not
a healthy thing to do.

For the week before Hunt, teammate would have an informal HQ in an AirBnB we booked near campus. Around
half the team would stay there, and the other half had booked hotels or were staying with friends. The
on-campus HQ would not be available until the Friday of Hunt, although we did get a classroom booked
the Thursday before.

When I landed, I asked for directions to the AirBnB, and was told the best balance
of money and time was taking the Logan Express.
So, I stood at the Logan Express stop, checking directions on my phone, and wondering why it claimed
taking public transit would take 5 hours. A few searches later, I found that the last Logan Express
of the night had left 10 minutes ago.

Shit.

Luckily calling a rideshare wasn't too bad, and I got there in time to see Ivan complain about
responsive CSS and "why can't all computers have the same size screen". What a classic teammate
puzzlehunt tech experience.

The Wyrm round art had a hardcoded
width/height that didn't fit on everyone's computer during the speedrun, and I had pawned off
fixing that to Ivan. This left me free to focus on connecting Conjuri, Terminal, and other interactives
into my load testing script. For me, this was the best trade deal in history, I will take infrastructure
over frontend any day of the week, but I did feel bad about it.

I finished writing up the 5D Barred Diagramless appendix I had worked on during the flight, then
went back to my hotel to sleep,

# Tuesday Before Hunt

## Factchecking

This day was a bit quiet, since not everyone had flown in yet. Most of my time was spent
addressing factcheck feedback to puzzles.

A few months back, we had set up PuzzUp to auto-generate a factcheck template spreadsheet for each
puzzle. Factcheckers would fill it out and it would auto-generate a comment in Puzzup when done.
I'm not sure how helpful the comment was, we hit message length limits often, but the spreadsheet
definitely helped!

The complexity in how we displayed puzzles on the site definitely contributed to difficulty
of factchecking and postproduction. I think this could be useful for other teams, so I'm actually
going to just
directly copy paste the questions in our factchecking template. (In retrospect, it looks a lot
like a prompt you'd give to a large language model.)

Instructions: Fill out this checklist. Completeness and Correctness are the most important points to check
carefully. If it does not apply, note it can skipped.

Completeness and Correctness

* With the help of the solution, I solved the puzzle 100% (all clues, no nutrimatic, etc)
* By using the solution, I did not need to think very hard to solve the puzzle.
* If there is "uniqueness" invoked in the puzzle, uniqueness is checked. e.g.. uniqueness in a logic puzzle, answers to clues, etc.
* Make sure facts are still true. **List info that could change on or before 1/13/2023 under Other Findings.**

Puzzle

* Copy to clipboard matches puzzle in both Firefox and Chrome
* Second person (if there's any person at all), present tense narration
* Flavortext is italicized. Non-puzzle info (e.g. controls, accessibility, and contacting HQ) should come with the "info icon"
    * Post-hunt note: this is a specific "circle" icon that can be seen on puzzles like Museum Rules
* Interactive components work for the intended solve path, doesn't crash on incorrect/malformed inputs, any shared team state is tested
* No unintended source code leakage (inspecting code doesn't spoil puzzle)

Display

* Puzzle is displayed (post-prodded) correctly and tested on small/large window sizes, interactions tested on both Firefox and Chrome (or note which browser was tested)
* Puzzle is displayed (post-prodded) correctly on a mobile device and fully navigable, if interactive, via touch
* Act 1 only: Puzzle looks correct in the void, i.e. no white on light text
    * Post-hunt note: after solving Reactivation, teammate shuts down Mystery Hunt. Museum puzzles could still be accessed from the Puzzle Factory,
but were put in dark mode (black background, white text) to signify that they were in a void "outside of puzzlehunt space", so to speak. Sometimes a puzzle would hardcode text colors that worked in light mode, but not dark mode.


Accessibility

* Do all images with puzzle content have alt text? If there's text in the image, can it be typeset with CSS instead of an image?
* Is the puzzle color-blind friendly? Does it rely on colors or can they be replaced with symbols or text?
* Do audio clips have transcripts when possible? Do videos have synchronized subtitles?
* For interactive puzzles, does it require very fine motor control? Is there an undo button if misclicks or typos will cause solvers to lose large amounts of work?
* Does the puzzle use inclusive language? If the puzzle talks about people, does it contain a representative sample (gender, race, age, etc.)?
* Print preview looks reasonable (can see all parts of crossword, clues, no unnecessary black backgrounds) on both Firefox and Chrome

Solution

* The answer is in Answerize format (bold + caps + monospace). All cluephrases are caps (not bold or monospace unless necessary).
* If puzzle involves images/visuals, they are included in the solution
* Solution is in first person plural ("we", "our", etc). It does not use terms like "the solver", etc
* There are no typos or grammatical mistakes in the solution.
* Solution is displayed (post-prodded) correctly.

Depending on the puzzle, there could be up to four different "versions" of it to check (the displayed puzzle,
its display in dark mode, its display in print preview, and its copy-to-clipboard). I think almost every puzzle I helped on failed
its first round of factchecking due to accessibilty or dark mode issues, so a lot of my time
was spent fixing CSS issues, updating solutions, and writing better alt text.

In the days before Hunt, I had stopped using my time tracker app, because it felt increasingly
useless - all my waking hours were spent on Hunt anyways, I wasn't going to get insight from recording
that fact.
When looking for suitable work music, I learned that Demetori, my favorite music group of all
time, had released a new album just 2 weeks ago. (They do Touhou remixes, msotly progressive metal.)
I spent most of the week playing it on repeat. The Youtube comments taught me facts no one should know ("corgi" is
edit distance 2 away from "orgy"), but I couldn't help thinking about the album's closer - "It's Better to Burn Out Than To Fade Away".

<iframe width="560" height="315" src="https://www.youtube.com/embed/iNnInh13tTc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


# Wednesday Before Hunt

## Load Testing

The tech work continues!

There were a rather worrying number of features left to implement, including fixing print previews,
cleaning up yet more bugs in interactions, and adding a Contact HQ + puzzle feedback flow.
Bryan had made a habit of speedrunning the Hunt daily for the past week, surfacing
new site bugs whenever he found them.

If we were not implementing it so late, Contact HQ would have been done properly, but we were doing
it late and all bets were off on code cleanliness. Internally, the Contact HQ page is implemented
by having an internal, unsolvable puzzle, that does not show up in the list of all puzzles, which
starts with hints unlocked, and has no cap on number of open hints.
Is this awful? Absolutely. But, it was the fastest thing to set up and we were at the point where everything needed to be speed.

Speaking of speed, I was testing Conjuri's speed under load. I had zero idea how the game worked,
or even how to fight any of the monsters,
but I got some pointers from the main Conjuri devs for how to start games and move around. The
load test was to login, connect to a Conjuri slot, then send a bunch of move commands (10/second,
roughly the speed they fire if someone holds down the arrow key). Along the way I found a security
flaw that could have allowed teleporting anywhere in the map, including past locked doors (oops).

My load test script would sometimes enter monster fights, and then immediately get stuck because
you can't enter move commands in the middle of a fight. I decided this was fine. The goal
was to test load, and the server responded with a no-op when given a move command that it couldn't
apply. This still forced a reply of *some* kind for load testing.

Brian had provisioned the full-size hunt server, and I hit it with 250 users. This immediately
had errors, but the site seemed fine, and the error message codes I was seeing looked
more like networking issues. Having seen a similar error when loadtesting Puzzles are Magic, I asked if our AirBnB WiFi could handle
250 fake people at once, and then immediately answered my own question with "no of course it can't".
(When loadtesting Puzzles are Magic, I did not realize this class of error could exist, and spent
two days trying to fix a bug that didn't exist, which was a painful experience.)

Alex Gotsis had a spare DigitalOcean machine they used for personal stuff, and we moved load
testing there. The new 250 user test went fine. Great! Let's scale that up to 500 users.
The CPU load looked fine, but we started seeing real site errors, ones that were breaking the site
for other people on teammate.

Oh dear. I left it up for a bit so we could collect more stats, then turned it down. The errors
were identical to the ones I'd seen when loadtesting Playmate from Teammate Hunt 2020. It was
definitely a database connection issue. I went back to our 2020 docs (thanking the gods that
we'd decided to write documentation in 2020), and told Brian to try setting up
[PgBouncer](https://www.pgbouncer.org/) and optionally swapping our Redis layer in Django Channels
to RedisPubSubChannelLayer (since someone on Huntinality had mentioned it made their load look
better). While Brian set that up, Alex Gotsis came by with some news.

He explains that the site sends us an email whenever we have a server error. This is the mechanism
we use to make server errors more noticable for debugging.
Sent emails create and save an `Email` object in our database, to integrate with our email handlers.

Now, suppose your site has no available database connections. It'll error out. The server will send an email
to tell us about the error. This creates an Email object, which will fail to save because there
are no databaes connections left. So the site has another error. Which sends another email. Which triggers
another failed save. Which triggers a 3rd error. Which...you get the idea.

I said "Oh no", and he said "Don't worry I'm fixing this". Once that was fixed, we removed a
few thousand failed-to-send emails from our Hunt database and everything was fine. Phew.

After all the websocket improvements were in, I reran the loadtest and the site no longer
broke at 500 users. The switch to RedisPubSubChannelLayer also appeared to cut our CPU usage by 3x, which
was cool. Between both of those, I was willing to declare the site battle-ready.

The one issue flagged was on Conjuri. All the Conjuri instances were running in the same
server as the rest of the Hunt. The codebase had evolved that way and we did not have the
time to refactor it to a separate machine and adjust our deploys accordingly.
I gave my opinion that 500 users continually submitting guesses and moving 10 times per second
in Conjuri was likely a big overestimation. I was pretty sure our true load would be a lot
lower, and not too many teams should be on Conjuri at once anyways.
That sounded fine to Conjuri devs - or at least, the game bugs to fix were more obviously
important than hardening against load.

## Physical Things

While all this tech discussion was happening, there was a flurry of work on set construction,
driving things from A to B, and building all the physical parts of the runaround. I got pulled into
a charades testsolve in between some tech discussions.

There were an incredible number of flight delays in the week before Hunt, which was quite scary
for us. because every copy of Weaver was with one person. Thankfully their flight was fine.
With loadtesting done, I could go back to some TODOs I had for the Wyrmhole round art, but
before that point I stopped by the physical puzzle station and said, "I want a mindless
task please give me something to do."

"Great! Cut all these pieces of paper."

It was for Tissues and I spent an hour using a paper cutter to slice large sheets into smaller
sheets, and folding flat tissue boxes into box-shaped boxes.

Meanwhile...

> I'm about to wreck y'all's day.
>
> I probably can't drive tomorrow.
>
> I am currently largely fine, so don't flip out, but as I was walking back to my car, a car hit me
> in a crosswalk.

THANKS UNIVERSE THAT WAS A REALLY FUN CURVEBALL YOU JUST THREW. Luckily the driver was very apologetic
and nice and the teammate hit was fine, aside from needing to have a leg in a cast for a while. We had
enough spare cars that it wasn't a big deal for our logistics.

## Wyrm Art, Again

![Wyrmhole Final](/pubilc/mh-2023/wyrmhole_final.png)
{: .centered }

Ivan had figured out how to make the Wyrm round page scale with screen size, but there were a few
lingering Wyrm art requests to handle.

First, switching out the zoom buttons. To fit the papercraft aesthetic, the buttons were made by hand - as in,
they were literally pictures of construction paper, laid out by hand, then photographed. It was a bit surreal to
be working on a tech task, then glance over at a real-life instantiation of an art asset I knew I'd need
to add into our hunt site an hour from now.

Second, the background. There was yet another Wyrm edgecase needed, this one to make sure the triangular
grid in the header lined up with the grid in the rest of the background. For reasons I don't want to
get into, this did not work by default and needed to be hardcoded specifically for Wyrm, keeping
up the theme of "Wyrm has an edge case for literally every part of Hunt"

Last, the hole. The puzzles were supposed to appear out of the jagged hole. This required adding one final
SVG cut out to the sight, slicing the outer rectangular image into the shape of the hole, and making
sure it aligned properly *with* the hole. With this, Wyrm was now truly, finally done.


# Thursday Before Hunt

## Hunt Hunt Hunt Hunt Hunt

Thursday was the first day that we had a room on-campus. We were also
starting to spot people who definitely-looked-like-Mystery-Hunters
in nearby hotels.

This time, we convened in a classroom on-campus, instead of the AirBnB.
We first had a too-long conversation about whether we should draw the blinds on
the windows to our classroom before we went into an incredibly spoiler-heavy
presentation. The decision was "no that's overkill". We then had a presentation
about the story of Hunt, structure of the rounds, and different responsibilities
that would need to be filled to man HQ. The previous day, a spreadsheet
had been shared, and people were asked to fill it out, but today that request
was now a demand. "Sign up for a slot or we will *find* a slot for you."
I checked and had been pre-signed up for "tech on-call / contact", so problem solved.

We then had a training session for how to use the hint and interactions
interface, which I mostly tuned out because I'd been fixing bugs there for several
weeks. Team T-shirts were distributed, all with a picture of MATE on the back, and
we were told to make sure we wore a hoodie or sweatshirt, something that hid the
back of the shirt, until MATE was revealed at kickoff.

The remaining time was allocated for people to finish up any last minute tasks for Hunt.
I went through the list of tech TODOs, and it was very clearly at a point where we
would not be able to finish all of it before Hunt. A lot of features were cut at this
stage, including solve sounds.

This made some people on teammate quite sad, but there were just too many other things. "We don't have bandwidth to do this. If you have bandwidth to help on tech, we'd prefer you'd work on these prioritized issues, but if you implement a deprioritized feature and send a PR someone will probably review it."
And that's why the Hunt has solve sounds, someone decided to do just that.

Mystery Hunt is an enormous thing. It is big enough that even team leads are not
aware of all the things going on.
Nor should they be. The coordination overhead
would be too big if they were. But, the natural consequence of this is
that sometimes, people's beliefs of what is getting done differs from what's
actually getting done. Tech is just one example of this. I
got passed copy of the MATE tutorial page that would be viewable before puzzle release,
and ended up rewriting a bunch of it because it suggested using chat messages that would
not pass our hardcoded parser. (The plan to try an ML-based intent classifier
instead had long been abandoned.)

This is approximately the level of chaos I expected the day before Mystery
Hunt, so I wasn't too concerned. Logistics got ironed out, more bugs got
fixed, and we'd made progress on the site. We started organizing our tech on-call
shifts, and I signed up for the 1 AM - 7 AM shift originally, before reconsidering and asking if I could take the one *ending* at 1 AM instead.

## Run (Slowly) Towards The Exit

As the last action item before Hunt, we did a "slowrun", where instead of solving
puzzles as fast as possible, a group of us would solve puzzles one at a time,
pause at every meaningful state change, and verify the site's behavior matched
expectations.
The original target was 7 PM, but it got pushed back, and back,
and back...

The delay was bad enough that we were going to lose our room reservation on
campus. Everyone not involved in verifying the slowrun was told
to get some sleep before Hunt, while the rest of us headed towards the AirBnB
HQ. It was going to be a long night.

The slowrun officially started around midnight. Over several hours, we
proofread all the site pages, checked that puzzles and round
pages unlocked properly, that the Puzzle Factory was gated properly, that
story updates and interactions appeared at the right time and had the correct
art assets and links, and more. There were around 10 "major" stopping points
where we would go through the entire site, both Museum and Factory, to check
nothing unlocked early and nothing unlocked late.
We also checked display issues similar to the factcheck spreadsheet, like
site functionality at different screen sizes. As we discovered bugs, we discussed
if they were worth fixing, or if the site was going to go as-is. With some
final visual touch-ups along the way, the slowrun finished around 8 AM.

Kickoff was in a few hours and some people in the slowrun needed to go straight
to rehearsal. I was not one of those people, had been up for a solid 24 hours,
and decided to find a corner to take an hour long nap before Hunt got started.
I really should have just gone to bed, but it's the first Mystery Hunt kickoff
after two years of remote Hunts. Proper sleep can wait.


# Friday of Hunt

I arrived at Kresge a bit out of it, but intact enough to say hi to people lining up outside.
I didn't have too much to say - talking about Hunt is something you do after, not before.

One very minor upside of pulling an all-nighter is that I did not have to remember
to bring my "Museum of Interesting Things" staff T-shirt. I had put it in my
bag yesterday afternoon, and then never made it to my hotel room, so I could just
put it on directly (along with a hoodie to hide MATE on the back).

While ushering people into the auditorium, I got the sense it was about as full as
previous years, which I wasn't quite expecting. I figured there'd be less presence due to
COVID concerns, but it didn't look that way to me.

I saw kickoff for the first time with everyone else; I'd been too caught up in tech to
see any of the rehearsals. The stage was decorated with a bunch of random decorations
people owned or bought from Home Depot while getting construction supplies for set
creation. Based on all the people taking pictures after kickoff, it looked like
a few hunters were trying to image ID the decorations, just in case they were a puzzle.
I've heard one group actually succeeded.

Now, we unfortunately did not have our official HQ in Building 10 available at the start of Hunt.
It would be available later that afternoon. For now, our HQ was two separate classrooms
that we'd reserved for a few hours through MIT Puzzle Club.
This complicated the start of Hunt. There are
just more communication
hurdles when people aren't in the same room.

Once we got to our rooms, we had a fun time trying to connect to the Wi-Fi. MIT was
applying a wireless network change to happen the Friday of Hunt. We were aware of this
beforehand, but MIT Puzzle Club both did not have the cachet to delay the change, and
was also assurred the migration would be fine.

Well, it wasn't totally fine, because two things happened.

* Anyone who'd connected to campus Wi-Fi before the transfer (i.e. literally everyone on teammate)
could not connect until they'd told their devices to forget the MIT guest network.
* Discord calls that worked yesterday didn't work today. We were able to figure out it
was a port issue - Discord uses a random UDP port from 50000-65535 for voice calls,
and the range was blocked on the guest Wi-Fi.

I had the fun time of needing to enter a code that was sent to my phone
or email to get my laptop connected, when my phone had no cell signal and I had no Wi-Fi signal.
I was lucky enough to suddenly get a tiny bit of cell signal, just enough to get the code
through, and successfully bootstrapped from there to full Internet connection. The Discord
problem was something we'd just need to deal with.

As puzzle release approached, people on teammate started poking around our internal HQ pages,
and immediately found a bunch of errors and exceptions on a bunch of them. This was disappointing,
but not too surprising. Our speedruns and slowrun were all very focused on the solver experience,
and we hadn't exercised our HQ backend
to nearly the same level of scrutiny. Tech immediately started preparing "hour one" patches, to
merge in after puzzle release was deemed stable.

Puzzles go live! And no guesses come in for a bit, which made us paranoid that our site had not
released puzzles properly. Then some guesses started coming in, and we breathed a sigh of relief.
T minus...we'll find out, until the coin was found.

I realize I have yet to describe what our Hunt weekend organization looks like. Here is the list
of roles:

* **Huntcomm:** People keeping an overall eye on the pulse of Hunt, comparing solve progress against
different forecasts, and in charge of decisions for deciding on interventions to either speed up
or slow down Hunt. Mostly made of people on hunt exec that were spoiled and had context on all of
Hunt.
* **Tech:** Self-explanatory. On-call to answer any questions about the site's functionality and implement
any fixes or feature requests needed to run Hunt.
* **Physical Puzzles:** Handles giving physical puzzles to teams that ask for them.
* **Phone:** Answer call-ins are gone, but we have a phone number! In charge of answering the phone.
We did not expect many phone calls, so this was merged with other roles.
* **Dispatcher:** The person to talk to if you are free and don't know what you can do to help run Hunt.
Tracks the point-of-contacts for tech, huntcomm, and other roles with on-call rotations.
* **Runners:** In charge of moving things between rooms. Generally not a dedicated role and only
important at certain points of Hunt.
* **Logistics:** Very similar to dispatcher, but not exactly the same. Roughly, dispatcher knows where
everyone is, while logistics knows what those people are doing. But if you want, feel free to treat
dispatcher and logistics as one big group.
* **Food:** Handled group orders for meals. Catering had already been ordered for the entire weekend
based on a team interest form from Thursday, but there were smaller bespoke group orders too. (Boba runs,
people who'd missed the food form, etc.)
* **Hints, Contact HQ, Email, etc:** Everyone else. In charge of replying to hints, emails, and other
communication channels between solvers and HQ.

These were not rigid roles, and very often work was quite fluid and picked up by whoever is free.

The phone was pretty interesting.
Two days before Hunt, someone on logistics told me story of going to MIT Puzzle Club's storage, and finding
10 phones in a closet. It took them a bit to figure out which phone was the correct one for the number
we'd advertised on the website. I'm guessing these phones date to when answers were called-in. Why would
you throw away perfectly good phones? That being said, we didn't expect to ever need to answer it.

Remember me mentioning we were seated in two different rooms? This meant tech was half-split
between rooms, huntcomm was half-split, and so on. Which makes things extra spicy when teammates
start reporting they're having trouble loading the site. We start with triaging whether this is
an admin-only issue or a site-wide issue. It's starting to look like a site wide issue.
Then the phone started ringing.
The room goes quiet as the phone person picks up the phone.

> "Hello?"
>
> "Yes, we know the site is down. We're working on it."
>
> ("Do we have an ETA for a fix?" "No, but tell them they don't need to spam-refresh the site.")
>
> "For now, please avoid refreshing the site too much, that'll make it worse."
>
> \*click\*

...Well, good to know the work spent finding the right phone wasn't in vain! It was also
funny to realize that there is a good reason to have a phone number in the year 2023: it's a good backup
when your site goes down.

The logistics person
in the room complains that conversation in the room cannot entirely stop if someone
else calls the phone. We need to be able to discuss spoiler-heavy topics while phone calls
are handled. The phone is moved close to the door, so that the person answering it can step outside
when fielding calls.

It does not take long for tech to figure out why the site went down despite our load testing. The default
behavior in tph-site is for all tabs from the same browser to connect to the same websocket. This
lets live updates on one tab auto-apply to all other tabs. When we set up the chat interface,
we'd made each tab use a different websocket, so that each tab could persist different chat histories.

The load testing script modeled a user as opening one tab, spamming tons of requests, then closing the tab.
This was not the right model for Hunt. The correct model is one user opening 100 tabs, then spamming many fewer
requests in just one of the tabs. Our server's CPU load is way under the load we saw in load testing,
while our number of Websocket connections is way over.

We discuss a bit, and given that our CPU load is only hovering around 30%, we believe the site will be fine if we
multiply all our Websocket handling numbers by a ton. If that causes use to run out of CPUs, we'll
handle that later. The "hour one" fixes were bundled with these config updates, and once deployed,
the site comes back up and stays up. The CPU usage also looks fine. Phew, crisis resolved.
(I just want to briefly point out that if we had not load tested as much, we would not have fixed
the email self-DDoS loop we had, and things could have been a lot worse.)

The main problem was that just like our internal testsolve at retreat, some teams would now surely
be assuming the loading animation was caused by server load, and was not a puzzle. But that wasn't
really fixable, we'd have to see how it played out. No one had solved the loading puzzle yet, but
most teams had not solved enough puzzles to get a long enough load time.
Before Hunt, I had not seen any projections of Hunt time,
but it did seem like people were going through the first round
pretty slowly.

This was part of why huntcomm recommended opening hints early, rather than at our planned time of
9 PM.
The way hints worked in our Hunt was that they would open when it was both past the global
hint release time, and a puzzle had been solved by at least N teams. The default setting for N was 25,
but this could be changed on the fly without redeploying the site.
It was still early enough in Hunt that teams had not separated much in number of solves, and
releasing with the default N = 25 would have practically opened hints on every puzzle.

As the tech point of contact for the half of HQ I was in, someone on huntcomm asked me to weigh in
on the plan of changing N to whatever value would make hints unlock for the first 3 puzzles of the Hunt,
which was N = 70.
I told them that yes, this would work, but if they did this, huntcomm would be signing up to continually
track and adjust N throughout the Hunt. Was that fine?

"Yes."

Okay then! But then the logistics person in our room immediately pointed out that we were about to switch
rooms to our official HQ location. Releasing hints at the same time as everyone moving rooms was a bad,
*bad* idea. Huntcomm wanted to release hints *now*, so we pushed moving rooms back by an hour and opened
hints immediately.
It was a gamble that no one would actually kick us out right when our room reservation ended, and that
gamble was correct.

As hints came in, it exposed a few more bugs in our HQ pages for hint management, which I started looking into.
We got our first emails from teams asking why puzzles were taking forever to load, and responded
with "Thanks for reporting this, we'll look into it. It's puzzling why it's taking
so long to load." And then we got our first official breakout into the Puzzle Factory! Hooray! The Puzzle
Factory reveal started cascading through the other lead teams, and this was as good a sign as any that
we could start cleaning up to move to our more permananet HQ.

A runner asked if I could carry the soda for Fountain. "It's in Matt.", they said, as they ran away.

Sorry, what?

We had named our temporary storage rooms Matt and Emma, and I did not know where either of them were.
Luckily someone else knew the way, and after doing a few trips we'd moved everything to Building 10
HQ. Back to hint writing.

Well, for most people, it was back to hint writing. A few teams were reporting that he
collaborative jigsaw puzzle was stuck, with one piece missing. We weren't able to reproduce the issue,
and we didn't have an easy way to reset the position of a single piece, the puzzle state was one
giant JSON blob.
The best solution we came up with was to manually skip the team past the jigsaw, by copying the
solved jigsaw state from the admin team into their team, and manually adjusting the story state
to mock the trigger they would have hit.

There was a new "stuck on jigsaw" email every 2-3 hours, so I wrote up a
quick playbook for what to do, before everyone wno knew the jigsaw backend code went to sleep.
We also started getting emails reporting some of the known issues that we'd found but
deprioritized the day before Hunt - no volume slider on solve notifications, accessibility issues
in navigating the Puzzle Factory rounds, no solve log, and so on. I alternated between replying to emails
acknowledging problems, and asking other web devs if they thought it was fixable during Hunt.

The volume slider got added during Hunt, and after a complicated deep dive through the code,
we figured out the complicated CSS bug that caused Factory puzzles to not be scrollable with
arrow keys. Fixes for both were sent out during Hunt. (Much later, a week after Mystery Hunt,
we learned that accessibility fix [broke printing on all Factory puzzles](https://devjoe.appspot.com/mitmh2023.html). In general, please contact HQ if you hit an issue with the site! We can't fix everything,
but this print issue only took 15 minutes to triage and fix posthunt.)

Huntcomm was still concerned about the pace of hunt, so they decided to give extra event rewards
for the first event.
They asked if there was a way to see how many event rewards a team had used, because they
wanted to see if teams were using event rewards or hoarding them. I pointed to the internal
admin pages that would show this, but realized event usage wasn't very legible and made a note
to address this sometime.

On sending the extra event reward message, we found that some of our emails were not going through.
Ever since [Teammate Hunt 2020's troubles with paid email services](https://2020.teammatehunt.com/wrapup#tech-email),
we've run email through a self-hosted email server. Broadly, this works until it doesn't. Somehow
we'd managed to hurt our email reputation, and emails weren't getting delivered to our Gmail. I
still don't know what happened. My guess is that when we self-DDoSed ourselves in load testing,
it caused our Gmail to treat our email server as a spammer? In any case it was very annoying,
we tried to fix it, it started working 6 hours later, and I'm pretty sure our fixes had
zero impact on making our email work again.

After I got dinner, Alex Gotsis came by, asked me how much sleep I'd gotten ("...1 hour?"),
and relieved me of duty so that I could sleep. I didn't leave, and an hour later he more directly
told me I *should* go to sleep so that we'd have full on-call coverage later.

I got back to my hotel room, but set up Discord alerts on event reward usage and puzzle feedback
before getting into bed.

![Me failing to sleep](/public/mh-2023/fridaysleep.png)
{: .centered }


# Saturday of Hunt

Sleeping so early meant I woke up much earlier in the morning than usual, and I arrived at
a mostly empty HQ. I caught up on overnight progress, then got back to answering hints.

In Palindrome's AMA about writing Mystery Hunt last year, they mentioned the initially unlocked puzzles made up around 30% of
all hint requests. That was looking true for us as well. When running a Hunt, it is very
common to get new teams that essentially want to be handholded through the entire solve. Of
course, we will answer the hints they send in, that's how those teams have fun,
but it did get a bit tiring when teams sent a new hint every time they wanted to confirm a single
cryptic in Inscryption or a single state in Museum Rules.
Luckily, answering hints has a natural "get better with experience" phenomenon, where after your
10th hint on a puzzle, you can quickly figure out where a team is stuck and what would be good
to tell them.

Things were progressing, albeit more slowly than anyone had expected. Someone opened
some of the soda set aside for Fountain. The physical puzzle distributor stopped them, and
said "Don't drink this soda, it's a puzzle", which is really the kind of sentence
you can only hear and believe at Mystery Hunt.

Running Hunt was simultaneously getting more boring and more exciting. Fewer things about the site
were on fire, which was good. More teams were solving the fun submission puzzles. Our first tirades
had come in Friday evening, but now a bunch of other teams were solving the tirades puzzle and
sending in even wilder speeches.

We did another round of team check-ins. Our first round was on Friday and went...okay. We had sent out
three groups of people, one with Mystery Hunt organizing veterans and two with newer people. The
new groups came back early, and the veteran group came back much later.
What had transpired was that the veteran group was much better at asking questions that got teams
to give real feedback and start a conversation about how their team was doing, rather than generically
making smalltask. The dispatcher (I think it was Edgar?) had that group write up a guide for to run
team check-ins, and I believe the 2nd round of check-ins went a lot better.
I had planned to go on one of those runs, but did not because I was the main tech person awake
that morning and needed to be on-call.

Speaking of tech, we had designed this entire interactions flow for team check-ins. There was
a magic button in our site, which would unlock a "check-in" interaction for all teams. People could
then mark check-ins as completed whenever they visited a team, reusing the work we'd put into
our hint answering flow. I asked how that was going...and most of the check-in infra wasn't used.

The main things missing from the dispatcher point of view:

* Interactions were ordered by team name, but the important feature was ordering by location.
Check-in squads needed to be sent to regions of campus and the site didn't support sorting by
that.
* Some teams had special instructions they'd sent over email.
Some were minor, like "knock first", but others were more important, like
"our team is very concerned about COVID, so we'd like anyone from HQ to mask up and present a negative
rapid test before visiting our room". This info wasn't always in our site's database.
* Bulk opening a check-in for everyone didn't do a good job at ignoring teams that had
self-reported they had stopped solving Hunt.

The people managing check-ins had decided to do everything over Google Sheets instead, which
was the right call.

Teams were still behind huntcomm's projections, so they handed out more event rewards. With
event reward alerting up, it was now very clear that lots of teams were stockpiling their
rewards. Teams were making progress, but we needed teams to use free answers *despite
making progress*. I understand why it was happening, but understanding didn't make it less
stressful.

<iframe width="560" height="315" src="https://www.youtube.com/embed/rgU4Oum8SLg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Huntcomm tried a lot of alternate wordings on the event page to nudge teams to use event
rewards more aggressively, and none did very much. Eventually the event page was updated to
say that free answers were usable in 8 rounds of Hunt, to imply there were more rounds
after that. At the time of the swap I believe teams were at most 6 rounds into the Hunt,
and there was a clear uptick in rewards spent after that change.

The event reward flood had also exposed new edge cases in our event handling code. In testing,
I had never tried redeeming multiple event rewards at once. Puzzles don't unlock immediately
in tph-site, they unlock lazily on first page load after you've advanced the unlock track.
The reward redemption page did not update the list of puzzles in between event usages,
so teams spending rewards on unlocks could be debited more rewards than they could use
in that round. I sent in a bug fix and we gave back event rewards to affected teams.

Normally, the first day of Hunt HQ is the most exciting, and then it gets more boring.
That was not true this year, due to all the pacing emergencies.
Huntcomm had already shipped a bunch of changes to unlock tracks, to open rounds sooner,
open metas sooner, and increase puzzle width in existing rounds.
They were now deciding that Hunt speedup needed to be even more drastic.

Anyone with an AI round puzzle was asked to brainstorm ways to pre-nerf their puzzle before
teams unlocked it, so that we didn't have to issue errata later. If we didn't come up with a
nerf, huntcomm was going to nerf the puzzle for them.

All of these nerfs would not be testsolved (everyone who could is busy answering hints),
so this restricted nerfs almost exclusively to flavortext nerfs or removal of
intermediate steps rather than changes to existing clues and designs.
We Made a Quiz Bowl Packet was marked unnerfable due to requiring clue rewrites.
5D Barred Diagramless was also basically unnerfable.
But, some of the other puzzles I worked on had easier nerfs. Period of Wyrm originally had only the professions of Astrologer, Laborer,
etc. and we added the names to cut an intermediate a-ha.

By now the hint threshold had been lowered to N = 15, and huntcomm was hesitant to lower it any
further. Partly it seemed unfair to bring N closer to teams that could be in contention
to find the coin, and partly it was unclear if we had the manpower to widen the hintstream.

Hints tend to be more fun for solvers, but they don't make
Hunt end as fast as free answers do.
One of the hypotheticals I've thought about after Hunt is what would have happened if we took N all the way
down to 0 to open hints to leading teams, and I believe the consequence would have been an
immediate collapse of the hint system.
Every top team would send in their work-so-far on puzzles that we'd never answered hints for before.
We'd spend 10-20 minutes deriving what they knew and what they didn't, and once our reply went through,
they'd immediately send a hint for a different puzzle.
We needed ways to push Hunt forward that didn't require us to be in-the-loop, and hints were not
one of them.

Saturday evening, Huntcomm announced that we were going to change the Reactivation requirement
to 3/4 input metas instead of 4/4. (The incoming meta requirements were the Office meta, Basement meta,
Factory Floor meta, and Museum metameta.) We would also change the scavenger hunt to reward
free answers that were usable in AI rounds. This would cause a lot of weirdness in story, but
was looking necessary to push teams through.

Changing logic close to Reactivation was especially scary. This
was easily the most complex part of our unlock structure. It was the source of many bugs we fixed
during the year, and now we needed to change it. Once we sat down to actually check
the details though, making it unlock on 3/4 was actually not too bad, since the majority of
the complexity was on what happened after Reactivation, not before, and everything post-solve
was staying the same.

The AI round free answers, on the other hand, were more complicated. Way back when, we'd decided they
wouldn't be supported at all. As the person who set up event rewards, it was on me to figure out
this feature request in the next few hours.

I found the event reward code, and jury-rigged AI round free answers by
doing a ton of copy-pasting of the old logic. I did not want to touch any of the existing
logic if I could. I decided to call them "strong rewards", because they were more powerful,
and the name stuck because making a better one was the least of our worries.
There were no plans to release strong rewards outside of the scavenger
hunt yet, but I decided to implement the functionality for doing so now in case that changed later.

Our goal for all these changes was to get them into the site before any team unlocked them.
The Reactivation and strong rewards changes made it in around 30 minutes before Reactivation was
unlocked.

Speaking of Reactivation, people were rightfully very excited to go do the first Reactivation
interaction. The way Reactivation worked was that when completed, teams would chat with MATE,
teammate would shut down the
Puzzle Factory, and a few minutes later teammate would barge into that team's on-campus room
to tell them Mystery Hunt was cancelled.

Shutting down the Factory was automated, but showing up on-campus was not.
*Logistically*, teams did not know we would be coming to their rooms, so we needed to know when
a team was close to solving Reactivation. The transit time from our HQ to other team's HQ could
be up to 15-20 minutes at the extremes, and although we wanted teams to have nothing but
a shutdown Puzzle Factory until we got there, we didn't want teams to *actually* be stuck
waiting for teammate.

Our solution was to send a Discord alert when 3/4 Reactivation minipuzzles were solved (time
to prepare) and 4/4 Reactivation minipuzzles were solved (time to barge into HQ). Kind of like a
"tornado watch" and "tornado warning" system, except with puzzles.

![Taco warning](/public/mh-2023/tacowarning.jpg)
{: .centered }

At 3/4, teammate members would leave HQ and nonchalantly hangout near the team HQ, trying
to not be too obvious. At 4/4, they would converge. Sometimes the gap between 3/4 and 4/4
would be very short, like Cardinality going from 3/4 to shutdown in 5 minutes.

![5 minutes](/public/mh-2023/5minshutdown.png)
{: .centered }

Sometimes, it was not, like when TTBNL solved 3/4, half of hunt HQ immediately left, and didn't
come back for an hour. Meanwhile, hints were still coming in, with only half of staff available
to reply. This made the dispatcher very upset, and they tried to get people who weren't directly
acting to stay behind in HQ and join a later run. But people were very excited to yell at
teams, and I never expected that cat herding to be successful.

We had now crossed another MIT campuse closure checkpoint. I packed my things and helped bring
physical puzzles to a hotel pickup point, checked they didn't need tech help, and went to my
room to set up alerting on strong reward redemption. I decided to sleep early ("early" = "4 AM")
so that I could stay awak through Sunday. Surely Mystery Hunt will end by then.


# Sunday of Hunt

I only got around two hours of sleep. Waking up at 6 AM, I checked the team
Discord to see if
there were any issues. There were, with strong rewards.
As implemented, they were *only* usable in the AI rounds. In the two hours I'd been sleeping,
huntcomm had decided they did want to send strong rewards to everyone. To make those
rewards useful to everyone, strong rewards needed to be redeemable outside AI rounds too.

"How urgently do you need this, like should I get out of bed right now?"

"Yes please."

So I got up and spent the next hour making that happen.

Internally, event rewards work the same way as hints in gph-site. In gph-site,
instead of directly storing the number of hints a team has left, the database stores the total
number of hints a team has ever received. On every page load, the backend
computes how many hints the team has used so far, then displays the difference to
the frontend.

An advantage of this design is that you never need to worry about database
transactions or keeping counts in sync across models. A hint is used if and only if the team
has gotten back a useful hint response. Event rewards did not originally work this way,
but on Saturday we'd had a bug where a team lost an event reward because of this. An exception triggered
during event answer handling, causing them to solve the event but not get the reward they
should have gotten. So I'd rewritten the logic to match the gph-site design.

A disadvantage of this design is that the more ways you can use hints, the more complicated
it is to compute hints live. For example, gph-site supports intro-round only hints, so a pure
"# hints requested" is no longer good enough - you need to check if a team's last hint is only
usable on intro rounds. I now needed to port similar logic into reward redemption.

The new logic would iterate through any event reward usage the team had, compare it to their
total regular + strong rewards, and greedily use the "weakest" reward required. This did
require describing the logic really awkwardly on the event redemption page, because there were
edge cases where a team could gain a strong reward by spending a regular reward, which
could be pretty confusing.

Once it was tested and reviewed, we shipped it to production and gave out the first set of
strong rewards. I said I'd stay up until I saw the first alert of
a team using a strong reward. That happened about 10 minutes later, and I told people I was
going back to bed.

I woke up a few hours later, decided that was as much as I'd reasonably get, and walked to
hunt HQ. You might think the mood was more somber, now that we knew Hunt was wildly
behind schedule, but it really wasn't. Maybe it was more panicky in Huntcomm, but I thought
the rest of HQ was holding up well and still running smoothly in an emergency.

A group of people were going to [Tosci's](https://www.tosci.com/) to do a double-check for
Eat Desserts on Main before leading teams unlocked Conjuri. They said they'd do a group order for ice cream for HQ, which I immediately
jumped on. I don't remember what flavor I got, but it was good. Also, now that we knew
around how many teams would unlock Fountain, we knew we had way too much soda. A
portion of puzzle soda was declared "no longer a puzzle, please drink", but a teammate
complained that they'd only drink the soda if it were a puzzle.
It was relabeled as "puzzle soda that's okay to drink". Drinking the soda, they described it
as "perplexing".

By now we were no longer getting many site bug reports. There was one issue in Wyrmhole
links caused by colliding [React keys](https://react.dev/learn/rendering-lists#keeping-list-items-in-order-with-key).
One part of our frontend had implicitly assumed that a puzzle title would never repeat in our Hunt.
The fix was easy, the bug was minor, but when I realized that Wyrm was *still* causing
havoc in our codebase in the middle of Hunt, I broke down laughing. It echoed the story so well!

We were also starting to get screenshots of teams first reaction to the complete
nonsense of Bootes round answers, which was pretty great.

![Realization](/public/mh-2023/realization.png)
{: .centered }

The dripfeed of strong answers continued, along with nerfs delivered via errata.
I've seen a bit of speculation about why we used errata to send nerfs. The deployment process
for Hunt was based on Docker and includes a step where Next.js needs to build
the application. Blah blah blah, tech tech tech, the short version is rebuilding and deploying
our site after a code change takes around 15 minutes, whereas errata is viewable immediately.
takes around 15 minutes. It just looks a bit weird.
I remember not liking the nerf given for The Scheme, it felt like it was just removing the
puzzle entirely, but in retrospect it was important to push off the rabbit hole of reusing
the layout from The Legend.

Sunday continued on and on, with no team finishing Hunt. We did see teams spending one free
answer on the scavenger hunt to get two free answers in return, which was simultaneously
disappointing and delightful. It was like we'd given teams a free money hack in the universe.
I mean, it was not actually free money, doing the scavenger hunt was probably easier than
solving any AI round puzzle, but some teams don't want to touch grass. What can you do?

Teams were now doing runs in Conjuri's Quest, and we were getting concerning reports that
it was lagging.
In load testing, we assumed not too many teams would be doing Conjuri at once, not many teams
would even *unlock* Conjuri before the coin was found, and that our sims overestimated server
load. However, we'd made Conjuri unlock much earlier, and our websocket load was *higher* than
our estimated load. Tha latency in Conjuri is bottlenecked on websocket load, since
the whole game runs on websockets rather than HTTP requests.

There was a very brief tech meeting where we tried to figure out if there was anything we
could to relieve load. I aruged that at this point, we probably did not have time to
improve our websocket code. The only reasonable fixes were to either heavily decrease
websocket usage, or spin up another server.

I was not expecting us to do anything about Conjuri, so an hour later I was surprised to
see huntcomm in the middle of migrating hunt progress to a second server. A lot of
tph-site assumes it runs on a single server, so we would not be trying to sync databases
between the two or auto-shard requests. Server load would instead be split manually.
Some teams would be given the URL to the new server, and others would not. Only the
original server would count for Hunt progress. I sent out an FYI that some Discord
alerts would get duplicated, and people should not worry about it.

Around Sunday evening, we got a complete flood of hint requests, most likely from teams
that believed HQ was closing soon.
We immediately drafted an email telling people Hunt was still going to slow down the flood,
while trying to handle the backlog.

Strong rewards continued to be given out every hour, until multiple teams were at a point
where they had every feeder answer in the AI rounds. Once that happened, the feed of
strong rewards was stopped, and it was now a waiting game to see which teams would
crack the metas first.

No team had solved all the metas by Monday 1 AM, the next MIT campus closure window,
but Huntcomm was now quite confident that they could push a team past the finish line.
If necessary, they would give increasingly strong meta hints until someone finished.
The logistics lead decided to close hunt HQ for part of the Monday overnight shift,
since we'd noticed very few pickups during the Sunday overnight shift, and we really
needed Huntcomm to catch up on sleep (many had stayed up later than they planned while
debating what to do about Hunt).

After I got back to my hotel, I saw that some teams had started the clickaround (MATE's
Team), and decided to stay up until they finished. Then I decided that no I don't need
to this, they won't finish until we're allowed back on campus anyways, I'm going to get sleep.
With a team on the clickaround, team leadership starts preparing a slide deck for wrap-up.
They ask if I can get them a GIF of the full loop of the Wyrmhole round art. I tell them
to give me a bit, and try messing around with generating it programmatically, but none
of these options were as good as a screen recording of me clicking the zoom button a few times.
This is the final GIF that made it to wrap-up.


# Monday of Hunt

Multiple teams have finished the clickaround. We've sent an email saying "the coin will have been
found", and schedule them for Monday morning which is...only a few hours from now.

I once again do not get much sleep and end up in HQ early to answer more hint requests. Perhaps
in a more normal Hunt, this would be when we'd open the floodgates and time unlock everything and
make hints available for all puzzles. But this year, it is 6 AM on Monday, and everyone is busy setting up final
runarounds, which we plan to run right up until a 10 AM cutoff. With wrap-up two hours later,
there is no energy to create more work or handle that work.

In [betaveros's post Hunt post](https://blog.vero.site/post/2023-hunt), he mentions that if you have to choose between a Hunt that goes short and a Hunt that goes long, it's clear people prefer a Hunt that runs
short. Let me just say, MY GOD is this true on the organizing side too. One of the more silent
consequences of a long Hunt is that you have less time to make your wrap-up, check your solution
release looks good, etc. We are now very certain that we cannot get solution release done by wrap-up,
because there are too many things to check in the time we have left between end of Hunt and wrap-up.
This is quite sad.

There are 4 timeslots for runarounds and more than 4 teams that finished. We end up grouping some of the
later teams together as we try to make sure everyone who finished early enough gets to see everything.
Then it's time to close up for real. The Puzzle Factory is disassembled - I take one of the gears
as a hard-to-explain keepsake for later. With Hunt finally over, I start reading some of the discussion
in puzzle Discords, but there's really still so much cleanup to do.

Wrap-up happens, and if teammate sounded a bit out of it, well, that is not unique to long Mystery Hunts.
I think the default state of Mystery Hunt wrap-up is that the presenters have stayed up too late and
are trying to compress a very long story into a very short presentation. This is not something I'm
that good at, if this post is any indication.

I will say that wrap-up does not go as badly as I thought it would. Some people are mad online but civil, friendly,
or reassuring in real life. After wrap-up, I catch up with puzzlehunting friends that I haven't gotten to see all
week because I've been busy running Mystery Hunt. I'm happy to hear puzzles I worked on listed in people's
favorites. People slowly file out of the room and soon it is just teammate. We do one more team photo
for the memories, then disperse until dinner.

For lunch, I end up at Roxy's (my 2nd time in 2 days). Someone from Galactic tells me they decided to 100%
Collage for fun, and they want me to guess what their last word was. I guess right on my first try.

There is a team dinner at Spring Shabu-Shabu. This is mostly uneventful. The hunt retrospective is officially
postponed to a later date so people can catch up on sleep, but I head back to the AirBnB instead of my
hotel because I am pretty awake after my nap, and don't really have anything else to do.

![Passing out](/public/mh-2023/passingout.png)
{: .centered }

When I arrive, a few other people from tech are working on getting solutions and basic puzzle stats out.
Most of it is done by now, the main TODOs are on updating our stats pages to account for free answers
and fixing access issues for accessing Museum puzzle stats if a team has shutdown Mystery Hunt.
We successfully get solutions out that night. Figuring out static conversion of the site, or public
access teams, is set for another day.

The official postmortem is not happening until later, but the part of team leadership staying at the
AirBnB is doing an unofficial one anyways.
They are going through various puzzles with the air of "why did we write
so many teammate-friendly puzzles and metas when not all teams are teammate, why was this puzzle made so long,
how could we miss this and this and this f\*\*\* f\*\*\* FFFF\*\*\*". Some of this self-criticism is
fair and some is not.

(The next day, we'd get some advice from a previous Mystery Hunt
organizer, where they 100% called that we had already done a panicked reflection on every mistake we'd made,
then went through the reasons it's bad to do that so close to Hunt.)

The world turns, and life moves on.

I do a [Boxaroo!](https://boxaroo.me/) escape room on Tuesday, play some board games, then head
to the airport to fly back home.


# February 2023

## Did You Think It Was Over?

The model you might have is that after MLK weekend, you can stop thinking about Mystery Hunt. This is sort of true?
It depends. In general, if you work on Mystery Hunt tech, Hunt really does not stop after
MLK weekend.

In our case, we have a big problem of figuring out how to make a public team on a site that makes
assumptions about team state on, like, literally everything. There is then more work on getting canned
hints onto our site, figuring out how and where to link the solution to a loading puzzle that doesn't
exist for the public team, and generating the customary Hunt stats page.

I start looking at feedback from the Mystery Hunt survey, but seeing feedback like
"you did not mean for anyone to have fun with these puzzles" when you have just worked a 100 hour week trying to make sure people had fun with the puzzles is pretty disheartening. I force myself to stop, telling myself that
if I read any more feedback, I'm just going to get depressed, and the stats page will get delayed until March,
and nobody wants that. I'll look later.

First things first: figuring out what the *hell* we can measure in our Hunt. In a typical Hunt, solve times
and solve counts are pretty free, but in our case we have nonsense like Hall of Innovation. I don't find
a clear place where this information is saved in the database, it looks like our stats page only reports
the last time a team has solved the puzzle
due to us literally deleting old puzzle submissions whenever the gizmos change. I eventually figure out that I can get the information if I write a Discord script to crawl our entire #guess-alerts channel, and parse out
the team and timestamp via some regular expressions on top.

![Regular expression xkcd comic](/public/mh-2023/regularexpressions.png)
{: .centered }

I also need to figure out how to merge solve data from the server spun up for Conjuri back into the main
server. Brian sends me some pointers to the database dump, and I figure out that all our existing documentation
only applies to setting up a Postgres database from scratch. We don't have any docs on merging databases together,
since this just isn't something we've done before. I end up loading both dumps locally and set up some hacky
scripts to merge data appropriately.

That just leaves everything else. Most of the novel stats like number of chats are not too bad to derive in Django shell after I
understand the data format. Data on when teams solved the collaborative jigsaw is partly incomplete for teams that
had to get fastforwarded past the puzzle, since they never got credited with an official solve. I spend too
long trying to reconstruct the full data from email logs before deciding it's not worth it. Would anyone
even notice the inconsistencies? Probably not. (They're still there, if you look closely enough.)

The spoilr HQ page has some built-in solve charts based on chart.js, which solves many problems while
introducing other ones. I get confused that the given chart.js code doesn't match the chart.js
documentation. Then, I figure out the chart.js version is out od date. There's a feature I want that's
only in the most recent chart.js release, so I migrate past a breaking API change to get access to it.
*Then* I learn that actually [there is an undiagnosed performance bug](https://github.com/chartjs/Chart.js/issues/10073)
in the newest version of chart.js that is causing the charts to render 2x slower, and I sigh as I undo all my migration work.
I care about speed more than making the charts display nicely on mobile.

As I generate the team size graph, I find it very cute that you can almost see the vertical lines where
teams round their team size to the closest 5 or 10. It reminds me of an old Jon Bois video showing that
football plays are biased towards starting from multiples of 10, due to refs having some leeway on certain
penalties for where to place the ball. You can see the lines of scrimmage, until Death & Mayhem breaks the
illusion with their reported team size of 133.

![Solves by team size](/public/mh-2023/solvechart.png)
{: .centered }

Over the rest of February, we figure out what we want the hunt tech to look like, whether we want to clean
up our codebase or release it as-is, and the list of tech TODOs to make all the interactive puzzles continue
working once the site is archived. Then I stop paying as much attention.


# March 2023

The site is not converted yet. It turns out it is a lot harder to work on Mystery Hunt 2023 when the world
has moved on from talking about Mystery Hunt 2023.

I suppose that's the deal. Write a tech-heavy hunt that strongly assumes a server exists, and it looks cool
live, but it's annoying to wrap up later. Given how long it took to package up Teammate Hunt 2020 and
Teammate Hunt 2021, I am entirely unsurprised Mystery Hunt is taking a while to wrap up. But we do want it
to wrap up.


Thoughts on Hunt
-----------------------------------------------------------------------------------------------

Obviously, some things went wrong.

I think it'd be a shame if the only story of this Hunt in 10 years is "it was really hard". As someone who
spent a lot of time on a bunch of non-puzzle aspects of Hunt, a lot of the stuff I'm most proud of is
in the overarching structure of Hunt, and supporting its visual presentation. I never want to deal with
responsive SVGs ever again, but my God, we figured out how to make the Wyrm round concept work.

IMAGE

That being said, I did write a lot of puzzles too. I wrote more puzzles this year than I have in
the past 10. (That isn't saying much when there was a 7 year hiatus in between, but still.) If anything,
it makes the criticism easier to go through. I've seen people simultanously put my puzzles as their
favorite and least favorite puzzles of Hunt, and that makes it easy to tell it's not an attack on, like,
my character or self-worth as a person.

I am about to go into a big list of interventions that I think could have addressed the Hunt difficulty,
but before doing so, I want it clear that I'm still proud of 80%-90% of the work I did for Hunt. It was a lot,
and took over much of my life towards the end of the year, and I'm not sure I ever need or want to go
through the experience again, but I did it and a lot of it was cool.

At the same time, I think it is important and interesting to consider where things started going wrong,
because I think some of the errors were pretty subtle. It is not an easy explanation, like "teammate
was inexperienced". teammate is a younger team, but collectively I believe members of teammate have run
around 5-7 different puzzlehunts before this one. It's not quite "teammate did not account for being biased
towards their own puzzles". This bias was a big topic in puzzle variety discussions, and is the reason
the hunt is only half-littered with math and video game puzzles instead of completely-littered. There
were other things going on.


Calibration
---------------------------------------------------------------------------------------------------

If you forced me to pick one thing that went wrong, it would easily be calibration. Puzzles were harder
than their ratings across the board.

But, it's interesting that they were *consistently* harder. The usual symptom of a Hunt that is undertestsolved
is that some puzzles are much easier than expected and some are much harder than expected. When you don't
have enough testsolve data, errors tend to go both ways. Here the errors went one way.

In Puzzup, puzzles are rated 1-6 in difficulty, and those ratings were actually pretty consistent with
each other, they just weren't consistent with the stated rubric. A 3 on the scale is marked as "comparable
to a puzzle in The Ministry". Looking at some puzzles that averaged a 3.0, they were certainly easier than
a 4.0 puzzle, but they were certainly *not* Ministry difficulty. Like, even the inside baseball
solve times of teammate testsolving teammate's puzzles seems too long.

My sense is that when puzzle guidelines first came out, there was overextrapolation on what a full-size
Mystery Hunt team could do, relative to the 3-4 person testsolve groups that were assembled per puzzle.
So the recommended solve times for puzzles were started too high, puzzles got written to that bar, and then
the bar stuck.
I agree with [Eric Berlin's](https://www.ericberlin.com/2023/01/20/how-hard-is-your-hunt/) comments
on Hunt difficulty, but I thought it was interesting that he casually mentioned testsolve teams of
4-8 people. We would have killed to have a lower bound that started at four! Our lower bound was often
2-3 people, and getting 5-6 people was logistically challenging enough that it only happened for puzzles
that really needed it.

If we had ever revisited a Ministry puzzle around the middle of the year, when feeders were getting
written in earnest, it would have been very obvious. But why would you solve a puzzle from an old Mystery Hunt
when there are a bunch of puzzles to testsolve for the upcoming Mystery Hunt? The clock is ticking,
after all.

Maybe that is the answer - go back to old Hunts and *actually* solve 1-2 of those puzzles in the same
conditions as your other testsolves, and assume this will be worth your time? If you ever actually do this,
let me know, because I have never seen it done or heard of it being done. Usually you assume your
calibration is correct.


Calibration Part 2
----------------------------------------------------------------------------------------------------

One of the consequences of starting with a higher difficulty bar is that it's harder to design and
testsolve puzzles.

I think some people might have a mental model where making a puzzle is like placing one brick at a time,
and you grow from a good 5 minute puzzle to a good 10 minute puzzle and so on, until you hit your time target.
This is really not how puzzle writing works.
As an art form (yes I am saying puzzles are art, no I
am not taking questions), puzzles are partly defined by how well they fit together as a cohesive whole.
You can have a multi-step puzzle with very disconnected steps, and it can even be fun, but it's
not what people imagine when they think of a good puzzle.

Usually what happens is that you start with some idea, and you try to expand it, as if you're inflating
a balloon. Holes will appear and you'll tape over them, testsolvers will say it isn't fun and you switch
out the material, but eventually it comes together. And at the end, you've got a puzzle.

The challenge with writing longer, more difficult puzzles is that you need to start with a bigger idea
at the beginning. It has to be big enough that once it's filled to a complete whole, the completion will
be a long enough puzzle. Then each round of iteration takes longer, since it takes longer to do the grunt
work and testsolve. If you assume a constant number of puzzlemaking hours per Mystery Hunt year, and
every puzzle starts harder than it should, the self-consistent conclusion is that every puzzle also
gets fewer iterations of refinement than it should. You can't push one up without pulling the other
down.

A lot of the puzzles could have done with more editing, and many of them *were*, but the later in the
year it got, the less prominent editing became. Much of the drumbeat during the year was "we are
behind our puzzle projections", so editing became more perfunctory. If a puzzle got rated as fun,
it went in, even if it may have been too long. This is 100% the correct decision, every time. Part of
[following the fun](https://www.reddit.com/r/gamedesign/comments/hfqh8r/what_does_follow_the_fun_mean_how_do_you_follow/)
is to not throw out things that testsolvers have told you are fun. The problems arose when
a puzzle was rated as sort-of fun, where editing it would take a lot of time and require another
testsolve when testsolve capacity was low.
Puzzles that should have been edited got rounded to "let's
ship it and move on" due to time pressure. The provocative way to put it is that we knew we had some clunkers,
but we understood exactly how they were clunkers and needed warm bodies (puzzles).

(And then feedback comes in pointing out the same flaws that came up in testsolving, and you feel bad
because you knew it existed, even if it made sense to ignore it at the time.
This seems to be a common theme in not just puzzlehunts, but all creative endeavors:
authors of a work know most of the problems in their work. I remember Palindrome saying that a lot of their posthunt
feedback was framed as "I think you overlooked this problem" when they usually had not overlooked the problem.
Please don't let this stop you from pointing out flaws though. It's useful to see what
problems people consider important, even if they're already known.)

The Conjuri feeders were the last ones written in Hunt, and when the feeders were released, they
were specifically asked to be puzzles that would be easy to edit and iterate (read: closer to
well-known puzzle archetypes). The shift to encouraging such puzzles should probably have happened much
earlier.



Testsolve Affinity
--------------------------------------------------------------------------------------------------------


For a lot of the year, authors were responsible for soliticiting testsolvers for their puzzles, usually at
general meetings. This started taking up a lot of people's time, so we later had a "testsolving lead". Everyone
filled out a survey with their puzzle preferences and availability. Authors would describe their hour estimate,
what kind of puzzle it was, and how many testsolvers they wanted, and the testsolving lead would handle scheduling
testsolves and wrangling testsolvers.

This was pretty helpful, but it may have done too good a job of pairing testsolvers to puzzles. Math puzzles
were routed to math people, geography puzzles were routed to geography fans, and so on. I'm not saying every
puzzles was directly routed to people who'd have an easy time with the puzzle, but it was certainly biased that way.

There are other biases of the testsolving process. You are encouraged to stick with a puzzle, so that other people
don't need to be spoiled. You usually are not solving with as much urgency as a live hunt, but you are also not
as tired as a live hunt. You start with a group of people, rather than having one person start a puzzle and slowly pull
in people over time. When testsolving Moral of the Story, I thought it seemed okay, but we had four people doing
the first step and were able to speed through it pretty quickly. If you start the puzzle solo, you're going to have
a bad time grinding through the start.

Still, I think the main issue was large puzzles that reduced the number of testsolves doable to make
every puzzle shipped in time.
Every puzzle has inherent uncertainty to it, and you can think of a testsolve as reducting th
- testsolves never
exactly match real solvers, and this is especially true in Mystery Hunt where team sizes and makeups have
such a wide range of variance. A team has to choose when it's worth investigating an uncertainty, and when
you should leave it alone and assume it's not worth investing the time to make it more certain.


Complexity and Scope Creep
------------------------------------------------------------------------------------------------

teammate wrote some **wild** puzzles this year. Some wack metas, some crazy feeders, all sorts of stuff.
Let me go requote the team goals from the start of the year.

> 1. Unique and memorable puzzles
> 2. Innovation in hunt structure
> 3. High production value
> 4. Build a great experience for small / less intense teams

It's possible we overreached here. There's a certain amount of complexity budget you get, defined by your
available manhours, and we spent a lot of it early when deciding to create the AI rounds. Then we spent more
of it creating some more complicated metameta structures, and spent more of it connecting rounds
together between the Museum and AI rounds. I have already mentioned how in retrospect, I think adding a dependency
chain between Museum and the Wyrmhole was a bad idea, but I think there were some second order effects
on late feeder release and relative lack of "normal" puzzle answers at the start of puzzle writing.
The gimmicks also made it hard to shuffle puzzles between acts if they tested harder than expected.
In previous Teammate Hunts, I've been in testsolves for puzzles that got moved from intro round to main
round and vice versa. I'm not sure if that happened this year. You can't easily swap a Museum puzzle with
a Bootes or Eye puzzle, their structural requirements are different.

I'd say that in general, teammate does rise to the occasion, but not without a lot of effort and struggle,
and usually with some scope creep along the way. Teammate Hunt 2020 started design with one Playmate game
as the runaround, and ended with eight. Teammate Hunt 2021's design proposal for the Carnival Conundrum
said "7-9 intro puzzles, 7-9 main round puzzles per half, 6-8 pairs of puzzles", and the final hunt picked
9, 9, and 8 respectively, the top of each band. I was not in the design process for Hall of Innovation, but
I remember someone telling me that when that was first designed, it was certainly not an entire round.

My guess is that you can make a hunt with complex hunt structure, and you can make a hunt with complex
puzzles, but you should give some pause before trying to do both. It's a bit tricky, because one of the
common themes in survey feedback on what makes Mystery Hunt special is its grandiosity and scale. This is a
very, very consistent sentiment. I would rather people try to push Mystery Hunt than feel required to play it
safe.


Time Unlocks
-----------------------------------------------------------------------------------------------------

This year, rounds were time unlocked as a whole, up until Reactivation, and no time unlocks were done past that
point.
I've seen some speculation that we did not time unlock past Reactivation because doing so would have spoiled
the answers to Reactivation. Personally, I don't think that mattered. If a team wants to see more rounds
they can probably live with skipping one puzzle of Hunt.

Requoting myself, the way I view teammate's approach to story is:

> * Decide on the story you want to tell, then make sure as much of the Hunt as possible acts
> consistently with that story. This will take a long time to polish - do so anyways.
> * Tie changes in narrative or story state to actions the solvers take. These actions are usually
> solving puzzles, but they don't have to be.
> * Use bottlenecks to direct team attention towards the same point, then put the most
> important story revelations at those bottlenecks.

I'd say the main
reason we did not time unlock past Reactivation was because the story was integrated so strongly into the
Hunt that we didn't have a good answer for how to time unlock past it. The entire conception
of the story beat is that is is a breaking, irrevocable change in how solvers use the site, and the solvers'
story objectives. You can't really have a team suddenly cause Mystery Hunt to be cancelled because it
happens to be Sunday. This was similar to why we decided not to time unlock teams past the intro round
in Teammate Hunt 2021.

To a smaller degree, we also had zero idea how the site would function if time unlocked past Reactivation.
All across the codebase, there are differnt checks for solve progress and story progress. The two tracks
are separate yet interconnected (think of them as "entangled" if you'd like). We had pulled a literal
all-nighter making sure it worked properly for a team with no time unlocks and did not have time to
verify any behavior of what happens if a team has a post-Reactivation solve state without the corresponding
puzzle solves beforehand. Even the relaxation of 4/4 metas to 3/4 metas to unlock Reactivation caused some
problems around Hall of Innovation that we had to fix live.

I think the devil's advocate argument is that not all teams care about story, but teammate is a team that
*does*. We care about story enough to allow it to partially intrude on UX of the site. This isn't just about
the shutdown of the website, but also the Museum puzzle list not listing Puzzle Factory puzzles, because
from the Museum's perspective the Factory does not exist.

There was probably a way to make time unlocks work, if we made them self-service, gated them on
scheduling a team interaction with teammate where we could catch up on story beats teams were skipping, and did the tech
work required to verify site behavior and allow for skipping certain cutscenes / bottlenecks.
These are all solvable problems, that are not solvable in the middle of a Hunt that is going too long. I'm sorry
that wasn't set up beforehand, there were just too many other things to work on.


Difficulty Buckets
-------------------------------------------------------------------------------------------------------

One of the common points of criticism is that the intro round was not really an intro round. Some of the puzzles
were quite involved and difficult.

I think it is more accurate to say that the Atrium was not designed as an intro round, but it was perceived
that way. During writing, there were guidance for three difficulty bands: Museum puzzles, Factory puzzles,
and AI round puzzles. There was then no further guidance within that, and puzzles were to be ordered afterwards
by hunt exec. This has never been a problem in past Teammate Hunts, where the intro round was 8-9 puzzles.

There were 40 puzzles among the Museum rounds. If you don't try to funnel easy puzzles to one round, they'll
get distributed roughly evenly. This left a trilemma where not all three could be true at once:

* Puzzles are mostly ordered by difficulty.
* A meta for a round is unlocked only after a solid fraction of its feeders are solved.
* A meta is unlocked early in Hunt.

The puzzle order picked for Hunt chose the last two. If teammate had known how hard Hunt was going to be,
I think it's likely we would have picked the first and last instead. A number of our metas were not very
backsolvable or doable from partial information, and I think it would have been safe to unlock those earlier.
It's still not an ideal solution if a meta unlocks way before you can solve it but it's still psychologically
more exciting.

Now, the real fix is to make sure you are not in this trilemma to begin with.
One of the big conclusions of the internal retrospective was that
round ordering should have been locked in much earlier so that people could know if their feeder was towards the start or
end of Hunt. The curve would have been smoother if Atrium was easier for Museum puzzles and Wyrmhole
was easier for AI round puzzles.

(Just as one final point: although I am framing puzzle ordering as if you should order puzzles from easiest to
hardest, this is just one of many criteria people can use for puzzle ordering. Variety of currently unlocked puzzles,
avoiding too many grindy puzzles at once, avoiding too few grindy puzzles in case solvers want a menial
straightforward task, difficulty walls, and so on are also all important points. These criteria often compete
with each other, making optimizing all of them hard. I have literally been in 8 hour long meetings for puzzle ordering
and unlock tuning, and that was for Teammate Hunt 2021, which was 1/4th the size.)


Small Team Objectives
------------------------------------------------------------------------------------------------------

One of the experimental decisions we made all way back at theme proposal time was to have the discovery of the Puzzle
Factory be the equivalent of solving the 1st meta as a small team objective.

How well this worked is probably directly related to how cool you found the Puzzle Factory and what expectations
you had around Hunt. If you went into Hunt with the goal of solving 1 meta, you may not have been hyped to
unlock a new round with new puzzles instead of more Atrium puzzles.

We did a lot to funnel teams towards unlocking the Puzzle Factory, including gating the release of the Atrium
meta behind finding the Puzzle Factory. I believe it is literally impossible to get enough Atrium solves to
get the meta without solving the loading puzzle, since we wanted teams to find the Puzzle Factory before focusing
on a meta. The side effect was that the Atrium meta unlocked quite late.

Story-wise, teammate is trying not to draw attention to the Puzzle Factory, and teams were supposed to feel like
they were breaking the rules by exploring the Puzzle Factory. I think that created tension with how to make teams
feel like they had done something important.
There was a planned story interaction with MATE,
where MATE would tell off solvers for entering the Factory, but let them explore anyways. I believe it got
deprioritized, but it probably would have helped on this front.

I suspect future teams will not have this problem, because they will just make solving the 1st meta the objective
for new teams.



Advice for Future Writers
-------------------------------------------------------------------------------------------------

I mean...

In some sense, this entire post is advice for future writers. In some sense, I got too lazy to distill it down
into actionable advice, and decided to just put my thoughts on everything and let people draw their own conclusions.
I'll add some more things that I don't think were covered earlier though.

* Pay attention to the processes you're creating. Hunt is big enough that there are pretty significant benefits
to improving them early.
* Hunt writing is very much a marathon. Make sure you are not burning out too early.
* Disasters will happen, this is just what it is like to work on a big project. Usually they are fixable.
* You will not always get your way, this is also just what happens in big projects. Try to deal with it.
* Absolutely do not feel obligated to do as much tech and art as teammate did in this Hunt. We went all-out
on both because we have a lot of artists, a lot of software developers, and pretty recent experience for both.
Out of Mystery Hunts in the past 5 years, Galactic's and teammate's are easily the most tech-heavy. It is not a coincidence
that both Hunts were written by teams that had recently ran puzzlehunts with many interactive puzzles. I do not
think our Hunt site would have been possible if the tech team had not written puzzlehunt code in the same
codebase for two years prior.
* Make sure you have at least one round of easy puzzles!
* Mystery Hunt got a *lot* more popular during the pandemic. It's shrunk a bit, but not a lot. For reference:
the 2020 Hunt had 150 teams and the 2023 Hunt had 300 teams. The load for answering hints, emails, etc.
scales with the number of teams. Make extra sure your flows there are seamless.
* Mystery Hunt is also becoming more multicultural. There are a larger number of foreign teams where English
is their second language. Maybe this is PTSD from hinting non-native speakers through Inscription, but
consider having your intro puzzles not depend on fluency with English language or US culture. Otherwise something
like this might happen.

![Hints](/public/mh-2023/hints.png)
{: .centered }
* Although we had accessibility checks throughout our Hunt writing process, we didn't have an accessibility lead
and we think this was a mistake in retrospect. The benefit of having an accessibility lead is that it empowers
someone to make a fuss about accessibility issues. Consider if you want to have advocates for other groups as well.
During Hunt we had someone unofficially start advocating for small teams while huntcomm was making
decisions that would only affect top teams, and I think it was good that happened.
* Read all the post-Hunt posts you can to see how people react to Hunt, then don't take them too seriously.
The class of Mystery Hunt writers is usually biased towards top teams and doesn't reflect everyone.


My Future of Puzzle Writing
------------------------------------------------------------------------------

I do not have any burning puzzle ideas left to write, but I told myself this at
the start of Mystery Hunt. I said I did not expect Mystery Hunt to encourage me to work harder on puzzles, and
then it did. I have, very continuously throughout the year, asked myself if
this was all worth it, and the answer was not clearly yes.
Still, if you offered me $1000 to never write another puzzle, I wouldn't take the money.

So, uh, ask me later? I care about puzzles
a lot. I wouldn't have written for Hunt if I did not care. That continues to be true. Puzzles are cool.
I'm just tired and want to do other things in my free time. Maybe I will write again, but right now
I want a hobby, not a job. We climbed the mountain, and came back down.


Cut Paragraphs
----------------------------------------------------------------------

If that's why people solve puzzles, why do people write puzzles? Well, some
people do so because they've had fun solving puzzles, and want to try something new.
I'd say that's why I first started.
solving puzzles, and want to try doing so. I'd say this is how I first got started.
Another reason is that you want to make a puzzle about one of your other niche
interests or hobbies, since puzzle solving lends itself well to pushing people to
research a topic in lots of detail. That's how I restarted writing puzzles.

In the long run though, I find I get the most motivation from figuring out how to
create an interesing solve path, and making the puzzle rhyme with itself as much as
possible. Designing a puzzle is itself a puzzle, where
you're trying to figure out what steps are interesting and guess what wrong turns
a solver could make. In doing so, you're naturally constrained in what information
you can give, because every bit of extra information is something that might point
in a *different* direction than the idea you want the puzzle to express. It's
challenging, but it can be very rewarding to find a way to solve design problems
within the constraints you've created for yourself.

On some puzzles I've made, I'd be satisfied
if just 1 person had fun solving it end-to-end, understanding its design to the same
level of depth that I did at construction time. I'd say this is my answer to
[why people spend so much time creating such ephemeral experiences](https://mitadmissions.org/blogs/entry/two-hundred-puzzles-4/#fading-together).
I get a lot of self-satisfication even before the puzzle goes live. Blogging is
similar for me. There are things I get out of the writing experience that I'd
still get if no one read my blog.

And, in the same way as blogging, I still want to share my puzzles, because as much
as I get from doing it for myself, there are things I get from broadcasting that
I don't get anywhere else.

Solver Constructor Tension
-------------------------------------------------------------------------------------

There is a core tension between puzzle solver and puzzle constructor. Puzzlehunts
are at their best when their solutions are not obvious.
The solutions *have* to be indirect. You want to leave enough space for the solvers
to work with the puzzle and discover the answer for themselves. If you don't give
that mental room, and railroad solvers from start to finish, then it's still an
object with an answer, but it's not a puzzle anymore. It's more following directions.

Or, put another way: if it's impossible to get completely lost, then it's not a puzzle.

> The key to making a detective game fun to puzzle out is that you have to give the player as many opportunities as possible to be wrong. If you steer them through finding the clues and give away the answer anyway then they can never be wrong. If you give them three dialog options to pick from then it’s pretty easily brute forced. Meanwhile, [Return of the Obra Dinn] has you fill in multiple blanks that all have multiple possible entries, and there could be hundreds if not thousands of wrong combinations. And you can’t brute force that, your only recourse is to actually be smart enough to figure it out.

([Yahtzee Croshaw, "Great Detective Games Let You Fail Miserable"](https://www.escapistmagazine.com/great-detective-games-let-you-fail-miserably-extra-punctuation/))
(: .centered }

(Side note: Return of the Obra Dinn is excellent, even if you aren't "a video game person",
definitely recommend.)

This, though, is where you have the tension. If it's possible to get completely lost in
a puzzle, not everyone is going to get the full experience.

but if it's possible to get completely lost, not everyone is going to get the full
experience.

Take the duck konundrum. It's literally a series of directions. They're hit-or-miss
as a puzzle type, but when they hit, it is usually because following the directions is
hard for some reason. What if we misread a line?
What if we have to start over? That tension is the chance for failure that makes a
puzzle a puzzle.

(does the section above even fit here)

Aphorisms About Writing MIT Mystery Hunt
---------------------------------------------------------------------

Although there are a lot of stories about Hunt, the main one is about its difficulty.

I don't want the difficulty of Hunt to overshadow everything else, but I'd be lying if I tried
to downplay the way it affected the solve experience. There is no one reason that Hunt went long. Like
most engineering postmortems, it's more like there were many small things that accumulated, which either
went unnoticed or were noticed at a point where it was too late to change.

Perhaps the easiest way to tell that story, is to present my theory on what it's like to write Mystery Hunt,
and we'll see how it played out.

**Writing Mystery Hunt is fundamentally a game where you run out of time.** It seems like every Mystery Hunt,
without fail, has lots of work happen in the week leading up to Hunt. That's not because every construction
team sucks at time management. It's because the amount of work that *could* be put into a Mystery Hunt is
essentially unbounded. You could always try to get more testsolve of a puzzle to get more data about
sticking points, or replace a puzzle that has low fun ratings, or address a website bug, clean up an art
asset. But you get one year to write Hunt. You don't have time to make things perfect, you have to ship.
Move fast, and try not to break things. A team has to choose where it will not fully resolve things for
the sake of getting other things done.

**Decisions you make early can have big repurcussions later.** One example is theme selection, which is
always done first but influences basically the entire year afterwards. The more notable example is the
puzzles themselves. The writing order is metametas -> metas -> feeders, and
they have to be done serially. Each metameta or set of metas is written simultaneously, so you're
picking how many puzzles the Hunt will have long before they exist. It's not impossible to adjust puzzle counts
later, but it's usually pretty hard to, especially if you're considering cutting a round with a meta
that someone worked hard on.

**Team member availability will always shift.** Some people will spend less time than they expected, some
will spend more, and in general, there are always people who will have to drop out for good reasons.
Everyone starts excited and motivated at the start of Hunt, when you are scoping things, but by the middle
some people will have checked out.

Hunt writing tends to follow a long tail distribution, where a few people will do a lot of the work
and a lot of people will do a bit of work. This is very consistent every year. Below are some plots of
"hunt contribution", as measured by puzzles written. I know that there are many ways people
work on Mystery Hunt besides writing puzzles, but puzzles are the easiest things to track.

<table>
    <tr>
        <td style="width:50%">
            <img src="/public/mh-2023/2018.png" alt="2018 contribution graph">
        </td>
        <td style="width:50%">
            <img src="/public/mh-2023/2019.png" alt="2019 contribution graph">
        </td>
    </tr>
    <tr>
        <td style="width:50%">
            <img src="/public/mh-2023/2020.png" alt="2020 contribution graph">
        </td>
        <td style="width:50%">
            <img src="/public/mh-2023/2021.png" alt="2021 contribution graph">
        </td>
    </tr>
    <tr>
        <td style="width:50%">
            <img src="/public/mh-2023/2022.png" alt="2022 contribution graph">
        </td>
        <td style="width:50%">
            <img src="/public/mh-2023/2023.png" alt="2023 contribution graph">
        </td>
    </tr>
</table>

Hunt writing tends to follow a [Zipfian distribution](https://en.wikipedia.org/wiki/Zipf%27s_law).
A small number of people will write many puzzles (or do a lot of leadership work), and a long tail
of people will write a small number of puzzles each. Usually, that long tail is where all the
new puzzle writers are. Put yourself in novice puzzle writer shoes. Do you want to write a puzzle
where whitespace and capitalization matters, a puzzle that must be closely tied to a language, or a normal
puzzle? Unless you're really excited about an idea for the gimmick, you'd probably prefer a normal
puzzle answer. If you didn't get in on the two rounds that had feeders released, then you had to
wait.

This isn't quite as bad as I'm making it out to be.
There's useful work that can be done without knowing the puzzle feeder. In general though, if you
have a feeder from the start, you'll do less redundant work, it's easier to stay motivated if you're
expected to pull through on a puzzle, and you can brainstorm ideas from the puzzle answer rather
than trying to generate one from the ether.

and we also should have adjusted
the difficulty estimates of future puzzles. We did not have the time to rewrite existing puzzles
to be easier, because the end goal is creating 150 units of "fun". In general, creating
fun is harder than creating difficulty. So the more correct decision would have been to bias
new puzzles (units of fun) to be easier, to make the average go down over time.



DALL-E and other image generation models were quite hot at the time and we used them accordingly. Although
a surprisingly large fraction of teammate works in machine learning or related fields, the choice to
have an AI-themed puzzlehunt in the year generative models became commercially viable was entirely
coincidental.

(If every team is limited to one open hint at a time, shouldn't the number of total open
hints not increase by much? Well, yes, that's true. But it will increase by a bit, since not
all teams continually keep one hint open. More importantly, new puzzles take longer to hint
than old ones, since you need to relearn how to hint those puzzles. Every time N was shifted,
there was sudden shock in the hint system, until the puzzle had been hinted a few times.)
