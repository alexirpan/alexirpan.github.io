---
layout: post
title:  "My AI Timelines Have Sped Up (Again)"
date:   2023-12-29 15:08:00 -0700
---

In August 2020, I wrote a post about [my AI timelines]({% post_url 2020-08-18-ai-timelines %}).
Using the following definition of AGI:

> An AI system that matches or exceeds humans at almost all (95%+)
> economically valuable work.

My forecast at the time was:

* 10% chance by 2035
* 50% chance by 2045
* 90% chance by 2070

Now I would say it's more like:

* 10% chance by 2028 (5ish years)
* 25% chance by 2035 (10ish years)
* 50% chance by 2045
* 90% chance by 2070

To explain why, I think it would be most instructive to directly compare my 2020 post to my current
one.


Why?
----------------------------------------------------------------------------

When I thought about AGI before, I saw two bimodal hypotheses.

**Hypothesis 1:** Scaling is enough for AGI. Many problems we consider challenging will disappear
at scale. Making models bigger won't be easy, but the challenges behind scaling up models will be tackled and solved sooner rather than later, and the rest will follow.

**Hypothesis 2:** Scaling current methods is not the right paradigm. We will reach the limits
of what scale can do, find we are not at AGI, and need new clever ideas to even find the correct next hill to climb. Doing so will take a while.

To be precise, let me quote myself from 2020.

> How much
> are AI capabilities driven by better hardware letting us scale existing models, and how much is driven
> by new ML ideas? This is a complicated question, especially because the two
> are not independent. New ideas enable better usage of hardware, and more hardware
> lets you try more ideas. My 2015 guess to the horrid simplification was that
> 50% of AGI progress would come from
> compute, and 50% would come from better algorithms. There were several things
> missing between 2015 models, and something that put the "general" in
> artificial general intelligence. I was not convinced more compute would fix that.
>
> Since then, there have been many successes powered by scaling up models, and [in 2020] I
> now think the balance is more like 65% compute, 35% algorithms. I suspect that
> many human-like learning behaviors could just be emergent properties of larger
> models. I also suspect that many things humans view as
> "intelligent" or "intentional" are neither. We just want to think we're
> intelligent and intentional. We're not, and the bar ML models need to cross is
> not as high as we think.

(2020 post)
{: .centered }

Most of the reason I started believing in faster timelines in 2020 was because I thought
hypothesis 1 (the scaling hypothesis) had proved it had real weight behind it. Not enough to
declare it had won, but enough that it deserved attention.

Now that it's 2024, do I get to say I called it? The view of "things emerge
at scale" is significantly more mainstream these days. I totally called it. This is the main
reason that I feel compelled to keep my 50% / 90% numbers the same but stretch my 10% number
forward. If scaling stops then it'll take a while, and if it keeps going I don't think
it'll take that long.

If there is something I did not call, it would be the flexibility of next token prediction.

> There are certainly problems with GPT-3. It has a fixed attention
> window. It doesn't have a way to learn anything it hasn't already learned from
> trying to predict the next character of text.
> Determining what it does know requires learning how to prompt GPT-3 to give
> the outputs you want, and not all simple prompts work. Finally, it
> has no notion of intent or agency. It's a next-word predictor. That's all it
> is, and I'd guess that trying to change its training loss to add
> intent or agency would be much, much more difficult than it sounds.

(2020 post)
{: .centered }

It turned out that next token prediction was enough to *pretend* to follow intent,
if you finetuned on enough "instruction: example" data, and pretending to follow intent is
close enough to actually following intent.
Supervised finetuning with the same loss was good enough and it was not much more difficult than that.
The finding that [instruction fine-tuning let
a 1.5B model outperform an untuned 175B model](https://openai.com/research/instruction-following)
was basically what made ChatGPT possible at current compute.

![InstructGPT chart](/public/ai-timelines-2023/instructgpt.png)
{: .centered }

Diagram from InstructGPT analysis, with relevant line comparing the 1.5B supervised finetuned model
to an untuned 175B model. Feel free to ignore the blue line that includes RLHF.
{: .centered }

I was correct in claiming
that something very important was happening at scale. I was wrong in how many
ideas would be needed to exploit it.

Every day it gets harder to argue it's impossible to brute force the step-functions
between toy and product with just scale and the right dataset.
I've been converted to the compute hype-train and think the fraction is like 80% compute
20% better ideas. Ideas are still important - things like chain-of-thought have been
especially influential, and in that respect, leveraging LLMs better is still an ideas game.
(At least for now - [see experiments](https://openreview.net/forum?id=92gvk82DE-) on [LLM-driven prompt optimization](https://arxiv.org/abs/2309.03409). Honestly
it wouldn't shock me if a lot of automatic prompt generation happens right now and just
doesn't get published, based on [what people have figured out about DALL-E 3](https://simonwillison.net/2023/Oct/26/add-a-walrus/).)


Unsupervised Learning
------------------------------------------------------------------

> Unsupervised learning got better way faster than I expected. Deep reinforcement
> learning got better
> a little faster than I expected. Transfer learning has been slower than
> expected.

(2020 post)
{: .centered }

Ah, transfer learning. I remember the good old days, where people got excited about
a paper that did like, 5 tasks, and showed you could speed up learning at a 6th task. Now it is
all about large internet-scale models that have gone through enough rounds of next token
prediction to zero-shot a wide class of tasks. Or to quote work from my colleagues,
"[large language models are general pattern machines](https://arxiv.org/abs/2307.04721)".
As far as I know, the dedicated transfer learning techniques like [PCGrad](https://arxiv.org/abs/2001.06782) are not
only unused, they don't get much further research either.

Suffice it to say that unsupervised and self-supervised methods have continued to shine as
the dark matter powering every large language and multimodal model. They still appear to
be the best methods for vaccuuming up compute and data.

> If you've got proof that a large Transformer can handle audio, image, and
> text in isolation, why not try doing so on all three simultaneously?
> Presumably this multi-modal learning will be easier if all the modalities
> go through a similar neural net architecture, and [current] research implies
> Transformers are good-enough job to be that architecture.

(2020 post)
{: .centered }

As for deep reinforcement learning, it certainly has less hype behind it.
I haven't worked with RLHF myself, but when talking to people about it, they think pretty much
any RL algorithm will give you okay results as long as you have good preference data. The
important questions are the ones surrounding the RL algorithm.

I still think
better generic RL algorithms are out there, but it's harder to justify searching for them when you
could spend the marginal compute on extra pretraining or supervised fine-tuning instead. Robot
learning in particular has drifted towards imitation learning because it's easier to work with
and uses compute more effectively.
At least
in my research bubble, the field seems to be moving from generic RL methods to ones that exploit
the structure of preference data, like [DPO](https://arxiv.org/abs/2305.18290) and its siblings.
Still, I feel obligated to plug [Q-Transformer](https://qtransformer.github.io/), an RL + Transformers
paper I worked on this year.


Better Tooling
--------------------------------------------------

> In the more empirical sides of ML, the obvious components of progress are your
> ideas and computational budget, but there are less obvious ones too, like
> your coding and debugging skills, and your ability to utilize your compute.
> It doesn't matter how many processors you have per machine, if your code doesn't
> use all the processors available.
>
> [...]
>
> The research stack has lots of parts, improvements continually happen across that
> entire stack, and most of
> these improvements have multiplicative benefits.

(2020 post)
{: .centered }

Nothing in the tooling department has really *surprised* me. However, as
more people have moved to Transformers-by-default, the tools have become more specialized
and concentrated. Stuff like [FlashAttention](https://github.com/Dao-AILab/flash-attention)
wouldn't be as much of a thing if it weren't relevant to like, literally every modern
ML project.

I had to pick something I missed,
it would be the rise of "research via API calls". API owners have a wider audience of
hobbyists, developers, and researchers, which justifies focusing more on user experience.
I also liked [Pete Warden's](https://petewarden.com/2023/10/15/the-unstoppable-rise-of-disposable-ml-frameworks/)
take, that people are increasingly interested in "the codebase that's already integrated
LLaMa or Whisper", since they don't need everything a generic ML framework provides.

Overall I'd say tools are progressing as expected and don't impact my timeline opinions.


Scaling Laws
-----------------------------------------------------

At the time I wrote the original post, the accepted scaling laws were from [(Kaplan et al, 2020)](https://arxiv.org/abs/2001.08361), and still had
room for a few orders of magnitude.

Two years after that post, [(Hoffman et al, 2022)](https://arxiv.org/abs/2203.15556) announced Chinchilla scaling laws showing that models could be much smaller given a fixed FLOPs budget, as long as you had a larger dataset.
An important detail is that Chincilla scaling laws were estimated assuming that you train a model, then run inference once on your benchmark. However, in a world where most large models are run for inference many times (as part of products or APIs), it's more compute optimal to train for longer than Chinchilla recommends, once you account for inference cost. Even ignoring this,
further [analysis from ThaddÃ©e Yann TYL's blog](https://espadrine.github.io/blog/posts/chinchilla-s-death.html) suggests model sizes could potentially be even lower than
previously assumed.

Despite the dramatic reductions in model size, I don't think the adjustments to scaling laws are that important to
model capabilities. I think the Pareto frontier has bent slightly, but not in a dramatic way. Maybe I am wrong, I have not seen the hard numbers because it seems like literally every
lab with the resources has decided that scaling laws are now need-to-know trade secrets. But for now, I am assuming that FLOPs and data are the bottleneck,
and when controlled for FLOPs we are only slightly more efficient.

I'd say the most important consequence is that *inference times are much smaller than previously assumed*, which has implications for latency in products. In the early 2010s, Google did a lot
of research into how much delays impact search engine usage, and the conclusion was ["it matters a ton"](https://www.thinkwithgoogle.com/future-of-marketing/digital-transformation/the-google-gospel-of-speed-urs-hoelzle/). This led to a heavy focus on minimizing search latency.
ML products are no different. People are impatient.

Speaking of...


Rise of the Product Cycle
------------------------------------------------------------------------------

As part of the 2020 post, I did an exercise I called "trying hard to say no". I decided to start from
the base assumption that short-term AGI was possible, put forward my best guess as to
how we'd end up in that world, then see how plausible I found that story to be.
The reason to do this is because if you want to be correct that AGI is far away,
you have to refute the strongest argument in favor of short-term AGI - so you should at least be able to refute the strongest argument you come up with yourself.

At the time, I described a hypothetical future where not many ideas would be needed,
aside from scale. I assumed someone developed an AI-powered app that's useful enough for
the average person.

> Perhaps someone develops an app or tool, using a model of GPT-3's size or
> larger, that's a huge productivity multiplier. Imagine the first computers,
> Lotus Notes, or Microsoft Excel taking over the business world.

This hypothetical app would bring enough revenue to fund its own improvement.

> If that productivity boost is valuable enough to make the economics work out,
> and you can earn net profit once you account for inference and training costs,
> then you're in business - literally. Big businesses pay for
> your tool.
> Paying customers drives more funding
> and investment, which pays for more hardware, which enables even larger
> training runs.

Since this idea would be based on scale, it would imply the concentration of research into a narrower set of ideas.

> As models grow larger, and continue to demonstrate improved performance,
> research coalesces around a small pool of methods that have been shown to scale
> with compute. Again, that happened and is still happening with deep learning.
> When lots of fields
> use the same set of techniques, you get more knowledge sharing, and that
> drives better research.
> Maybe five years from now, we'll have a new buzzword that takes deep learning's place.

I thought this was a helpful exercise, and concluded by saying
"the number of things that have to go right makes me think
it's unlikely this will occur, but it's worth considering".

And then everything I thought was unlikely came true.

We have a ChatGPT app, which went viral
and inspired a large cast of competitors. It is not a huge productivity booster, but
it's enough of one that people are willing to pay for it.
Supposedly [Microsoft loses $20/user on
Copilot](https://aibusiness.com/nlp/github-copilot-loses-20-a-month-per-user), but
[David Holz's has claimed Midjourney is already profitable](https://www.theregister.com/2022/08/01/david_holz_midjourney/).
I'd split the difference and say most AI services could be profitable, but run at a loss in the name
of growth.

This has driven tech giants and VCs to throw billions at hardware and ML talent hiring.
Deep learning is old news - now everyone says "LLM" or "prompt engineering". [Masked Autoencoders](https://openreview.net/forum?id=MAMOi89bOL) are handling audio,
multimodal Gemini and GPT-4V handle vision, and a few video generation models are uncanny
but making good progress.

To me it now seems completely obvious that transformers will be pushed much, much further
than any other model architecture in the history of machine learning. There is too much
hype, the scaling is inevitable, and even if the doomers' point of view become more popular,
there are enough optimists that I expect *somebody* will push forward,
safety and alignment be damned.
To borrow a point from [Gwern's essay on timing](https://gwern.net/timing),
speculative technology is created by the experts with the
most faith that it will succeed. It will always seem insane
that those experts could be correct, and in fact those experts will usually be too early,
but when they are right they will succeed before the world catches up to their ideas.
And experts with the most faith tend to *understand*
the possible negative externalities, but assume they'll be fine. For better or for worse.


Trying to Say No, Again
----------------------------------------------------------------------------------

Let's run this exercise of "let's assume near-term AGI is possible, how do we get there" again,
to see what's changed.

Once again, progress should be driven primarily by larger compute budgets and scale.
Maybe it's not transformers, maybe one of the
"transformer replacements" that claims to be more efficient will finally win. (I know some people are excited about [state-space models](https://arxiv.org/abs/2312.00752).)
Increasing parameter count is very easy if you
have the compute and data to exploit it, so let's assume the bottlenecks are there.
We can take "ML powers products powers funding powers ML" as a given. That's just
what's happening.

I don't really know enough about hardware to discuss it in any detail, so let's just assume
it'll continue to grow. Chips have gotten more expensive, given that everyone is keeping their eyes on it,
including nation states. Still, computers are useful, ML models are useful, seems reasonable
to assume the competing factions will figure something out. The silicon must flow, after all.

We have already crossed the event horizon of training on everything we can find on the Internet.
To me, I think the main question is on the high-quality data to augment it.
The rumor is that GPT-4 is good at coding in part because
OpenAI spent a lot of time, effort, and money on acquiring good coding data. Adobe
[put out an ad asking for "500 to 1000 photos of bananas in real life situations"](https://petapixel.com/2023/10/04/adobe-wants-1000-photos-of-bananas-to-help-train-its-ai-image-generator/)
for their AI projects.
Anthropic has a dedicated ["tokens" team to acquire and understand data](https://www.builtinsf.com/job/operations/research-manager-tokens/158569), based on job listings.
Everyone wants good data, and they're willing to pay for it, because people trust
the models can use that data effectively as long as they can get it.

Getting this data by hand doesn't seem good enough. Trying to manually fight up a
power law sounds awful.
However, there's growing interest in synthetic data. There was an entire [NeurIPS workshop](https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/SyntheticData4ML#tab-accept-poster) about it.
One of the early results around GPT-4 by [(Pan, Chan, Zou et al)](https://arxiv.org/abs/2304.03279)
was that GPT-4's label accuracy was competitive with human crowdworkers. Diffusion based
image augmentation has been shown to improve [robot learning](https://diffusion-rosie.github.io/),
and Anthropic has based a lot of its branding on constitutional AI and "RL from AI feedback".

A long time ago, when OpenAI still did RL in games / simulation, they were very into [self-play](https://openai.com/research/competitive-self-play). You run agents against copies of themselves, score
their interactions, and update the models towards interactions with higher reward.
Given enough time, they learn complex strategies through competition.
At the time, I remember Ilya said they cared because self-play was a method to turn compute into
data. You run your model, get model-generated data, funnel it back in, and get an exponential
improvement in your Elo curves. What was quickly clear was that this was true,
but only in the narrow regime where self-play was possible. In practice that usually meant
game-like environments, with at most a few hundred different entities, a ground truth reward function
that was not too easy or hard, and easy ability to reset and run faster than real time. Without
all those qualities, self-play sputtered and died with nothing to show for it besides
warmer GPUs.

I think it's possible we're at the start of a world where self-play or self-play-like ideas work on any text generation task.
A world where direct human feedback is only used to bootstrap reward models or sanity check
new ones. Everything else is model-generated and model-supervised.
Model-generated data, model-based rewards, feeding the same engine.
Even if it the accuracy of synthetic labels is worse, it's often cheaper. Think harder, then
teach yourself to come up with that idea the first time.
If that's true, and
data becomes untethered from human effort, then progress just fully turns into how many FLOPs you
can shovel into the system.

Mix some search algorithms on top, and you're in business. During the Sam Altman drama, there was
mention of some kind of method called Q\*, driving a lot of hype. Eventually
Yann LeCun put out a post saying people really needed to cool down, since literally every lab has
looked into combining search with LLMs, and it would not be that surprising if someone made it work.

He was 100% correct. It is a very obvious idea to try. DeepMind put out a preprint that
[CNNs are good evaluators of Go moves](https://arxiv.org/abs/1412.6564) in December 2014, added search
via MCTS, and turned it into AlphaGo within a year. It was the ML success story of the decade.
People do not forget the lessons from machine learning's crowning achievements.
There are details from here to there, on whether it can justify its computational inefficiency and
whether our base models are good enough to act as good search subroutines. But it really ought to
work, and will always be there to eat marginal FLOPs if people don't have any better ideas.

If that plays out, it just leaves the question of whether intelligence is bottlenecked on physical
embodiment.

> If this model is good at language, speech, and visual data, what sensor inputs
> do humans have that this doesn't? It's just the sensors tied to physical
> embodiment, like taste and touch. Can we claim intelligence is bottlenecked
> on those stimuli?

(2020 post)
{: .centered }

My impression is that people outside robotics have a religion of disembodiment, and people in
robot learning have a religion of embodiment. Which has always left me a bit of an odd duck,
given that I work in robot learning but don't have much of an embodiment religion. I think it's very
obvious that robotics data should help with spatial understanding, and is inherently agentic in nature.
It is very non-obvious that embodiment is the only route towards more agentic goal-directed
systems. To me it feels like it is just one route out of many. (But see [alternative views hera](https://keerthanapg.com/tech/embodiment-agi/).)

Overall I think it's plausible that we'll continue to scale, some of the perceived bottlenecks won't
matter, and we'll use weaker ML models to widen the ones that do. I'm finding it increasingly hard to
refute that view of the world. Of course, it could all cap out - but
let me know when you *see* the scaling laws hit a wall, instead of talking about why they ought to
stop. Since so far, I don't think they have.


On Hype
--------------------------------------------------------------------------------

In 2016, a few prominent ML researchers decided to pull a prank. They set up a site for
"Rocket AI", based on some mysterious method called "Temporally Recurrent Optimal Learning",
then all coordinated stories about a wild launch party at NeurIPS 2016 that ended with policy
involvement. It was all fake, as [detailed in this postmortem](https://medium.com/the-mission/rocket-ai-2016s-most-notorious-ai-launch-and-the-problem-with-ai-hype-d7908013f8c9).
There's a fun quote near the end:

> AI is at peak hype, and everyone in the community knows it.

Here's the Google search trends for "artificial intelligence" and "AI" since 2016. Both are scaled
out of 100, with 100 matching peak search interest. Let's see how peak 2016 hype compares to now.

![Artificial intelligence chart](/public/ai-timelines-2023/artificialsearchtrend.png)
{: .centered }

![AI chart](/public/ai-timelines-2023/aisearchtrend.png)
{: .centered }

Oh, what naive children we were! I do think it's funny that AI is one of only a few research topics
where people try to play down hype for the sake of the conversational commons. Imagine feeling like
you need to push people away from being interested in your research. There's a real privilege there.

I have really really tried to avoid getting caught up in hype, but I do think AGI could be soon.
The routes for improvement are clear, it's just a question of if they can't work or if they can work
assuming billions of dollars of funding.
In AI, models can **never** do everything people claim they can, but what the models **can** do is ever-growing
and never slides backward. Today is the worst AI will ever be. Even if all the VC companies bust and LLMs
dry up, we'll still have the models that are already trained and the ideas already derived. There is no going back.
I'd recommend people think about what that means.

> Everything has been changing since last generation was born  
> And they won't try to take in change is a two edged sword
