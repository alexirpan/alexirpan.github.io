---
layout: post
title:  "My AI Timelines Have Sped Up (Again)"
date:   2023-12-29 15:08:00 -0700
---

In August 2020, I wrote a post about [my AI timelines]({% post_url 2020-08-18-ai-timelines %}).
My forecast at the time was:

* 10% chance by 2035
* 50% chance by 2045
* 90% chance by 2070

Now I would say it's more like

* 10% chance by 2028 (i.e. 5 years away)
* 50% chance by 2043
* 90% chance by 2070

Like last time, there is no hard "X happened so I will subtract Y years" logic going on. It
is a vibes-based adjustment. The TL;DR reasoning is that in 2020, I noted some trends that
made me think near-term AGI was unlikely but more plausible. Since then, some events I considered
unlikely came to pass, and the new trends are pushing towards faster progress.

I think it would be most instructive if I directly compared my 2020 post to my new one, since it's
a rare opportunity to directly rate how I did at forecasting.


The Role of Compute
---------------------------------------------------------------

> How much
> are AI capabilities driven by better hardware letting us scale existing models, and how much is driven
> by new ML ideas? This is a complicated question, especially because the two
> are not independent. New ideas enable better usage of hardware, and more hardware
> lets you try more ideas. My 2015 guess to the horrid simplification was that
> 50% of AGI progress would come from
> compute, and 50% would come from better algorithms. There were several things
> missing between 2015 models, and something that put the "general" in
> artificial general intelligence. I was not convinced more compute would fix that.
>
> Since then, there have been many successes powered by scaling up models, and I
> now think the balance is more like 65% compute, 35% algorithms. I suspect that
> many human-like learning behaviors could just be emergent properties of larger
> models. I also suspect that many things humans view as
> "intelligent" or "intentional" are neither. We just want to think we're
> intelligent and intentional. We're not, and the bar ML models need to cross is
> not as high as we think.

(2020 post)
{: .centered }

Now that it's 2023, do I get to say I called it? The view of "things emerge
at scale" is significantly more mainstream these days. I totally called it.

At the same time, I think I only get to claim a partial win here. I assumed
next token prediction would be hard to improve.

> There are certainly problems with GPT-3. It has a fixed attention
> window. It doesn't have a way to learn anything it hasn't already learned from
> trying to predict the next character of text.
> Determining what it does know requires learning how to prompt GPT-3 to give
> the outputs you want, and not all simple prompts work. Finally, it
> has no notion of intent or agency. It's a next-word predictor. That's all it
> is, and I'd guess that trying to change its training loss to add
> intent or agency would be much, much more difficult than it sounds.

On this front, it turns out I was quite wrong. You could train a model to
follow intent just by finetuning on enough "instruction: example" data, with the same
supervised loss, and it was not much more difficult than that.
The finding that [instruction fine-tuning let
a 1B model outperform an untuned 175B model](https://openai.com/research/instruction-following)
was basically what made ChatGPT possible at current compute.

![InstructGPT chart](/public/ai-timelines-2023/instructgpt.png)
{: .centered }

I was correct in claiming
that something very important was happening at scale. I was wrong in how many
ideas would be needed to exploit it. This feels like the most important reason for me
to assume AGI should be faster than I thought.

Every day it gets harder to argue it's impossible to brute force the step-functions
between toy and product with just scale and the right dataset.
I've been converted to the compute hype-train and think it's like 80% compute
20% better ideas. Ideas are still important - chain-of-thought prompting has been
especially influential, and in that respect, leveraging LLMs better is still an ideas game.
(At least for now - [see experiments](https://openreview.net/forum?id=92gvk82DE-).) on [LLM-driven prompt optimization](https://arxiv.org/abs/2309.03409). Honestly
it wouldn't shock me if a lot of automatic prompt generation happens right now and just
doesn't get published, based on [what people have figured out about DALL-E 3](https://simonwillison.net/2023/Oct/26/add-a-walrus/).)


Unsupervised Learning
------------------------------------------------------------------

> Unsupervised learning got better way faster than I expected. Deep reinforcement
> learning got better
> a little faster than I expected. Transfer learning has been slower than
> expected.

(2020 post)
{: .centered }

Ah, transfer learning. I remember the good old days, where people got excited about
a paper that did like, 5 tasks, and showed you could speed up learning at a 6th task. Now it is
all about large internet-scale models that have gone through enough rounds of next token
prediction to zero-shot a wide class of tasks. Or to quote work from my colleagues,
"[large language models are general pattern machines](https://arxiv.org/abs/2307.04721)".
As far as I know, the dedicated transfer learning techniques like [PCGrad](https://arxiv.org/abs/2001.06782) are just, not used anymore.

Suffice it to say that unsupervised and self-supervised methods have continued to shine as
the dark matter powering every large language and multimodal model. They still appear to
be the best methods for vaccuuming up compute.

> If you've got proof that a large Transformer can handle audio, image, and
> text in isolation, why not try doing so on all three simultaneously?
> Presumably this multi-modal learning will be easier if all the modalities
> go through a similar neural net architecture, and [current] research implies
> Transformers are good-enough job to be that architecture.

(2020 post)
{: .centered }

As for deep reinforcement learning, it certainly has less hype behind it. I felt like there used
to be a new "standard RL algorithm" every 3-4 years, but nowadays I mostly see SAC or PPO, which date to 2018.
I haven't worked with RLHF myself, but when talking to people about it, they think pretty much
any RL algorithm will give you okay results as long as you have good preference data.
I still think
better generic RL algorithms are out there, but it's harder to justify searching for them when you
could spend the marginal compute on extra pretraining or supervised fine-tuning instead. Robot
learning in particular has drifted towards imitation learning because it's easier to work with
and uses compute more effectively.
At least
in my research bubble, the field seems to be moving towards RL methods that exploit the structure of preference
data, like [DPO](https://arxiv.org/abs/2305.18290) and its siblings.


Better Tooling
--------------------------------------------------

> In the more empirical sides of ML, the obvious components of progress are your
> ideas and computational budget, but there are less obvious ones too, like
> your coding and debugging skills, and your ability to utilize your compute.
> It doesn't matter how many processors you have per machine, if your code doesn't
> use all the processors available.
>
> [...]
>
> The research stack has lots of parts, improvements continually happen across that
> entire stack, and most of
> these improvements have multiplicative benefits.

(2020 post)
{: .centered }

I'd say that nothing in the tooling department has really *surprised* me. However, as
more people have moved to Transformers-by-default, the specialization of the tooling
has gotten more concetrated. Stuff like [FlashAttention](https://github.com/Dao-AILab/flash-attention)
wouldn't be as much of a thing if it weren't relevant to like, literally every modern
deep learning project.

There is also the wider adoption of HuggingFace, but if I had to pick something I missed,
it would be the rise of "research via API calls". API owners have a wider audience of
hobbyists, developers, and researchers, which justifies focusing more on user experience.
I also liked [Pete Warden's](https://petewarden.com/2023/10/15/the-unstoppable-rise-of-disposable-ml-frameworks/)
take, that people are increasingly interested in "the codebase that's already integrated
LLaMa or Whisper", rather than generic ML frameworks.

Overall I'd say this is progressing as expected and doesn't impact my timeline opinions.


Scaling Laws
-----------------------------------------------------

At the time I wrote the original post, the accepted scaling laws were from [(Kaplan et al, 2020)](https://arxiv.org/abs/2001.08361). Two years afterwards, [(Hoffman et al, 2022)](https://arxiv.org/abs/2203.15556) announced Chinchilla scaling laws showing that models could be much smaller given a fixed FLOPs budget, as long as you had a larger dataset.

An important detail is that Chincilla scaling laws were estimated assuming that you train a model, then run inference once on your benchmark. However, in a world where most large models are run for inference many times (as part of products or APIs), it's more compute optimal to train for longer than Chinchilla recommends, once you account for inference cost. Even ignoring this,
further [analysis from ThaddÃ©e Yann TYL's blog](https://espadrine.github.io/blog/posts/chinchilla-s-death.html) suggests model sizes can probably be even lower than previously assumed.

Despite the dramatic reductions in model size, I don't think the adjustments to scaling laws are that important to
model capabilities. My general sense is that the Pareto frontier has bent slightly, but not in a dramatic way. Maybe I am wrong, I have not seen the hard numbers because it seems like literally every
lab with the resources has decided that scaling laws are now need-to-know trade secrets. But it is cause for me to conclude timelines have sped up a bit.

I'd say the most important consequence is that *inference times are much smaller than previously assumed*, which has implications for latency in products. When Google was ascending, they did a lot
of research into how much delays impact search engine usage, and the conclusion was ["it matters a ton"](https://www.thinkwithgoogle.com/future-of-marketing/digital-transformation/the-google-gospel-of-speed-urs-hoelzle/). ML products are no different.

Speaking of...


Rise of the Product Cycle
------------------------------------------------------------------------------

In 2020, I tried to describe what a short-term AGI world would look like, to see how believable
the story sounded.

> The AGI debate is always a bit of a mess, because people have wildly
> divergent beliefs over what matters. One useful exercise is to assume AGI
> is possible in the short term, determine what could be true in that hypothetical
> future, then evaluate whether it sounds reasonable.
>
> This is crucially *very* different from coming up with reasons why AGI can't happen,
> because there are tons of arguments why it can't happen. There are also tons
> of arguments why it can happen. This exercise is about putting more effort into
> the latter, and seeing how hard it is to say "no" to all of them. This helps
> you focus on the arguments that are actually important.

In this story, I described a hypothetical future where not many ideas would be needed,
aside from scale. I assumed someone develops an AI-powered app that's useful enough for
the average person.

> Perhaps someone develops an app or tool, using a model of GPT-3's size or
> larger, that's a huge productivity multiplier. Imagine the first computers,
> Lotus Notes, or Microsoft Excel taking over the business world.

And how this hypothetical app drives its own funding.

> If that productivity boost is valuable enough to make the economics work out,
> and you can earn net profit once you account for inference and training costs,
> then you're in business - literally. Big businesses pay for
> your tool.
> Paying customers drives more funding
> and investment, which pays for more hardware, which enables even larger
> training runs.

Which encourages the concentraion of research into a few ideas.

> As models grow larger, and continue to demonstrate improved performance,
> research coalesces around a small pool of methods that have been shown to scale
> with compute. Again, that happened and is still happening with deep learning.
> When lots of fields
> use the same set of techniques, you get more knowledge sharing, and that
> drives better research.

And then, you know, maybe we'll stop saying the phrase "deep learning" in favor
of something else.

> Maybe five years from now, we'll have a new buzzword that takes deep learning's place.

Which just leaves the question of whether intelligence is bottlenecked on physical
embodiment.

> If this model is good at language, speech, and visual data, what sensor inputs
> do humans have that this doesn't? It's just the sensors tied to physical
> embodiment, like taste and touch. Can we claim intelligence is bottlenecked
> on those stimuli? Sure, but I don't think it is. You arguably only need
> text to pretend to be human.

But, you know, this was just a hypothetical future I described in 2020. It's not
like any of that was guaranteed to play out.

Fine, I will stop being coy and spell it out. We have a ChatGPT app, which went viral
and inspired a large cast of competitors. It is not a huge productivity booster, but
it's enough of one that people are willing to pay for better access.
Supposedly [Microsoft loses $20/user on
Copilot](https://aibusiness.com/nlp/github-copilot-loses-20-a-month-per-user), but
[David Holz's has claimed Midjourney is already profitable](https://www.theregister.com/2022/08/01/david_holz_midjourney/). My best guess is that most services are running at a loss in the
name of growth, but they could be profitable now if they turned off the growth spigot.

This has driven tech giants and VCs to throw billions at hardware and ML talent hiring.
Now everyone says words like "LLM" and "prompt engineering". The current wave
of releases are on incorporating vision data, and there's enough evidence from Whisper and
[Masked Autoencoders](https://openreview.net/forum?id=MAMOi89bOL) to show they'll handle
audio too.

I claimed that a number of things would have to go right for this future to exist, which
made me think it was unlikely. But then everything I thought was unlikely came true.

