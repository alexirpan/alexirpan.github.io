---
layout: post
title:  "Who is AI For?"
date:   2025-01-28 00:32:00 -0800
---

As AI gets better, I've been thinking about who it's actually for.

There are obvious use cases that are popular right now. Image generation for people
who want filler art, coding assistance for people who code, vibe coding for people who
don't. But also...math contest solving? Science Olympiad questions? Who is that for?

I think the easy answer to this question, is that right now AI is for the AI developers.
People working in AI respect math and science contests, doing well on them is high status,
so they try testing on those domains, even if most people do not care.
It's why so many LLM developers are focusing on code, it's why we get announcement posts
advertising that a new model is good at answering Typescript questions. A viral tweet of
GPT-3 writing React code was what inspired OpenAI to work more on coding. It's useful,
it makes money,
it feeds into the story of AI making
the development of AI more efficient.
and it's *easy*. Not in it being easy to improve coding, but in it being easy
to evaluate. Should we be surprised that people who write code can tell what models are good
at coding?
Did we focus on code because it was the best
thing to do, or because it was the closest thing that seemed approachable?

There are arguments that teaching models to reason in these specific math/code/science domains generalizes
to reasoning elsewhere, and to be clear, those arguments *are* bearing fruit as far as I can tell.
Yet it doesn't feel like the only path we could have taken. We say we want our AI to be
truth-seeking, and then define truth-seeking as answering Q&A questions or solving unit tests.

> Recently, I sharpened a #2 pencil and took the history section of "Humanity's Last Exam.” Consisting of 3,000 extremely difficult questions, the test is intended for AI, not me. According to its creators and contributors, Humanity’s Last Exam will tell us when artificial general intelligence has arrived to supersede human beings, once a brilliant bot scores an A.
> [...] Of the thousands of questions on the test, a mere 16 are on history. By comparison, over 1,200 are on mathematics. This is a rather rude ratio for a purported Test of All Human Knowledge.

[Asking Good Questions is Harder Than Giving Great Answers](https://newsletter.dancohen.org/archive/asking-good-questions-is-harder-than-giving-great-answers/), by Dan Cohen
{: .centered }

If CGP Grey videos have taught me anything, [we should have been asking the historians what it means for something to be true](https://www.youtube.com/watch?v=Ex74x_gqTU0).
Or philosophers, if the point was to build rigorous arguments in natural language.
The story of philosophy is people writing increasingly nitpicky arguments attacking imperfect
imprecise definitions of how to view the world. Maybe in another universe we'd have DeepConfucius or
OpenAristotle.

I'm not an AI pessimist, despite working in safety. I think AI can be good for the world. But it's not
going to be good for the world unless we (the field) proactively work in that direction.
When OpenAI first announced their video generation model, Sora, they gave closed beta access to a
few filmmakers to see what they made of it. The result was that it didn't help much,
because the model didn't understand the filmmakers, because OpenAI devs did not know the language
of cinematography in the way filmmakers did.

Presumably this is better now - but only because they asked.

> With cinematic shots, the ideas of ‘tracking’, ‘panning’, ’tilting’ or ‘pushing in’ are all not terms or concepts captured by metadata. As much as object permanency is critical for shot production, so is being able to describe a shot, which Patrick noted was not initially in SORA. “Nine different people will have nine different ideas of how to describe a shot on a film set. And the (OpenAI) researchers, before they approached artists to play with the tool, hadn’t really been thinking like filmmakers.” Shy Kids knew that their access was very early, but “the initial version about camera angles was kind of random.” Whether or not SORA was actually going to register the prompt request or understand it was unknown as the researchers had just been focused on image generation. Shy Kids were almost shocked by how much the OpenAI was surprised by this request. “But I guess when you’re in the silo of just being researchers, and not thinking about how storytellers are going to use it… SORA is improving, but I would still say the control is not quite there. You can put in a ‘Camera Pan’ and I think you’d get it six out of 10 times.”

[Actually Using Sora](https://www.fxguide.com/fxfeatured/actually-using-sora/)
{: .centered }

To be fair, I assume this is better now, given this quote is from last year. OpenAI deserves
props for asking for feedback in the first place, because some people do not.
We automate art not because it is easy, not because the artists want it, but because we can and it
looks impressive when it works.

Things move fast in AI, and we've speedrun the journey from cute toy to symbol of capitalism
in record time. If people expect their benevolent AGI to cure cancer and stop aging and open up
new vistas of knowledge, it'd be real cool if we make sure that's actually something we focus on.
Since right now, I'm not convinced it is. It feels like the most likely outcome is that people go
all-in on pushing raw intelligence, in the way that AI developers can measure it, leaving behind
humanities people that ought not to be left behind, following the path of easy-to-generate text
rather than text that encourages a better life, and creating the final victory of capital over labor.
I don't know what those better paths look like, they just don't feel like this one.
