---
layout: post
title:  "Not even sure if this will be a post"
date:   2025-01-28 00:32:00 -0800
---

As AI gets better, I've been thinking about who it's actually for.

There are obvious use cases for it, ones popular right now. Image generation for people
who want filler art, coding assistance for people who code. But also...math contest
solving? Science Olympiad questions? Who is that for?

I think the easy answer to this question, is that right now, AI is getting made for
the AI developers. This is why all LLM developers are focusing on code. Anecdotally, a
demo of GPT-3 writing React code was the first one that made people take notice of LLMs.
It's one of the clear money making domains. And, it feeds into the story of AI making
the development of AI more efficient.

But, how much intentionality is in this story? Did we focus on code because it was the best
thing to do, or because it was the closest thing that seemed approachable?

This view, that AI is made for AI developers, helps explain why there's been so much focus
on math and science benchmarks. These fields are generally considered high status for engineers.
Many AI developers did math contests or science olympiads, so they know how hard these questions are, so they
know how impressive it would be to make a model that solves them. More recently, it's been argued
that math and code have natural logical structure, where you need to chain facts together to reason to
an answer. Combined with ground truth feedback, the arugment is that training math and code into
LLMs makes them better at logic and reasoning in general.

This seems true so far, but I do wonder if it was the only path. Like, philosophy is lacking in ground
truth feedback, but as a field it's defined heavily by creating more and more specific definitions to
try to support or break apart a system for viewing the world. You could imagine a world where for some reason
all engineers were really into philosophy, and tried to make LLMs be the best philosophers in the world,
and maybe a similar kind of reasoning LLM would get developed. We'd have some
DeepConfucius and OpenAristotle or whatever.

When OpenAI first announced their Sora video generation model, they touted the risk of deepfakes and the
way it could be promising for use in filmmaking. So, they gave closed beta access to a few filmmakers to see
what they made of it.

The first comment was that the model was incredibly bad at handling camera moves. It didn't know how to handle
dolly zooms. what a pan was, how to handle pushing in or pushing out the camera. There was this whole language
of cinematography that the model just couldn't do. Presumably this is better now - but only because they asked.

https://www.fxguide.com/fxfeatured/actually-using-sora/

> With cinematic shots, the ideas of ‘tracking’, ‘panning’, ’tilting’ or ‘pushing in’ are all not terms or concepts captured by metadata. As much as object permanency is critical for shot production, so is being able to describe a shot, which Patrick noted was not initially in SORA. “Nine different people will have nine different ideas of how to describe a shot on a film set. And the (OpenAI) researchers, before they approached artists to play with the tool, hadn’t really been thinking like filmmakers.” Shy Kids knew that their access was very early, but “the initial version about camera angles was kind of random.” Whether or not SORA was actually going to register the prompt request or understand it was unknown as the researchers had just been focused on image generation. Shy Kids were almost shocked by how much the OpenAI was surprised by this request. “But I guess when you’re in the silo of just being researchers, and not thinking about how storytellers are going to use it… SORA is improving, but I would still say the control is not quite there. You can put in a  ‘Camera Pan’ and I think you’d get it six out of 10 times.”
