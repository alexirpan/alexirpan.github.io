---
layout: post
title:  "The Rise of Do What I Mean"
date:   2022-04-11 02:17:00 -0700
---

Boy, last week was busy for deep learning. Let's start with the paper I worked on.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ysFav0b472w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

SayCan is a robot learning system that we've been developing for about the past year. The
paper is on arXiv [here](https://arxiv.org/abs/2204.01691), and it builds on a lot of past
work that's been done in conditional imitation learning and reinforcement learning.

In short: suppose you have a robot that can do some tasks we tell it to do in natural language,
like "pick up the apple" or "go to the trash can". If you have these primitive tasks, you can
chain them together into more complex tasks. If I want the robot to throw away the apple, I could say,
"pick up the apple, then go to the trash can, then place it in the trash can", and assuming those
three primitives are learned well, the robot will complete the task.

Now, you wouldn't want to actually say "pick up the apple, then go to the trash can, then place it
in the trash can". We'd like to just say "throw away the apple" and have the rest be done automatically.
Well, in the past few years, large language models (LLMs) have shown they can do reasonably well at many
problems, as long as you can describe the input and output with just language. And this problem of
mappining "throw away the apple" to "pick / go to trash / place" fits that description exactly!

![Diagram of the SayCan model](/public/do-what-i-mean/saycan.png)
{: .centered }

The LLM is not aware of the robot's surroundings or capabilities,
so using it naively may generate sentences the robot isn't capable of performing. This is handled with
a two-pronged approach.

1. The language generation is constrained to the primitives the robot can (currently) perform.
2. The generated primitive is scored based on a learned value function, which maps the image + language to
the estimated probability the robot can complete the task.

You can view this as the LLM estimating the probability an instruction helps the high-level goal assuming a perfect
robot, and the value function acting as a correction to drive behavior towards things the robot can do.

This glosses over a lot of work on how to learn the value function, how to learn the policy for the primitive tasks,
prompt engineering for the large language model, and more. If you want more details, feel free to read the paper!
My main takeaway is that LLMs are pretty good. The language generation is the easy part and its outputs are usually
reasonable, while the value function + policy are the hard parts. Even assuming that LLMs don't get better, there is
a lot of slack left for robot capabilities to get better and move towards robots that do what you mean, not just what
you say.

\* \* \*
{: .centered }

To what should be the surprise of no one, large language models have been getting better. I mean, this was explained in the
GPT-3 paper, the trendlines didn't show any signs of stopping and there was room for at least 1 more order of magnitude.

DeepMind put out a paper for their [Chinchilla model](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training). Through
more careful investigation, they found that training corpus size had not increased in size relative to parameter count
as much as it should have. By using about 4x more training data (300 billion tokens --> 1.4 trillion tokens), they
coud reduce model size 4x (280B parameters --> 70B parameters) while achieving better performance.

![Chincilla extrapolation curve](/public/do-what-i-mean/chinchilla.png)
{: .centered }

Estimated compute-optimal scaling, using larger datasets and fewer parameters than previous scaling laws predicted.
{: .centered }

Meanwhile, Google Brain announced their [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) language model, trained with 540B parameters on 780 billion tokens,
and again, we see something similar to GPT-3. Performance increases on many tasks that were already doing well, but on some
tasks, there are discontinous improvements, where the increase in scale leads to a larger increase in performance than
predicted from small scale experiments.

![PaLM result curves](/public/do-what-i-mean/palm.png)
{: .centered }

Figure 5 of PaLM paper. Each plots shows model performance on a set of tasks where PaLM's performance vs model size is log-linear (left), "discontinous" (middle), or relatively flat (right). I'm not even sure the flat examples are even that flat, they look slightly under log-linear at worst.

The emoji to movie and joke explanation results are especially interesting to me. They feel qualitatively better in a way
that's hard to describe, often combining concepts with a higher level of complexity than I expect.

![Emoji movie explanation](/public/do-what-i-mean/emoji.png)
{: .centered }

Both of these works have noted we have yet to hit a ceiling on model scaling. As far as I know, no one is willing to
predict whether or what qualitatively new capabilities we'll see from the next large language model. This is worth
emphasizing - people genuinely don't know. Before seeing the results of the PaLM paper, I *think* you could argue that
language models would have more trouble learning math-based tasks, and the results corroborate this (both `navigate` and
`mathematical_induction` are math-based). However, I don't think you could have predicted that `english_proverbs` and
`logical_sequence` *in particular* would exceed log-linear in performance. You could predict that something would get better,
but not what.

Note also that the Chinchilla curves project that given the PaLM compute budget, using 140B params with a big enough dataset
would be more compute efficient. In other words, there is room for improvement without changing the model architecture,
as long as you crawl more training data - a probem with far less research uncertainty than anything from the ML side.

Let's just say it's not a good look for anyone claiming deep learning models are plateauing.

\* \* \*
{: .centered }

That takes us to DALLÂ·E 2.

![DALL-E 2 generations](/public/do-what-i-mean/dalle.png)
{: .centered }

On one hand, image generation is something that naturally captures the imagination. You don't have to explain why it's cool,
it's just obviously cool. Similar to language generation, progress here might overstate the state of the field.
And yet, I find it hard to say this doesn't portend something.

From a purely research standpoint, I was a bit out of the loop on what was state-of-the-art in image generation, and I didn't
realize diffusion based image synthesis was outperforming autoregressive image synthesis. Very crudely, the difference between the
two is that diffusion gradually updates the entire image towards a desired target, while autoregressive generation draws each image
patch in sequence. Empirically, diffusion has been working better, and some colleagues told me that it's because diffusion better
handles the high-dimensional space of image generation. That seems reasonable to me, but, look, we're in the land of deep learning.
Everything is high-dimensional. Are we going to claim that language is not a high-D problem? If diffusion models are inherently better
in that regime, then diffusion models should be eating the ML world.

Well, maybe they are. I've been messing around with Codex a bit, and would describe it as "occasionally amazing, often frustrating".
It's great when it's correct and annoying when it's not. Almost-correct language is amusing. Almost-correct code is just wrong.
In any case, there was a [recent announcement](https://openai.com/blog/gpt-3-edit-insert/) of enabling these models to edit and insert
text, instead of just completing it, and in hindsight it seems likely it's based on similar core technology to the DALL-E image
editing examples. It's the same problem after all.

IMAGES

Where does this leave us?

In general, there is a lot of hype and excitement about models with a natural language API. There is a building consensus that text
is a rich enough input space to describe our intentiosn towards ML models, and although it may not be the *only* input space, it seems
like a key component. If you believe the thesis that language unlocked humanity's ability to share complex ideas in short amounts of
time, then computers learning what to do based on language should be viewed as a similar sea change in how we interact with ML
models. It feels like we are heading for a future where more computer systems are "do what I mean", where we hand more agency to models
that we believe have earned the right to that agency. That's something we'll need to be careful about. I hope people are paying
attention.
